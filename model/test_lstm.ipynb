{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_lstm.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OzVcfdF1iOLF","executionInfo":{"status":"ok","timestamp":1624277102588,"user_tz":-540,"elapsed":23495,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"d70ad417-29bf-4da9-d18b-286af10e1f86"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Htkb2kteiTN1","executionInfo":{"status":"ok","timestamp":1624277105378,"user_tz":-540,"elapsed":415,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"4518f744-f69a-40c1-9e6b-0597526b16e5"},"source":["%cd /content/drive/My Drive/Colab Notebooks/swcon_capstone"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/swcon_capstone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FfXfpCYS9Gib","executionInfo":{"status":"ok","timestamp":1624277112576,"user_tz":-540,"elapsed":7202,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}}},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from sklearn.metrics import classification_report"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pkj278b29LzD"},"source":["X_train_0703_1 = np.load(\"X_train_0703_1.npy\")\n","y_train_0703_1 = np.load(\"y_train_0703_1.npy\")\n","X_test_0703_1 = np.load(\"X_test_0703_1.npy\")\n","y_test_0703_1 = np.load(\"y_test_0703_1.npy\")\n","\n","X_train_0703_2 = np.load(\"X_train_0703_2.npy\")\n","y_train_0703_2 = np.load(\"y_train_0703_2.npy\")\n","X_test_0703_2 = np.load(\"X_test_0703_2.npy\")\n","y_test_0703_2 = np.load(\"y_test_0703_2.npy\")\n","\n","X_train_0705 = np.load(\"X_train_0705.npy\")\n","y_train_0705 = np.load(\"y_train_0705.npy\")\n","X_test_0705 = np.load(\"X_test_0705.npy\")\n","y_test_0705 = np.load(\"y_test_0705.npy\")\n","\n","X_train_0707 = np.load(\"X_train_0707.npy\")\n","y_train_0707 = np.load(\"y_train_0707.npy\")\n","X_test_0707 = np.load(\"X_test_0707.npy\")\n","y_test_0707 = np.load(\"y_test_0707.npy\")\n","\n","X_train_0710_1 = np.load(\"X_train_0710_1.npy\")\n","y_train_0710_1 = np.load(\"y_train_0710_1.npy\")\n","X_test_0710_1 = np.load(\"X_test_0710_1.npy\")\n","y_test_0710_1 = np.load(\"y_test_0710_1.npy\")\n","\n","X_train_0710_2 = np.load(\"X_train_0710_2.npy\")\n","y_train_0710_2 = np.load(\"y_train_0710_2.npy\")\n","X_test_0710_2 = np.load(\"X_test_0710_2.npy\")\n","y_test_0710_2 = np.load(\"y_test_0710_2.npy\")\n","\n","X_train_0710_3 = np.load(\"X_train_0710_3.npy\")\n","y_train_0710_3 = np.load(\"y_train_0710_3.npy\")\n","X_test_0710_3 = np.load(\"X_test_0710_3.npy\")\n","y_test_0710_3 = np.load(\"y_test_0710_3.npy\")\n","\n","X_train_0712 = np.load(\"X_train_0712.npy\")\n","y_train_0712 = np.load(\"y_train_0712.npy\")\n","X_test_0712 = np.load(\"X_test_0712.npy\")\n","y_test_0712 = np.load(\"y_test_0712.npy\")\n","\n","X_train_0713 = np.load(\"X_train_0713.npy\")\n","y_train_0713 = np.load(\"y_train_0713.npy\")\n","X_test_0713 = np.load(\"X_test_0713.npy\")\n","y_test_0713 = np.load(\"y_test_0713.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nzMRZHaN-1BF"},"source":["X_train = np.concatenate((X_train_0703_1, X_train_0703_2, X_train_0705, X_train_0707, X_train_0710_1, X_train_0710_2, X_train_0710_3, X_train_0712, X_train_0713), axis=0)\n","y_train = np.concatenate((y_train_0703_1, y_train_0703_2, y_train_0705, y_train_0707, y_train_0710_1, y_train_0710_2, y_train_0710_3, y_train_0712, y_train_0713), axis=0)\n","X_test = np.concatenate((X_test_0703_1, X_test_0703_2, X_test_0705, X_test_0707, X_test_0710_1, X_test_0710_2, X_test_0710_3, X_test_0712, X_test_0713), axis=0)\n","y_test = np.concatenate((y_test_0703_1, y_test_0703_2, y_test_0705, y_test_0707, y_test_0710_1, y_test_0710_2, y_test_0710_3, y_test_0712, y_test_0713), axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V9KdionzBvQY","executionInfo":{"elapsed":349,"status":"ok","timestamp":1623022025270,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"cb53f67f-dd69-4eb2-9b96-45ab3d6cd47b"},"source":["print(X_train.shape)\n","print(X_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(157501, 160)\n","(39381, 160)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lFqLewlEf_mU"},"source":["### 아래는  전 데이터셋에 대해 머러 모델을 돌린 것"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6ttq_nKiJfA","executionInfo":{"elapsed":12851323,"status":"ok","timestamp":1623757373896,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"959d22c4-4ac0-4ec5-b711-f724e92439b0"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 2\n","batch_size = 512\n","num_epochs = len(y_train) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train, y_train)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=512, shuffle=True)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/307], Step [100/308], Loss: 0.3804796636104584\n","Epoch [1/307], Step [200/308], Loss: 0.34118932485580444\n","Epoch [1/307], Step [300/308], Loss: 0.2741008400917053\n","Epoch [2/307], Step [100/308], Loss: 0.2053924947977066\n","Epoch [2/307], Step [200/308], Loss: 0.16862598061561584\n","Epoch [2/307], Step [300/308], Loss: 0.1630077064037323\n","Epoch [3/307], Step [100/308], Loss: 0.14090394973754883\n","Epoch [3/307], Step [200/308], Loss: 0.12961216270923615\n","Epoch [3/307], Step [300/308], Loss: 0.10931194573640823\n","Epoch [4/307], Step [100/308], Loss: 0.08132340759038925\n","Epoch [4/307], Step [200/308], Loss: 0.07448147237300873\n","Epoch [4/307], Step [300/308], Loss: 0.06636938452720642\n","Epoch [5/307], Step [100/308], Loss: 0.08204329758882523\n","Epoch [5/307], Step [200/308], Loss: 0.06890326738357544\n","Epoch [5/307], Step [300/308], Loss: 0.04920678585767746\n","Epoch [6/307], Step [100/308], Loss: 0.0635007843375206\n","Epoch [6/307], Step [200/308], Loss: 0.04735895246267319\n","Epoch [6/307], Step [300/308], Loss: 0.03447982668876648\n","Epoch [7/307], Step [100/308], Loss: 0.05678758770227432\n","Epoch [7/307], Step [200/308], Loss: 0.035758256912231445\n","Epoch [7/307], Step [300/308], Loss: 0.02458455041050911\n","Epoch [8/307], Step [100/308], Loss: 0.015172148123383522\n","Epoch [8/307], Step [200/308], Loss: 0.019815081730484962\n","Epoch [8/307], Step [300/308], Loss: 0.03919865936040878\n","Epoch [9/307], Step [100/308], Loss: 0.039190929383039474\n","Epoch [9/307], Step [200/308], Loss: 0.027991997078061104\n","Epoch [9/307], Step [300/308], Loss: 0.019211383536458015\n","Epoch [10/307], Step [100/308], Loss: 0.014114399440586567\n","Epoch [10/307], Step [200/308], Loss: 0.02150576002895832\n","Epoch [10/307], Step [300/308], Loss: 0.027826085686683655\n","Epoch [11/307], Step [100/308], Loss: 0.0373084656894207\n","Epoch [11/307], Step [200/308], Loss: 0.025234485045075417\n","Epoch [11/307], Step [300/308], Loss: 0.019025055691599846\n","Epoch [12/307], Step [100/308], Loss: 0.01376794371753931\n","Epoch [12/307], Step [200/308], Loss: 0.007009263150393963\n","Epoch [12/307], Step [300/308], Loss: 0.02698921225965023\n","Epoch [13/307], Step [100/308], Loss: 0.010627519339323044\n","Epoch [13/307], Step [200/308], Loss: 0.004703194368630648\n","Epoch [13/307], Step [300/308], Loss: 0.026103107258677483\n","Epoch [14/307], Step [100/308], Loss: 0.014620169065892696\n","Epoch [14/307], Step [200/308], Loss: 0.015838535502552986\n","Epoch [14/307], Step [300/308], Loss: 0.025677135214209557\n","Epoch [15/307], Step [100/308], Loss: 0.034074150025844574\n","Epoch [15/307], Step [200/308], Loss: 0.013476102612912655\n","Epoch [15/307], Step [300/308], Loss: 0.0062456028535962105\n","Epoch [16/307], Step [100/308], Loss: 0.017321191728115082\n","Epoch [16/307], Step [200/308], Loss: 0.005258295219391584\n","Epoch [16/307], Step [300/308], Loss: 0.007526300381869078\n","Epoch [17/307], Step [100/308], Loss: 0.02064477652311325\n","Epoch [17/307], Step [200/308], Loss: 0.01671670377254486\n","Epoch [17/307], Step [300/308], Loss: 0.011529389768838882\n","Epoch [18/307], Step [100/308], Loss: 0.014799202792346478\n","Epoch [18/307], Step [200/308], Loss: 0.021161122247576714\n","Epoch [18/307], Step [300/308], Loss: 0.011737344786524773\n","Epoch [19/307], Step [100/308], Loss: 0.009872888214886189\n","Epoch [19/307], Step [200/308], Loss: 0.014700554311275482\n","Epoch [19/307], Step [300/308], Loss: 0.019959935918450356\n","Epoch [20/307], Step [100/308], Loss: 0.016285594552755356\n","Epoch [20/307], Step [200/308], Loss: 0.0056326547637581825\n","Epoch [20/307], Step [300/308], Loss: 0.015226651914417744\n","Epoch [21/307], Step [100/308], Loss: 0.005688492674380541\n","Epoch [21/307], Step [200/308], Loss: 0.017281001433730125\n","Epoch [21/307], Step [300/308], Loss: 0.01871269755065441\n","Epoch [22/307], Step [100/308], Loss: 0.0023841538932174444\n","Epoch [22/307], Step [200/308], Loss: 0.005081286188215017\n","Epoch [22/307], Step [300/308], Loss: 0.019065536558628082\n","Epoch [23/307], Step [100/308], Loss: 0.023236025124788284\n","Epoch [23/307], Step [200/308], Loss: 0.015955783426761627\n","Epoch [23/307], Step [300/308], Loss: 0.005083627533167601\n","Epoch [24/307], Step [100/308], Loss: 0.006292708683758974\n","Epoch [24/307], Step [200/308], Loss: 0.005440991837531328\n","Epoch [24/307], Step [300/308], Loss: 0.01800035499036312\n","Epoch [25/307], Step [100/308], Loss: 0.022274984046816826\n","Epoch [25/307], Step [200/308], Loss: 0.021697016432881355\n","Epoch [25/307], Step [300/308], Loss: 0.011471539735794067\n","Epoch [26/307], Step [100/308], Loss: 0.013413488864898682\n","Epoch [26/307], Step [200/308], Loss: 0.007172596175223589\n","Epoch [26/307], Step [300/308], Loss: 0.004392710514366627\n","Epoch [27/307], Step [100/308], Loss: 0.00938167329877615\n","Epoch [27/307], Step [200/308], Loss: 0.01756007969379425\n","Epoch [27/307], Step [300/308], Loss: 0.02707039564847946\n","Epoch [28/307], Step [100/308], Loss: 0.008375334553420544\n","Epoch [28/307], Step [200/308], Loss: 0.013866993598639965\n","Epoch [28/307], Step [300/308], Loss: 0.008195243775844574\n","Epoch [29/307], Step [100/308], Loss: 0.016374977305531502\n","Epoch [29/307], Step [200/308], Loss: 0.005217410158365965\n","Epoch [29/307], Step [300/308], Loss: 0.009429636411368847\n","Epoch [30/307], Step [100/308], Loss: 0.004666410852223635\n","Epoch [30/307], Step [200/308], Loss: 0.023454509675502777\n","Epoch [30/307], Step [300/308], Loss: 0.013873626478016376\n","Epoch [31/307], Step [100/308], Loss: 0.005896249320358038\n","Epoch [31/307], Step [200/308], Loss: 0.007497732527554035\n","Epoch [31/307], Step [300/308], Loss: 0.01768266037106514\n","Epoch [32/307], Step [100/308], Loss: 0.004477520473301411\n","Epoch [32/307], Step [200/308], Loss: 0.014636528678238392\n","Epoch [32/307], Step [300/308], Loss: 0.015688402578234673\n","Epoch [33/307], Step [100/308], Loss: 0.009661665186285973\n","Epoch [33/307], Step [200/308], Loss: 0.010535859502851963\n","Epoch [33/307], Step [300/308], Loss: 0.0022724028676748276\n","Epoch [34/307], Step [100/308], Loss: 0.004967277403920889\n","Epoch [34/307], Step [200/308], Loss: 0.0018906237091869116\n","Epoch [34/307], Step [300/308], Loss: 0.0025638872757554054\n","Epoch [35/307], Step [100/308], Loss: 0.004059290047734976\n","Epoch [35/307], Step [200/308], Loss: 0.006222388707101345\n","Epoch [35/307], Step [300/308], Loss: 0.005629857536405325\n","Epoch [36/307], Step [100/308], Loss: 0.005375523120164871\n","Epoch [36/307], Step [200/308], Loss: 0.006623967085033655\n","Epoch [36/307], Step [300/308], Loss: 0.015528520569205284\n","Epoch [37/307], Step [100/308], Loss: 0.018239857628941536\n","Epoch [37/307], Step [200/308], Loss: 0.01342085748910904\n","Epoch [37/307], Step [300/308], Loss: 0.006684204563498497\n","Epoch [38/307], Step [100/308], Loss: 0.003779663937166333\n","Epoch [38/307], Step [200/308], Loss: 0.008747551590204239\n","Epoch [38/307], Step [300/308], Loss: 0.005811549257487059\n","Epoch [39/307], Step [100/308], Loss: 0.00486300652846694\n","Epoch [39/307], Step [200/308], Loss: 0.009276055730879307\n","Epoch [39/307], Step [300/308], Loss: 0.006650060415267944\n","Epoch [40/307], Step [100/308], Loss: 0.006868504453450441\n","Epoch [40/307], Step [200/308], Loss: 0.006769049912691116\n","Epoch [40/307], Step [300/308], Loss: 0.007481621112674475\n","Epoch [41/307], Step [100/308], Loss: 0.00947621464729309\n","Epoch [41/307], Step [200/308], Loss: 0.030088283121585846\n","Epoch [41/307], Step [300/308], Loss: 0.014970725402235985\n","Epoch [42/307], Step [100/308], Loss: 0.016460968181490898\n","Epoch [42/307], Step [200/308], Loss: 0.0036290097050368786\n","Epoch [42/307], Step [300/308], Loss: 0.014890284277498722\n","Epoch [43/307], Step [100/308], Loss: 0.02510901913046837\n","Epoch [43/307], Step [200/308], Loss: 0.015781190246343613\n","Epoch [43/307], Step [300/308], Loss: 0.01759045012295246\n","Epoch [44/307], Step [100/308], Loss: 0.006032979115843773\n","Epoch [44/307], Step [200/308], Loss: 0.03632465377449989\n","Epoch [44/307], Step [300/308], Loss: 0.0067553143016994\n","Epoch [45/307], Step [100/308], Loss: 0.012297756038606167\n","Epoch [45/307], Step [200/308], Loss: 0.011756949126720428\n","Epoch [45/307], Step [300/308], Loss: 0.006479939911514521\n","Epoch [46/307], Step [100/308], Loss: 0.005802073981612921\n","Epoch [46/307], Step [200/308], Loss: 0.0022820807062089443\n","Epoch [46/307], Step [300/308], Loss: 0.028618985787034035\n","Epoch [47/307], Step [100/308], Loss: 0.005303871352225542\n","Epoch [47/307], Step [200/308], Loss: 0.0022815195843577385\n","Epoch [47/307], Step [300/308], Loss: 0.015402590855956078\n","Epoch [48/307], Step [100/308], Loss: 0.010124552994966507\n","Epoch [48/307], Step [200/308], Loss: 0.004232138395309448\n","Epoch [48/307], Step [300/308], Loss: 0.023670149967074394\n","Epoch [49/307], Step [100/308], Loss: 0.005166665650904179\n","Epoch [49/307], Step [200/308], Loss: 0.004625851754099131\n","Epoch [49/307], Step [300/308], Loss: 0.02040005661547184\n","Epoch [50/307], Step [100/308], Loss: 0.027872130274772644\n","Epoch [50/307], Step [200/308], Loss: 0.00182319525629282\n","Epoch [50/307], Step [300/308], Loss: 0.023997250944375992\n","Epoch [51/307], Step [100/308], Loss: 0.021548409014940262\n","Epoch [51/307], Step [200/308], Loss: 0.010138893499970436\n","Epoch [51/307], Step [300/308], Loss: 0.019533857703208923\n","Epoch [52/307], Step [100/308], Loss: 0.004819020628929138\n","Epoch [52/307], Step [200/308], Loss: 0.011150090023875237\n","Epoch [52/307], Step [300/308], Loss: 0.002840520581230521\n","Epoch [53/307], Step [100/308], Loss: 0.008960158564150333\n","Epoch [53/307], Step [200/308], Loss: 0.014726027846336365\n","Epoch [53/307], Step [300/308], Loss: 0.002486424520611763\n","Epoch [54/307], Step [100/308], Loss: 0.009372998028993607\n","Epoch [54/307], Step [200/308], Loss: 0.00466705160215497\n","Epoch [54/307], Step [300/308], Loss: 0.009689511731266975\n","Epoch [55/307], Step [100/308], Loss: 0.0037385751493275166\n","Epoch [55/307], Step [200/308], Loss: 0.007866022177040577\n","Epoch [55/307], Step [300/308], Loss: 0.005465131253004074\n","Epoch [56/307], Step [100/308], Loss: 0.010254031047224998\n","Epoch [56/307], Step [200/308], Loss: 0.0037543128710240126\n","Epoch [56/307], Step [300/308], Loss: 0.0022273529320955276\n","Epoch [57/307], Step [100/308], Loss: 0.003994898870587349\n","Epoch [57/307], Step [200/308], Loss: 0.011893825605511665\n","Epoch [57/307], Step [300/308], Loss: 0.00104261445812881\n","Epoch [58/307], Step [100/308], Loss: 0.005360227543860674\n","Epoch [58/307], Step [200/308], Loss: 0.01956123672425747\n","Epoch [58/307], Step [300/308], Loss: 0.001652987441048026\n","Epoch [59/307], Step [100/308], Loss: 0.009657137095928192\n","Epoch [59/307], Step [200/308], Loss: 0.006420984864234924\n","Epoch [59/307], Step [300/308], Loss: 0.013081114739179611\n","Epoch [60/307], Step [100/308], Loss: 0.0013030971167609096\n","Epoch [60/307], Step [200/308], Loss: 0.007626727689057589\n","Epoch [60/307], Step [300/308], Loss: 0.016819244250655174\n","Epoch [61/307], Step [100/308], Loss: 0.008532633073627949\n","Epoch [61/307], Step [200/308], Loss: 0.003974115010350943\n","Epoch [61/307], Step [300/308], Loss: 0.02394852787256241\n","Epoch [62/307], Step [100/308], Loss: 0.0016587931895628572\n","Epoch [62/307], Step [200/308], Loss: 0.006401726044714451\n","Epoch [62/307], Step [300/308], Loss: 0.014525831677019596\n","Epoch [63/307], Step [100/308], Loss: 0.003555148607119918\n","Epoch [63/307], Step [200/308], Loss: 0.013204334303736687\n","Epoch [63/307], Step [300/308], Loss: 0.023987524211406708\n","Epoch [64/307], Step [100/308], Loss: 0.014939878135919571\n","Epoch [64/307], Step [200/308], Loss: 0.02176128700375557\n","Epoch [64/307], Step [300/308], Loss: 0.027607591822743416\n","Epoch [65/307], Step [100/308], Loss: 0.005704283248633146\n","Epoch [65/307], Step [200/308], Loss: 0.014003866352140903\n","Epoch [65/307], Step [300/308], Loss: 0.0014266096986830235\n","Epoch [66/307], Step [100/308], Loss: 0.019190596416592598\n","Epoch [66/307], Step [200/308], Loss: 0.015343514271080494\n","Epoch [66/307], Step [300/308], Loss: 0.007106258068233728\n","Epoch [67/307], Step [100/308], Loss: 0.01910361647605896\n","Epoch [67/307], Step [200/308], Loss: 0.002698335563763976\n","Epoch [67/307], Step [300/308], Loss: 0.006659823935478926\n","Epoch [68/307], Step [100/308], Loss: 0.010210294276475906\n","Epoch [68/307], Step [200/308], Loss: 0.004920543637126684\n","Epoch [68/307], Step [300/308], Loss: 0.0069017731584608555\n","Epoch [69/307], Step [100/308], Loss: 0.016647210344672203\n","Epoch [69/307], Step [200/308], Loss: 0.0047990260645747185\n","Epoch [69/307], Step [300/308], Loss: 0.019976545125246048\n","Epoch [70/307], Step [100/308], Loss: 0.017463482916355133\n","Epoch [70/307], Step [200/308], Loss: 0.0067045981995761395\n","Epoch [70/307], Step [300/308], Loss: 0.013431674800813198\n","Epoch [71/307], Step [100/308], Loss: 0.011236726306378841\n","Epoch [71/307], Step [200/308], Loss: 0.010690586641430855\n","Epoch [71/307], Step [300/308], Loss: 0.016406813636422157\n","Epoch [72/307], Step [100/308], Loss: 0.004792428109794855\n","Epoch [72/307], Step [200/308], Loss: 0.0025120913051068783\n","Epoch [72/307], Step [300/308], Loss: 0.004091938026249409\n","Epoch [73/307], Step [100/308], Loss: 0.005009243730455637\n","Epoch [73/307], Step [200/308], Loss: 0.0038730695378035307\n","Epoch [73/307], Step [300/308], Loss: 0.009218445047736168\n","Epoch [74/307], Step [100/308], Loss: 0.008802666328847408\n","Epoch [74/307], Step [200/308], Loss: 0.008928278461098671\n","Epoch [74/307], Step [300/308], Loss: 0.03911823034286499\n","Epoch [75/307], Step [100/308], Loss: 0.010472748428583145\n","Epoch [75/307], Step [200/308], Loss: 0.023738613352179527\n","Epoch [75/307], Step [300/308], Loss: 0.02111874707043171\n","Epoch [76/307], Step [100/308], Loss: 0.005199737846851349\n","Epoch [76/307], Step [200/308], Loss: 0.01565057970583439\n","Epoch [76/307], Step [300/308], Loss: 0.006793757900595665\n","Epoch [77/307], Step [100/308], Loss: 0.0070067173801362514\n","Epoch [77/307], Step [200/308], Loss: 0.006240489427000284\n","Epoch [77/307], Step [300/308], Loss: 0.004437755327671766\n","Epoch [78/307], Step [100/308], Loss: 0.0031452809926122427\n","Epoch [78/307], Step [200/308], Loss: 0.01442366000264883\n","Epoch [78/307], Step [300/308], Loss: 0.006333090364933014\n","Epoch [79/307], Step [100/308], Loss: 0.009695185348391533\n","Epoch [79/307], Step [200/308], Loss: 0.0019560502842068672\n","Epoch [79/307], Step [300/308], Loss: 0.007852910086512566\n","Epoch [80/307], Step [100/308], Loss: 0.005340711213648319\n","Epoch [80/307], Step [200/308], Loss: 0.007573849055916071\n","Epoch [80/307], Step [300/308], Loss: 0.028403831645846367\n","Epoch [81/307], Step [100/308], Loss: 0.007008748594671488\n","Epoch [81/307], Step [200/308], Loss: 0.001505913445726037\n","Epoch [81/307], Step [300/308], Loss: 0.006588195916265249\n","Epoch [82/307], Step [100/308], Loss: 0.001858054893091321\n","Epoch [82/307], Step [200/308], Loss: 0.014300956390798092\n","Epoch [82/307], Step [300/308], Loss: 0.0048987953923642635\n","Epoch [83/307], Step [100/308], Loss: 0.014563681557774544\n","Epoch [83/307], Step [200/308], Loss: 0.02351228892803192\n","Epoch [83/307], Step [300/308], Loss: 0.004395377356559038\n","Epoch [84/307], Step [100/308], Loss: 0.018845701590180397\n","Epoch [84/307], Step [200/308], Loss: 0.001694684848189354\n","Epoch [84/307], Step [300/308], Loss: 0.0191238671541214\n","Epoch [85/307], Step [100/308], Loss: 0.011028002016246319\n","Epoch [85/307], Step [200/308], Loss: 0.003785940818488598\n","Epoch [85/307], Step [300/308], Loss: 0.01934531331062317\n","Epoch [86/307], Step [100/308], Loss: 0.009453159756958485\n","Epoch [86/307], Step [200/308], Loss: 0.0077206408604979515\n","Epoch [86/307], Step [300/308], Loss: 0.006649582181125879\n","Epoch [87/307], Step [100/308], Loss: 0.01723562180995941\n","Epoch [87/307], Step [200/308], Loss: 0.021223075687885284\n","Epoch [87/307], Step [300/308], Loss: 0.004940419923514128\n","Epoch [88/307], Step [100/308], Loss: 0.027916736900806427\n","Epoch [88/307], Step [200/308], Loss: 0.003702871035784483\n","Epoch [88/307], Step [300/308], Loss: 0.003104056930169463\n","Epoch [89/307], Step [100/308], Loss: 0.023426704108715057\n","Epoch [89/307], Step [200/308], Loss: 0.015617456287145615\n","Epoch [89/307], Step [300/308], Loss: 0.01484178751707077\n","Epoch [90/307], Step [100/308], Loss: 0.004075935110449791\n","Epoch [90/307], Step [200/308], Loss: 0.012420598417520523\n","Epoch [90/307], Step [300/308], Loss: 0.009514352306723595\n","Epoch [91/307], Step [100/308], Loss: 0.02479221299290657\n","Epoch [91/307], Step [200/308], Loss: 0.005982684437185526\n","Epoch [91/307], Step [300/308], Loss: 0.005770821589976549\n","Epoch [92/307], Step [100/308], Loss: 0.005490259267389774\n","Epoch [92/307], Step [200/308], Loss: 0.012134702876210213\n","Epoch [92/307], Step [300/308], Loss: 0.012210901826620102\n","Epoch [93/307], Step [100/308], Loss: 0.007927813567221165\n","Epoch [93/307], Step [200/308], Loss: 0.005550393369048834\n","Epoch [93/307], Step [300/308], Loss: 0.002380205551162362\n","Epoch [94/307], Step [100/308], Loss: 0.006689157336950302\n","Epoch [94/307], Step [200/308], Loss: 0.0031890266109257936\n","Epoch [94/307], Step [300/308], Loss: 0.0036105953622609377\n","Epoch [95/307], Step [100/308], Loss: 0.002610760508105159\n","Epoch [95/307], Step [200/308], Loss: 0.029293520376086235\n","Epoch [95/307], Step [300/308], Loss: 0.011101880110800266\n","Epoch [96/307], Step [100/308], Loss: 0.005279151722788811\n","Epoch [96/307], Step [200/308], Loss: 0.016662299633026123\n","Epoch [96/307], Step [300/308], Loss: 0.019005186855793\n","Epoch [97/307], Step [100/308], Loss: 0.005468824412673712\n","Epoch [97/307], Step [200/308], Loss: 0.007178302854299545\n","Epoch [97/307], Step [300/308], Loss: 0.06620697677135468\n","Epoch [98/307], Step [100/308], Loss: 0.019083386287093163\n","Epoch [98/307], Step [200/308], Loss: 0.025759531185030937\n","Epoch [98/307], Step [300/308], Loss: 0.05586201325058937\n","Epoch [99/307], Step [100/308], Loss: 0.005719735287129879\n","Epoch [99/307], Step [200/308], Loss: 0.003480430692434311\n","Epoch [99/307], Step [300/308], Loss: 0.015338773839175701\n","Epoch [100/307], Step [100/308], Loss: 0.006274771876633167\n","Epoch [100/307], Step [200/308], Loss: 0.025715148076415062\n","Epoch [100/307], Step [300/308], Loss: 0.005059818737208843\n","Epoch [101/307], Step [100/308], Loss: 0.016638917848467827\n","Epoch [101/307], Step [200/308], Loss: 0.0032921708188951015\n","Epoch [101/307], Step [300/308], Loss: 0.02654085122048855\n","Epoch [102/307], Step [100/308], Loss: 0.012972253374755383\n","Epoch [102/307], Step [200/308], Loss: 0.00894143432378769\n","Epoch [102/307], Step [300/308], Loss: 0.00941526796668768\n","Epoch [103/307], Step [100/308], Loss: 0.009219911880791187\n","Epoch [103/307], Step [200/308], Loss: 0.004237643908709288\n","Epoch [103/307], Step [300/308], Loss: 0.014646447263658047\n","Epoch [104/307], Step [100/308], Loss: 0.008061463013291359\n","Epoch [104/307], Step [200/308], Loss: 0.023288195952773094\n","Epoch [104/307], Step [300/308], Loss: 0.0053533692844212055\n","Epoch [105/307], Step [100/308], Loss: 0.0029440540820360184\n","Epoch [105/307], Step [200/308], Loss: 0.02984786033630371\n","Epoch [105/307], Step [300/308], Loss: 0.005952242761850357\n","Epoch [106/307], Step [100/308], Loss: 0.007145602721720934\n","Epoch [106/307], Step [200/308], Loss: 0.011376156471669674\n","Epoch [106/307], Step [300/308], Loss: 0.003908962476998568\n","Epoch [107/307], Step [100/308], Loss: 0.001922392169944942\n","Epoch [107/307], Step [200/308], Loss: 0.02837323024868965\n","Epoch [107/307], Step [300/308], Loss: 0.009020884521305561\n","Epoch [108/307], Step [100/308], Loss: 0.022967424243688583\n","Epoch [108/307], Step [200/308], Loss: 0.0011938842944800854\n","Epoch [108/307], Step [300/308], Loss: 0.005131828598678112\n","Epoch [109/307], Step [100/308], Loss: 0.005700114648789167\n","Epoch [109/307], Step [200/308], Loss: 0.008513919077813625\n","Epoch [109/307], Step [300/308], Loss: 0.0014733675634488463\n","Epoch [110/307], Step [100/308], Loss: 0.014815899543464184\n","Epoch [110/307], Step [200/308], Loss: 0.004202863667160273\n","Epoch [110/307], Step [300/308], Loss: 0.01823592185974121\n","Epoch [111/307], Step [100/308], Loss: 0.02615582011640072\n","Epoch [111/307], Step [200/308], Loss: 0.01058289036154747\n","Epoch [111/307], Step [300/308], Loss: 0.015144235454499722\n","Epoch [112/307], Step [100/308], Loss: 0.005476940888911486\n","Epoch [112/307], Step [200/308], Loss: 0.004832718521356583\n","Epoch [112/307], Step [300/308], Loss: 0.01977997086942196\n","Epoch [113/307], Step [100/308], Loss: 0.010723342187702656\n","Epoch [113/307], Step [200/308], Loss: 0.0037041776813566685\n","Epoch [113/307], Step [300/308], Loss: 0.01767328940331936\n","Epoch [114/307], Step [100/308], Loss: 0.00565261859446764\n","Epoch [114/307], Step [200/308], Loss: 0.0322999432682991\n","Epoch [114/307], Step [300/308], Loss: 0.019094061106443405\n","Epoch [115/307], Step [100/308], Loss: 0.013642018660902977\n","Epoch [115/307], Step [200/308], Loss: 0.0007628178573213518\n","Epoch [115/307], Step [300/308], Loss: 0.03196220099925995\n","Epoch [116/307], Step [100/308], Loss: 0.006229087244719267\n","Epoch [116/307], Step [200/308], Loss: 0.016181722283363342\n","Epoch [116/307], Step [300/308], Loss: 0.004156189505010843\n","Epoch [117/307], Step [100/308], Loss: 0.011260824277997017\n","Epoch [117/307], Step [200/308], Loss: 0.01263556070625782\n","Epoch [117/307], Step [300/308], Loss: 0.005039696116000414\n","Epoch [118/307], Step [100/308], Loss: 0.02177492529153824\n","Epoch [118/307], Step [200/308], Loss: 0.01094087865203619\n","Epoch [118/307], Step [300/308], Loss: 0.008172075264155865\n","Epoch [119/307], Step [100/308], Loss: 0.01652834378182888\n","Epoch [119/307], Step [200/308], Loss: 0.003008741419762373\n","Epoch [119/307], Step [300/308], Loss: 0.004702414385974407\n","Epoch [120/307], Step [100/308], Loss: 0.0047041550278663635\n","Epoch [120/307], Step [200/308], Loss: 0.022358745336532593\n","Epoch [120/307], Step [300/308], Loss: 0.013406342826783657\n","Epoch [121/307], Step [100/308], Loss: 0.027619661763310432\n","Epoch [121/307], Step [200/308], Loss: 0.006531404796987772\n","Epoch [121/307], Step [300/308], Loss: 0.0031714735087007284\n","Epoch [122/307], Step [100/308], Loss: 0.03320286050438881\n","Epoch [122/307], Step [200/308], Loss: 0.005007678177207708\n","Epoch [122/307], Step [300/308], Loss: 0.008150115609169006\n","Epoch [123/307], Step [100/308], Loss: 0.0026505659334361553\n","Epoch [123/307], Step [200/308], Loss: 0.002532889135181904\n","Epoch [123/307], Step [300/308], Loss: 0.010045783594250679\n","Epoch [124/307], Step [100/308], Loss: 0.00660476041957736\n","Epoch [124/307], Step [200/308], Loss: 0.004938121419399977\n","Epoch [124/307], Step [300/308], Loss: 0.030394859611988068\n","Epoch [125/307], Step [100/308], Loss: 0.00914356205612421\n","Epoch [125/307], Step [200/308], Loss: 0.012930629774928093\n","Epoch [125/307], Step [300/308], Loss: 0.0072799138724803925\n","Epoch [126/307], Step [100/308], Loss: 0.018226394429802895\n","Epoch [126/307], Step [200/308], Loss: 0.009243451058864594\n","Epoch [126/307], Step [300/308], Loss: 0.03403286635875702\n","Epoch [127/307], Step [100/308], Loss: 0.0023206130135804415\n","Epoch [127/307], Step [200/308], Loss: 0.015464568510651588\n","Epoch [127/307], Step [300/308], Loss: 0.018733879551291466\n","Epoch [128/307], Step [100/308], Loss: 0.0028810661751776934\n","Epoch [128/307], Step [200/308], Loss: 0.017338745296001434\n","Epoch [128/307], Step [300/308], Loss: 0.0035026909317821264\n","Epoch [129/307], Step [100/308], Loss: 0.0013160018716007471\n","Epoch [129/307], Step [200/308], Loss: 0.01176472008228302\n","Epoch [129/307], Step [300/308], Loss: 0.012649822980165482\n","Epoch [130/307], Step [100/308], Loss: 0.00557317677885294\n","Epoch [130/307], Step [200/308], Loss: 0.01198335736989975\n","Epoch [130/307], Step [300/308], Loss: 0.014338500797748566\n","Epoch [131/307], Step [100/308], Loss: 0.007477441802620888\n","Epoch [131/307], Step [200/308], Loss: 0.014400743879377842\n","Epoch [131/307], Step [300/308], Loss: 0.016518305987119675\n","Epoch [132/307], Step [100/308], Loss: 0.01054046768695116\n","Epoch [132/307], Step [200/308], Loss: 0.006915499456226826\n","Epoch [132/307], Step [300/308], Loss: 0.009284124709665775\n","Epoch [133/307], Step [100/308], Loss: 0.02380145713686943\n","Epoch [133/307], Step [200/308], Loss: 0.022742033004760742\n","Epoch [133/307], Step [300/308], Loss: 0.001204417203553021\n","Epoch [134/307], Step [100/308], Loss: 0.0050148433074355125\n","Epoch [134/307], Step [200/308], Loss: 0.011032141745090485\n","Epoch [134/307], Step [300/308], Loss: 0.005098992493003607\n","Epoch [135/307], Step [100/308], Loss: 0.01342085562646389\n","Epoch [135/307], Step [200/308], Loss: 0.00191327219363302\n","Epoch [135/307], Step [300/308], Loss: 0.010820841416716576\n","Epoch [136/307], Step [100/308], Loss: 0.005296977236866951\n","Epoch [136/307], Step [200/308], Loss: 0.018183574080467224\n","Epoch [136/307], Step [300/308], Loss: 0.002403086517006159\n","Epoch [137/307], Step [100/308], Loss: 0.014099475927650928\n","Epoch [137/307], Step [200/308], Loss: 0.02321852557361126\n","Epoch [137/307], Step [300/308], Loss: 0.001533033442683518\n","Epoch [138/307], Step [100/308], Loss: 0.005104761105030775\n","Epoch [138/307], Step [200/308], Loss: 0.0023195496760308743\n","Epoch [138/307], Step [300/308], Loss: 0.005193599499762058\n","Epoch [139/307], Step [100/308], Loss: 0.01523154228925705\n","Epoch [139/307], Step [200/308], Loss: 0.002872415352612734\n","Epoch [139/307], Step [300/308], Loss: 0.02774251252412796\n","Epoch [140/307], Step [100/308], Loss: 0.002390996553003788\n","Epoch [140/307], Step [200/308], Loss: 0.013376815244555473\n","Epoch [140/307], Step [300/308], Loss: 0.01104783359915018\n","Epoch [141/307], Step [100/308], Loss: 0.005407620687037706\n","Epoch [141/307], Step [200/308], Loss: 0.0128147779032588\n","Epoch [141/307], Step [300/308], Loss: 0.014078797772526741\n","Epoch [142/307], Step [100/308], Loss: 0.020971059799194336\n","Epoch [142/307], Step [200/308], Loss: 0.007172257639467716\n","Epoch [142/307], Step [300/308], Loss: 0.031872283667325974\n","Epoch [143/307], Step [100/308], Loss: 0.025334512814879417\n","Epoch [143/307], Step [200/308], Loss: 0.0064854929223656654\n","Epoch [143/307], Step [300/308], Loss: 0.015757111832499504\n","Epoch [144/307], Step [100/308], Loss: 0.003785316366702318\n","Epoch [144/307], Step [200/308], Loss: 0.007081678602844477\n","Epoch [144/307], Step [300/308], Loss: 0.0071516879834234715\n","Epoch [145/307], Step [100/308], Loss: 0.009008429944515228\n","Epoch [145/307], Step [200/308], Loss: 0.029422063380479813\n","Epoch [145/307], Step [300/308], Loss: 0.00938118901103735\n","Epoch [146/307], Step [100/308], Loss: 0.009146668016910553\n","Epoch [146/307], Step [200/308], Loss: 0.001480292878113687\n","Epoch [146/307], Step [300/308], Loss: 0.006998040713369846\n","Epoch [147/307], Step [100/308], Loss: 0.004788688849657774\n","Epoch [147/307], Step [200/308], Loss: 0.023311346769332886\n","Epoch [147/307], Step [300/308], Loss: 0.00568636879324913\n","Epoch [148/307], Step [100/308], Loss: 0.010188322514295578\n","Epoch [148/307], Step [200/308], Loss: 0.013457837514579296\n","Epoch [148/307], Step [300/308], Loss: 0.0139232836663723\n","Epoch [149/307], Step [100/308], Loss: 0.01088118925690651\n","Epoch [149/307], Step [200/308], Loss: 0.00526059465482831\n","Epoch [149/307], Step [300/308], Loss: 0.01455729454755783\n","Epoch [150/307], Step [100/308], Loss: 0.008109075017273426\n","Epoch [150/307], Step [200/308], Loss: 0.009081711061298847\n","Epoch [150/307], Step [300/308], Loss: 0.015296872705221176\n","Epoch [151/307], Step [100/308], Loss: 0.005933466367423534\n","Epoch [151/307], Step [200/308], Loss: 0.0067824204452335835\n","Epoch [151/307], Step [300/308], Loss: 0.024378541857004166\n","Epoch [152/307], Step [100/308], Loss: 0.003576027462258935\n","Epoch [152/307], Step [200/308], Loss: 0.006545964162796736\n","Epoch [152/307], Step [300/308], Loss: 0.0034809575881808996\n","Epoch [153/307], Step [100/308], Loss: 0.013518675230443478\n","Epoch [153/307], Step [200/308], Loss: 0.009433848783373833\n","Epoch [153/307], Step [300/308], Loss: 0.004500541836023331\n","Epoch [154/307], Step [100/308], Loss: 0.0053734974935650826\n","Epoch [154/307], Step [200/308], Loss: 0.01113835722208023\n","Epoch [154/307], Step [300/308], Loss: 0.011979707516729832\n","Epoch [155/307], Step [100/308], Loss: 0.024236412718892097\n","Epoch [155/307], Step [200/308], Loss: 0.005403376650065184\n","Epoch [155/307], Step [300/308], Loss: 0.00869699101895094\n","Epoch [156/307], Step [100/308], Loss: 0.018976865336298943\n","Epoch [156/307], Step [200/308], Loss: 0.012500649318099022\n","Epoch [156/307], Step [300/308], Loss: 0.016108715906739235\n","Epoch [157/307], Step [100/308], Loss: 0.023252950981259346\n","Epoch [157/307], Step [200/308], Loss: 0.009366273880004883\n","Epoch [157/307], Step [300/308], Loss: 0.01545723993331194\n","Epoch [158/307], Step [100/308], Loss: 0.004109217319637537\n","Epoch [158/307], Step [200/308], Loss: 0.01128232292830944\n","Epoch [158/307], Step [300/308], Loss: 0.09205213189125061\n","Epoch [159/307], Step [100/308], Loss: 0.015619681216776371\n","Epoch [159/307], Step [200/308], Loss: 0.005643064621835947\n","Epoch [159/307], Step [300/308], Loss: 0.011102036572992802\n","Epoch [160/307], Step [100/308], Loss: 0.006375554949045181\n","Epoch [160/307], Step [200/308], Loss: 0.006419404875487089\n","Epoch [160/307], Step [300/308], Loss: 0.010728802531957626\n","Epoch [161/307], Step [100/308], Loss: 0.014335603453218937\n","Epoch [161/307], Step [200/308], Loss: 0.004050991032272577\n","Epoch [161/307], Step [300/308], Loss: 0.00415374618023634\n","Epoch [162/307], Step [100/308], Loss: 0.00041280503501184285\n","Epoch [162/307], Step [200/308], Loss: 0.018070433288812637\n","Epoch [162/307], Step [300/308], Loss: 0.00716839823871851\n","Epoch [163/307], Step [100/308], Loss: 0.0027610925026237965\n","Epoch [163/307], Step [200/308], Loss: 0.013571275398135185\n","Epoch [163/307], Step [300/308], Loss: 0.020990759134292603\n","Epoch [164/307], Step [100/308], Loss: 0.01005999930202961\n","Epoch [164/307], Step [200/308], Loss: 0.015044720843434334\n","Epoch [164/307], Step [300/308], Loss: 0.01305953785777092\n","Epoch [165/307], Step [100/308], Loss: 0.004061627667397261\n","Epoch [165/307], Step [200/308], Loss: 0.0024795192293822765\n","Epoch [165/307], Step [300/308], Loss: 0.012388267554342747\n","Epoch [166/307], Step [100/308], Loss: 0.0029232241213321686\n","Epoch [166/307], Step [200/308], Loss: 0.01254289411008358\n","Epoch [166/307], Step [300/308], Loss: 0.010594133287668228\n","Epoch [167/307], Step [100/308], Loss: 0.0057413470931351185\n","Epoch [167/307], Step [200/308], Loss: 0.008286966942250729\n","Epoch [167/307], Step [300/308], Loss: 0.0009564652573317289\n","Epoch [168/307], Step [100/308], Loss: 0.008939722552895546\n","Epoch [168/307], Step [200/308], Loss: 0.003386335214599967\n","Epoch [168/307], Step [300/308], Loss: 0.007217581383883953\n","Epoch [169/307], Step [100/308], Loss: 0.003951143939048052\n","Epoch [169/307], Step [200/308], Loss: 0.001625657663680613\n","Epoch [169/307], Step [300/308], Loss: 0.000944115745369345\n","Epoch [170/307], Step [100/308], Loss: 0.015192884020507336\n","Epoch [170/307], Step [200/308], Loss: 0.00985244195908308\n","Epoch [170/307], Step [300/308], Loss: 0.0015611785929650068\n","Epoch [171/307], Step [100/308], Loss: 0.006968359928578138\n","Epoch [171/307], Step [200/308], Loss: 0.007164549548178911\n","Epoch [171/307], Step [300/308], Loss: 0.021001586690545082\n","Epoch [172/307], Step [100/308], Loss: 0.02224176749587059\n","Epoch [172/307], Step [200/308], Loss: 0.007006398867815733\n","Epoch [172/307], Step [300/308], Loss: 0.0037422634195536375\n","Epoch [173/307], Step [100/308], Loss: 0.00427772244438529\n","Epoch [173/307], Step [200/308], Loss: 0.007434818893671036\n","Epoch [173/307], Step [300/308], Loss: 0.011011210270226002\n","Epoch [174/307], Step [100/308], Loss: 0.014898751862347126\n","Epoch [174/307], Step [200/308], Loss: 0.011479460634291172\n","Epoch [174/307], Step [300/308], Loss: 0.012487407773733139\n","Epoch [175/307], Step [100/308], Loss: 0.00190758949611336\n","Epoch [175/307], Step [200/308], Loss: 0.006606544833630323\n","Epoch [175/307], Step [300/308], Loss: 0.001137439627200365\n","Epoch [176/307], Step [100/308], Loss: 0.0014122759457677603\n","Epoch [176/307], Step [200/308], Loss: 0.009327992796897888\n","Epoch [176/307], Step [300/308], Loss: 0.0013144140830263495\n","Epoch [177/307], Step [100/308], Loss: 0.0020573087967932224\n","Epoch [177/307], Step [200/308], Loss: 0.0019695544615387917\n","Epoch [177/307], Step [300/308], Loss: 0.0178403127938509\n","Epoch [178/307], Step [100/308], Loss: 0.008687274530529976\n","Epoch [178/307], Step [200/308], Loss: 0.008243146352469921\n","Epoch [178/307], Step [300/308], Loss: 0.004757283255457878\n","Epoch [179/307], Step [100/308], Loss: 0.0031021784525364637\n","Epoch [179/307], Step [200/308], Loss: 0.017441583797335625\n","Epoch [179/307], Step [300/308], Loss: 0.014362028799951077\n","Epoch [180/307], Step [100/308], Loss: 0.01763119362294674\n","Epoch [180/307], Step [200/308], Loss: 0.007445333059877157\n","Epoch [180/307], Step [300/308], Loss: 0.005292796529829502\n","Epoch [181/307], Step [100/308], Loss: 0.012769324705004692\n","Epoch [181/307], Step [200/308], Loss: 0.003317949827760458\n","Epoch [181/307], Step [300/308], Loss: 0.015963906422257423\n","Epoch [182/307], Step [100/308], Loss: 0.021840713918209076\n","Epoch [182/307], Step [200/308], Loss: 0.007288668304681778\n","Epoch [182/307], Step [300/308], Loss: 0.01186479814350605\n","Epoch [183/307], Step [100/308], Loss: 0.010427475906908512\n","Epoch [183/307], Step [200/308], Loss: 0.004860398825258017\n","Epoch [183/307], Step [300/308], Loss: 0.013899318873882294\n","Epoch [184/307], Step [100/308], Loss: 0.05079110711812973\n","Epoch [184/307], Step [200/308], Loss: 0.008727440610527992\n","Epoch [184/307], Step [300/308], Loss: 0.011933095753192902\n","Epoch [185/307], Step [100/308], Loss: 0.022293949499726295\n","Epoch [185/307], Step [200/308], Loss: 0.004190342966467142\n","Epoch [185/307], Step [300/308], Loss: 0.0021648374386131763\n","Epoch [186/307], Step [100/308], Loss: 0.010721073485910892\n","Epoch [186/307], Step [200/308], Loss: 0.0015571509720757604\n","Epoch [186/307], Step [300/308], Loss: 0.014214850030839443\n","Epoch [187/307], Step [100/308], Loss: 0.0026354084257036448\n","Epoch [187/307], Step [200/308], Loss: 0.00169572071172297\n","Epoch [187/307], Step [300/308], Loss: 0.021896446123719215\n","Epoch [188/307], Step [100/308], Loss: 0.010229835286736488\n","Epoch [188/307], Step [200/308], Loss: 0.0011078155366703868\n","Epoch [188/307], Step [300/308], Loss: 0.009211819618940353\n","Epoch [189/307], Step [100/308], Loss: 0.010833662934601307\n","Epoch [189/307], Step [200/308], Loss: 0.01713080331683159\n","Epoch [189/307], Step [300/308], Loss: 0.001897959504276514\n","Epoch [190/307], Step [100/308], Loss: 0.003495476907119155\n","Epoch [190/307], Step [200/308], Loss: 0.012454659678041935\n","Epoch [190/307], Step [300/308], Loss: 0.0009743932168930769\n","Epoch [191/307], Step [100/308], Loss: 0.0030500153079628944\n","Epoch [191/307], Step [200/308], Loss: 0.002166363410651684\n","Epoch [191/307], Step [300/308], Loss: 0.002381617436185479\n","Epoch [192/307], Step [100/308], Loss: 0.00238276575691998\n","Epoch [192/307], Step [200/308], Loss: 0.021467309445142746\n","Epoch [192/307], Step [300/308], Loss: 0.021054595708847046\n","Epoch [193/307], Step [100/308], Loss: 0.0008734667208045721\n","Epoch [193/307], Step [200/308], Loss: 0.002602317603304982\n","Epoch [193/307], Step [300/308], Loss: 0.012555981986224651\n","Epoch [194/307], Step [100/308], Loss: 0.002354875672608614\n","Epoch [194/307], Step [200/308], Loss: 0.00462963804602623\n","Epoch [194/307], Step [300/308], Loss: 0.01242823526263237\n","Epoch [195/307], Step [100/308], Loss: 0.020809421315789223\n","Epoch [195/307], Step [200/308], Loss: 0.013312075287103653\n","Epoch [195/307], Step [300/308], Loss: 0.012929332442581654\n","Epoch [196/307], Step [100/308], Loss: 0.0014645111514255404\n","Epoch [196/307], Step [200/308], Loss: 0.021095681935548782\n","Epoch [196/307], Step [300/308], Loss: 0.014632139354944229\n","Epoch [197/307], Step [100/308], Loss: 0.01143636368215084\n","Epoch [197/307], Step [200/308], Loss: 0.013101903721690178\n","Epoch [197/307], Step [300/308], Loss: 0.0031354220118373632\n","Epoch [198/307], Step [100/308], Loss: 0.006953028030693531\n","Epoch [198/307], Step [200/308], Loss: 0.0037906130310148\n","Epoch [198/307], Step [300/308], Loss: 0.006392777897417545\n","Epoch [199/307], Step [100/308], Loss: 0.0017234752885997295\n","Epoch [199/307], Step [200/308], Loss: 0.008794889785349369\n","Epoch [199/307], Step [300/308], Loss: 0.006965726148337126\n","Epoch [200/307], Step [100/308], Loss: 0.023466704413294792\n","Epoch [200/307], Step [200/308], Loss: 0.005457873456180096\n","Epoch [200/307], Step [300/308], Loss: 0.013015062548220158\n","Epoch [201/307], Step [100/308], Loss: 0.004377623554319143\n","Epoch [201/307], Step [200/308], Loss: 0.006624516565352678\n","Epoch [201/307], Step [300/308], Loss: 0.014585605822503567\n","Epoch [202/307], Step [100/308], Loss: 0.0011680542957037687\n","Epoch [202/307], Step [200/308], Loss: 0.008736494928598404\n","Epoch [202/307], Step [300/308], Loss: 0.021378133445978165\n","Epoch [203/307], Step [100/308], Loss: 0.0022578660864382982\n","Epoch [203/307], Step [200/308], Loss: 0.010027141310274601\n","Epoch [203/307], Step [300/308], Loss: 0.005935388617217541\n","Epoch [204/307], Step [100/308], Loss: 0.003206811845302582\n","Epoch [204/307], Step [200/308], Loss: 0.01563147082924843\n","Epoch [204/307], Step [300/308], Loss: 0.004472560249269009\n","Epoch [205/307], Step [100/308], Loss: 0.030991457402706146\n","Epoch [205/307], Step [200/308], Loss: 0.008524195291101933\n","Epoch [205/307], Step [300/308], Loss: 0.02079315297305584\n","Epoch [206/307], Step [100/308], Loss: 0.00424678809940815\n","Epoch [206/307], Step [200/308], Loss: 0.006397222634404898\n","Epoch [206/307], Step [300/308], Loss: 0.0070031690411269665\n","Epoch [207/307], Step [100/308], Loss: 0.003736387472599745\n","Epoch [207/307], Step [200/308], Loss: 0.009407605044543743\n","Epoch [207/307], Step [300/308], Loss: 0.0036542194429785013\n","Epoch [208/307], Step [100/308], Loss: 0.003482011379674077\n","Epoch [208/307], Step [200/308], Loss: 0.011267933994531631\n","Epoch [208/307], Step [300/308], Loss: 0.009927166625857353\n","Epoch [209/307], Step [100/308], Loss: 0.009896772913634777\n","Epoch [209/307], Step [200/308], Loss: 0.004742308985441923\n","Epoch [209/307], Step [300/308], Loss: 0.032670315355062485\n","Epoch [210/307], Step [100/308], Loss: 0.006516745779663324\n","Epoch [210/307], Step [200/308], Loss: 0.015089568682014942\n","Epoch [210/307], Step [300/308], Loss: 0.004500475246459246\n","Epoch [211/307], Step [100/308], Loss: 0.0009312456822954118\n","Epoch [211/307], Step [200/308], Loss: 0.016574522480368614\n","Epoch [211/307], Step [300/308], Loss: 0.006624237634241581\n","Epoch [212/307], Step [100/308], Loss: 0.0009778916137292981\n","Epoch [212/307], Step [200/308], Loss: 0.01178933959454298\n","Epoch [212/307], Step [300/308], Loss: 0.015797462314367294\n","Epoch [213/307], Step [100/308], Loss: 0.0016000804025679827\n","Epoch [213/307], Step [200/308], Loss: 0.015320881269872189\n","Epoch [213/307], Step [300/308], Loss: 0.009086907841265202\n","Epoch [214/307], Step [100/308], Loss: 0.0059543633833527565\n","Epoch [214/307], Step [200/308], Loss: 0.009738394990563393\n","Epoch [214/307], Step [300/308], Loss: 0.0009810971096158028\n","Epoch [215/307], Step [100/308], Loss: 0.07111362367868423\n","Epoch [215/307], Step [200/308], Loss: 0.00728866970166564\n","Epoch [215/307], Step [300/308], Loss: 0.0013126176781952381\n","Epoch [216/307], Step [100/308], Loss: 0.009178565815091133\n","Epoch [216/307], Step [200/308], Loss: 0.002793225459754467\n","Epoch [216/307], Step [300/308], Loss: 0.024181773886084557\n","Epoch [217/307], Step [100/308], Loss: 0.0008890257449820638\n","Epoch [217/307], Step [200/308], Loss: 0.001200366299599409\n","Epoch [217/307], Step [300/308], Loss: 0.0034414276015013456\n","Epoch [218/307], Step [100/308], Loss: 0.006620008498430252\n","Epoch [218/307], Step [200/308], Loss: 0.007668076083064079\n","Epoch [218/307], Step [300/308], Loss: 0.006996497046202421\n","Epoch [219/307], Step [100/308], Loss: 0.003166190581396222\n","Epoch [219/307], Step [200/308], Loss: 0.005657932721078396\n","Epoch [219/307], Step [300/308], Loss: 0.00304873613640666\n","Epoch [220/307], Step [100/308], Loss: 0.008367340080440044\n","Epoch [220/307], Step [200/308], Loss: 0.011876878328621387\n","Epoch [220/307], Step [300/308], Loss: 0.003937095403671265\n","Epoch [221/307], Step [100/308], Loss: 0.01302214153110981\n","Epoch [221/307], Step [200/308], Loss: 0.0009802397107705474\n","Epoch [221/307], Step [300/308], Loss: 0.0012376210652291775\n","Epoch [222/307], Step [100/308], Loss: 0.0013236766681075096\n","Epoch [222/307], Step [200/308], Loss: 0.002752381144091487\n","Epoch [222/307], Step [300/308], Loss: 0.0022673746570944786\n","Epoch [223/307], Step [100/308], Loss: 0.009202688932418823\n","Epoch [223/307], Step [200/308], Loss: 0.005470531526952982\n","Epoch [223/307], Step [300/308], Loss: 0.0005680514150299132\n","Epoch [224/307], Step [100/308], Loss: 0.018087178468704224\n","Epoch [224/307], Step [200/308], Loss: 0.006253822706639767\n","Epoch [224/307], Step [300/308], Loss: 0.0016012148698791862\n","Epoch [225/307], Step [100/308], Loss: 0.0027641206979751587\n","Epoch [225/307], Step [200/308], Loss: 0.0025787746999412775\n","Epoch [225/307], Step [300/308], Loss: 0.021650873124599457\n","Epoch [226/307], Step [100/308], Loss: 0.03018718957901001\n","Epoch [226/307], Step [200/308], Loss: 0.0030731898732483387\n","Epoch [226/307], Step [300/308], Loss: 0.008857385255396366\n","Epoch [227/307], Step [100/308], Loss: 0.004619344137609005\n","Epoch [227/307], Step [200/308], Loss: 0.0017923037521541119\n","Epoch [227/307], Step [300/308], Loss: 0.021644974127411842\n","Epoch [228/307], Step [100/308], Loss: 0.0007907823892310262\n","Epoch [228/307], Step [200/308], Loss: 0.0013604570413008332\n","Epoch [228/307], Step [300/308], Loss: 0.0024813951458781958\n","Epoch [229/307], Step [100/308], Loss: 0.002869020914658904\n","Epoch [229/307], Step [200/308], Loss: 0.0008831864106468856\n","Epoch [229/307], Step [300/308], Loss: 0.0010910058626905084\n","Epoch [230/307], Step [100/308], Loss: 0.009465322829782963\n","Epoch [230/307], Step [200/308], Loss: 0.004388198722153902\n","Epoch [230/307], Step [300/308], Loss: 0.027875442057847977\n","Epoch [231/307], Step [100/308], Loss: 0.0027411857154220343\n","Epoch [231/307], Step [200/308], Loss: 0.012025445699691772\n","Epoch [231/307], Step [300/308], Loss: 0.011960324831306934\n","Epoch [232/307], Step [100/308], Loss: 0.022648537531495094\n","Epoch [232/307], Step [200/308], Loss: 0.03202008828520775\n","Epoch [232/307], Step [300/308], Loss: 0.019098693504929543\n","Epoch [233/307], Step [100/308], Loss: 0.0024611400440335274\n","Epoch [233/307], Step [200/308], Loss: 0.0059148212894797325\n","Epoch [233/307], Step [300/308], Loss: 0.006234918255358934\n","Epoch [234/307], Step [100/308], Loss: 0.0023932054173201323\n","Epoch [234/307], Step [200/308], Loss: 0.012880906462669373\n","Epoch [234/307], Step [300/308], Loss: 0.009213536977767944\n","Epoch [235/307], Step [100/308], Loss: 0.023853763937950134\n","Epoch [235/307], Step [200/308], Loss: 0.003398655680939555\n","Epoch [235/307], Step [300/308], Loss: 0.002839375287294388\n","Epoch [236/307], Step [100/308], Loss: 0.007133470848202705\n","Epoch [236/307], Step [200/308], Loss: 0.012782747857272625\n","Epoch [236/307], Step [300/308], Loss: 0.013395054265856743\n","Epoch [237/307], Step [100/308], Loss: 0.010127424262464046\n","Epoch [237/307], Step [200/308], Loss: 0.0015511879464611411\n","Epoch [237/307], Step [300/308], Loss: 0.003410275559872389\n","Epoch [238/307], Step [100/308], Loss: 0.01312984712421894\n","Epoch [238/307], Step [200/308], Loss: 0.0005925379227846861\n","Epoch [238/307], Step [300/308], Loss: 0.0013212994672358036\n","Epoch [239/307], Step [100/308], Loss: 0.009555479511618614\n","Epoch [239/307], Step [200/308], Loss: 0.01567654497921467\n","Epoch [239/307], Step [300/308], Loss: 0.007645235396921635\n","Epoch [240/307], Step [100/308], Loss: 0.007215061690658331\n","Epoch [240/307], Step [200/308], Loss: 0.0022922325879335403\n","Epoch [240/307], Step [300/308], Loss: 0.00803106278181076\n","Epoch [241/307], Step [100/308], Loss: 0.003641600487753749\n","Epoch [241/307], Step [200/308], Loss: 0.010037233121693134\n","Epoch [241/307], Step [300/308], Loss: 0.0200562234967947\n","Epoch [242/307], Step [100/308], Loss: 0.004458468873053789\n","Epoch [242/307], Step [200/308], Loss: 0.004512739833444357\n","Epoch [242/307], Step [300/308], Loss: 0.006482189055532217\n","Epoch [243/307], Step [100/308], Loss: 0.011057591997087002\n","Epoch [243/307], Step [200/308], Loss: 0.019672587513923645\n","Epoch [243/307], Step [300/308], Loss: 0.0025509418919682503\n","Epoch [244/307], Step [100/308], Loss: 0.0022298924159258604\n","Epoch [244/307], Step [200/308], Loss: 0.01595374010503292\n","Epoch [244/307], Step [300/308], Loss: 0.01117531768977642\n","Epoch [245/307], Step [100/308], Loss: 0.005411951337009668\n","Epoch [245/307], Step [200/308], Loss: 0.006369816139340401\n","Epoch [245/307], Step [300/308], Loss: 0.008489550091326237\n","Epoch [246/307], Step [100/308], Loss: 0.005073659121990204\n","Epoch [246/307], Step [200/308], Loss: 0.0016601489624008536\n","Epoch [246/307], Step [300/308], Loss: 0.003559557721018791\n","Epoch [247/307], Step [100/308], Loss: 0.005099988542497158\n","Epoch [247/307], Step [200/308], Loss: 0.01013868860900402\n","Epoch [247/307], Step [300/308], Loss: 0.003329108702018857\n","Epoch [248/307], Step [100/308], Loss: 0.004876983817666769\n","Epoch [248/307], Step [200/308], Loss: 0.017400767654180527\n","Epoch [248/307], Step [300/308], Loss: 0.015524111688137054\n","Epoch [249/307], Step [100/308], Loss: 0.0019406380597501993\n","Epoch [249/307], Step [200/308], Loss: 0.002433815971016884\n","Epoch [249/307], Step [300/308], Loss: 0.014201169833540916\n","Epoch [250/307], Step [100/308], Loss: 0.0080221276730299\n","Epoch [250/307], Step [200/308], Loss: 0.012288244441151619\n","Epoch [250/307], Step [300/308], Loss: 0.004883079789578915\n","Epoch [251/307], Step [100/308], Loss: 0.012721002101898193\n","Epoch [251/307], Step [200/308], Loss: 0.011720020323991776\n","Epoch [251/307], Step [300/308], Loss: 0.0040228418074548244\n","Epoch [252/307], Step [100/308], Loss: 0.00047068786807358265\n","Epoch [252/307], Step [200/308], Loss: 0.001606413279660046\n","Epoch [252/307], Step [300/308], Loss: 0.015502678230404854\n","Epoch [253/307], Step [100/308], Loss: 0.011519658379256725\n","Epoch [253/307], Step [200/308], Loss: 0.0030270437709987164\n","Epoch [253/307], Step [300/308], Loss: 0.007574026472866535\n","Epoch [254/307], Step [100/308], Loss: 0.016002414748072624\n","Epoch [254/307], Step [200/308], Loss: 0.0032345231156796217\n","Epoch [254/307], Step [300/308], Loss: 0.004869300406426191\n","Epoch [255/307], Step [100/308], Loss: 0.0029106619767844677\n","Epoch [255/307], Step [200/308], Loss: 0.01765305921435356\n","Epoch [255/307], Step [300/308], Loss: 0.007568086497485638\n","Epoch [256/307], Step [100/308], Loss: 0.008660933934152126\n","Epoch [256/307], Step [200/308], Loss: 0.007685615215450525\n","Epoch [256/307], Step [300/308], Loss: 0.010289384052157402\n","Epoch [257/307], Step [100/308], Loss: 0.004037884529680014\n","Epoch [257/307], Step [200/308], Loss: 0.007991493679583073\n","Epoch [257/307], Step [300/308], Loss: 0.0011421471135690808\n","Epoch [258/307], Step [100/308], Loss: 0.025213386863470078\n","Epoch [258/307], Step [200/308], Loss: 0.012445300817489624\n","Epoch [258/307], Step [300/308], Loss: 0.00990232639014721\n","Epoch [259/307], Step [100/308], Loss: 0.00858341995626688\n","Epoch [259/307], Step [200/308], Loss: 0.004083555191755295\n","Epoch [259/307], Step [300/308], Loss: 0.011642901226878166\n","Epoch [260/307], Step [100/308], Loss: 0.008003979921340942\n","Epoch [260/307], Step [200/308], Loss: 0.005074836313724518\n","Epoch [260/307], Step [300/308], Loss: 0.000620629929471761\n","Epoch [261/307], Step [100/308], Loss: 0.026958448812365532\n","Epoch [261/307], Step [200/308], Loss: 0.0010880393674597144\n","Epoch [261/307], Step [300/308], Loss: 0.001831205328926444\n","Epoch [262/307], Step [100/308], Loss: 0.0037183472886681557\n","Epoch [262/307], Step [200/308], Loss: 0.025326693430542946\n","Epoch [262/307], Step [300/308], Loss: 0.0067465826869010925\n","Epoch [263/307], Step [100/308], Loss: 0.00571527611464262\n","Epoch [263/307], Step [200/308], Loss: 0.00729120709002018\n","Epoch [263/307], Step [300/308], Loss: 0.003354227403178811\n","Epoch [264/307], Step [100/308], Loss: 0.006262098904699087\n","Epoch [264/307], Step [200/308], Loss: 0.00974942371249199\n","Epoch [264/307], Step [300/308], Loss: 0.016701841726899147\n","Epoch [265/307], Step [100/308], Loss: 0.0028133494779467583\n","Epoch [265/307], Step [200/308], Loss: 0.0020218093413859606\n","Epoch [265/307], Step [300/308], Loss: 0.008374260738492012\n","Epoch [266/307], Step [100/308], Loss: 0.0026779028121382\n","Epoch [266/307], Step [200/308], Loss: 0.00850251130759716\n","Epoch [266/307], Step [300/308], Loss: 0.002057078294456005\n","Epoch [267/307], Step [100/308], Loss: 0.008572449907660484\n","Epoch [267/307], Step [200/308], Loss: 0.028953608125448227\n","Epoch [267/307], Step [300/308], Loss: 0.0019938454497605562\n","Epoch [268/307], Step [100/308], Loss: 0.012658799067139626\n","Epoch [268/307], Step [200/308], Loss: 0.0032985142897814512\n","Epoch [268/307], Step [300/308], Loss: 0.0013119722716510296\n","Epoch [269/307], Step [100/308], Loss: 0.001815545023418963\n","Epoch [269/307], Step [200/308], Loss: 0.016024749726057053\n","Epoch [269/307], Step [300/308], Loss: 0.00755165982991457\n","Epoch [270/307], Step [100/308], Loss: 0.007947206497192383\n","Epoch [270/307], Step [200/308], Loss: 0.019912822172045708\n","Epoch [270/307], Step [300/308], Loss: 0.0014824748504906893\n","Epoch [271/307], Step [100/308], Loss: 0.002549361903220415\n","Epoch [271/307], Step [200/308], Loss: 0.003948220517486334\n","Epoch [271/307], Step [300/308], Loss: 0.011361951008439064\n","Epoch [272/307], Step [100/308], Loss: 0.0007037095492705703\n","Epoch [272/307], Step [200/308], Loss: 0.0032850138377398252\n","Epoch [272/307], Step [300/308], Loss: 0.0011198255233466625\n","Epoch [273/307], Step [100/308], Loss: 0.010340229608118534\n","Epoch [273/307], Step [200/308], Loss: 0.014440215192735195\n","Epoch [273/307], Step [300/308], Loss: 0.025034908205270767\n","Epoch [274/307], Step [100/308], Loss: 0.001618994981981814\n","Epoch [274/307], Step [200/308], Loss: 0.01022198423743248\n","Epoch [274/307], Step [300/308], Loss: 0.01092312391847372\n","Epoch [275/307], Step [100/308], Loss: 0.006906274706125259\n","Epoch [275/307], Step [200/308], Loss: 0.010875990614295006\n","Epoch [275/307], Step [300/308], Loss: 0.008735014125704765\n","Epoch [276/307], Step [100/308], Loss: 0.00092389399651438\n","Epoch [276/307], Step [200/308], Loss: 0.007342109456658363\n","Epoch [276/307], Step [300/308], Loss: 0.0009190123528242111\n","Epoch [277/307], Step [100/308], Loss: 0.0014502700651064515\n","Epoch [277/307], Step [200/308], Loss: 0.0017056893557310104\n","Epoch [277/307], Step [300/308], Loss: 0.0060112858191132545\n","Epoch [278/307], Step [100/308], Loss: 0.014418092556297779\n","Epoch [278/307], Step [200/308], Loss: 0.002396273659542203\n","Epoch [278/307], Step [300/308], Loss: 0.014809388667345047\n","Epoch [279/307], Step [100/308], Loss: 0.0028686278965324163\n","Epoch [279/307], Step [200/308], Loss: 0.006376625504344702\n","Epoch [279/307], Step [300/308], Loss: 0.00655668368563056\n","Epoch [280/307], Step [100/308], Loss: 0.0019639318343251944\n","Epoch [280/307], Step [200/308], Loss: 0.01552600134164095\n","Epoch [280/307], Step [300/308], Loss: 0.0019706489983946085\n","Epoch [281/307], Step [100/308], Loss: 0.0056067476980388165\n","Epoch [281/307], Step [200/308], Loss: 0.006118616089224815\n","Epoch [281/307], Step [300/308], Loss: 0.012529127299785614\n","Epoch [282/307], Step [100/308], Loss: 0.01033036969602108\n","Epoch [282/307], Step [200/308], Loss: 0.00104161084163934\n","Epoch [282/307], Step [300/308], Loss: 0.0007590415189042687\n","Epoch [283/307], Step [100/308], Loss: 0.0024875595699995756\n","Epoch [283/307], Step [200/308], Loss: 0.005459781736135483\n","Epoch [283/307], Step [300/308], Loss: 0.010699233040213585\n","Epoch [284/307], Step [100/308], Loss: 0.001867632381618023\n","Epoch [284/307], Step [200/308], Loss: 0.00527673214673996\n","Epoch [284/307], Step [300/308], Loss: 0.00253605330362916\n","Epoch [285/307], Step [100/308], Loss: 0.0012965736677870154\n","Epoch [285/307], Step [200/308], Loss: 0.008681438863277435\n","Epoch [285/307], Step [300/308], Loss: 0.0008951539639383554\n","Epoch [286/307], Step [100/308], Loss: 0.005641637835651636\n","Epoch [286/307], Step [200/308], Loss: 0.006432048976421356\n","Epoch [286/307], Step [300/308], Loss: 0.021587112918496132\n","Epoch [287/307], Step [100/308], Loss: 0.007680865935981274\n","Epoch [287/307], Step [200/308], Loss: 0.01226387545466423\n","Epoch [287/307], Step [300/308], Loss: 0.005827037151902914\n","Epoch [288/307], Step [100/308], Loss: 0.013005880638957024\n","Epoch [288/307], Step [200/308], Loss: 0.007259828969836235\n","Epoch [288/307], Step [300/308], Loss: 0.004142850171774626\n","Epoch [289/307], Step [100/308], Loss: 0.001371773425489664\n","Epoch [289/307], Step [200/308], Loss: 0.0032568275928497314\n","Epoch [289/307], Step [300/308], Loss: 0.008777587674558163\n","Epoch [290/307], Step [100/308], Loss: 0.011896614916622639\n","Epoch [290/307], Step [200/308], Loss: 0.006552963517606258\n","Epoch [290/307], Step [300/308], Loss: 0.05546989291906357\n","Epoch [291/307], Step [100/308], Loss: 0.005767487920820713\n","Epoch [291/307], Step [200/308], Loss: 0.003977377433329821\n","Epoch [291/307], Step [300/308], Loss: 0.0017817593179643154\n","Epoch [292/307], Step [100/308], Loss: 0.001051241299137473\n","Epoch [292/307], Step [200/308], Loss: 0.013672753237187862\n","Epoch [292/307], Step [300/308], Loss: 0.008567363023757935\n","Epoch [293/307], Step [100/308], Loss: 0.0085953613743186\n","Epoch [293/307], Step [200/308], Loss: 0.0021067908965051174\n","Epoch [293/307], Step [300/308], Loss: 0.014933088794350624\n","Epoch [294/307], Step [100/308], Loss: 0.027170704677700996\n","Epoch [294/307], Step [200/308], Loss: 0.05704866722226143\n","Epoch [294/307], Step [300/308], Loss: 0.00365253584459424\n","Epoch [295/307], Step [100/308], Loss: 0.006518013775348663\n","Epoch [295/307], Step [200/308], Loss: 0.00925927422940731\n","Epoch [295/307], Step [300/308], Loss: 0.0043589030392467976\n","Epoch [296/307], Step [100/308], Loss: 0.0015243567759171128\n","Epoch [296/307], Step [200/308], Loss: 0.003789339680224657\n","Epoch [296/307], Step [300/308], Loss: 0.001011030632071197\n","Epoch [297/307], Step [100/308], Loss: 0.0015567382797598839\n","Epoch [297/307], Step [200/308], Loss: 0.005310078151524067\n","Epoch [297/307], Step [300/308], Loss: 0.006498685572296381\n","Epoch [298/307], Step [100/308], Loss: 0.006747962441295385\n","Epoch [298/307], Step [200/308], Loss: 0.012942597270011902\n","Epoch [298/307], Step [300/308], Loss: 0.0043621971271932125\n","Epoch [299/307], Step [100/308], Loss: 0.007500465027987957\n","Epoch [299/307], Step [200/308], Loss: 0.001695640035904944\n","Epoch [299/307], Step [300/308], Loss: 0.002539833541959524\n","Epoch [300/307], Step [100/308], Loss: 0.0062363944016397\n","Epoch [300/307], Step [200/308], Loss: 0.0021735329646617174\n","Epoch [300/307], Step [300/308], Loss: 0.001089898869395256\n","Epoch [301/307], Step [100/308], Loss: 0.0012066146591678262\n","Epoch [301/307], Step [200/308], Loss: 0.003725458635017276\n","Epoch [301/307], Step [300/308], Loss: 0.01381553802639246\n","Epoch [302/307], Step [100/308], Loss: 0.004753199405968189\n","Epoch [302/307], Step [200/308], Loss: 0.001691751996986568\n","Epoch [302/307], Step [300/308], Loss: 0.001313125598244369\n","Epoch [303/307], Step [100/308], Loss: 0.009679713286459446\n","Epoch [303/307], Step [200/308], Loss: 0.0038070594891905785\n","Epoch [303/307], Step [300/308], Loss: 0.016653193160891533\n","Epoch [304/307], Step [100/308], Loss: 0.004112768918275833\n","Epoch [304/307], Step [200/308], Loss: 0.00829196535050869\n","Epoch [304/307], Step [300/308], Loss: 0.03614325448870659\n","Epoch [305/307], Step [100/308], Loss: 0.01790231466293335\n","Epoch [305/307], Step [200/308], Loss: 0.0006086193025112152\n","Epoch [305/307], Step [300/308], Loss: 0.003461961867287755\n","Epoch [306/307], Step [100/308], Loss: 0.012365381233394146\n","Epoch [306/307], Step [200/308], Loss: 0.005182953085750341\n","Epoch [306/307], Step [300/308], Loss: 0.008834091015160084\n","Epoch [307/307], Step [100/308], Loss: 0.004684382118284702\n","Epoch [307/307], Step [200/308], Loss: 0.016049927100539207\n","Epoch [307/307], Step [300/308], Loss: 0.002009000862017274\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfkZu9lInsw3","executionInfo":{"elapsed":5685,"status":"ok","timestamp":1623035006031,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"e83ba1fa-a2f6-452d-d80a-8343884efe0b"},"source":["# Test the model\n","test_dataset = CustomDataset(X_test, y_test)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (data, labels) in enumerate(test_loader):\n","        data = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 100.0\n","Iteration:1,  Accuracy: 100.0\n","Iteration:2,  Accuracy: 100.0\n","Iteration:3,  Accuracy: 100.0\n","Iteration:4,  Accuracy: 100.0\n","Iteration:5,  Accuracy: 100.0\n","Iteration:6,  Accuracy: 100.0\n","Iteration:7,  Accuracy: 100.0\n","Iteration:8,  Accuracy: 99.66666666666667\n","Iteration:9,  Accuracy: 99.7\n","Iteration:10,  Accuracy: 99.72727272727273\n","Iteration:11,  Accuracy: 99.66666666666667\n","Iteration:12,  Accuracy: 99.6923076923077\n","Iteration:13,  Accuracy: 99.71428571428571\n","Iteration:14,  Accuracy: 99.73333333333333\n","Iteration:15,  Accuracy: 99.75\n","Iteration:16,  Accuracy: 99.6470588235294\n","Iteration:17,  Accuracy: 99.66666666666667\n","Iteration:18,  Accuracy: 99.6842105263158\n","Iteration:19,  Accuracy: 99.7\n","Iteration:20,  Accuracy: 99.71428571428571\n","Iteration:21,  Accuracy: 99.72727272727273\n","Iteration:22,  Accuracy: 99.73913043478261\n","Iteration:23,  Accuracy: 99.70833333333333\n","Iteration:24,  Accuracy: 99.72\n","Iteration:25,  Accuracy: 99.73076923076923\n","Iteration:26,  Accuracy: 99.74074074074075\n","Iteration:27,  Accuracy: 99.75\n","Iteration:28,  Accuracy: 99.75862068965517\n","Iteration:29,  Accuracy: 99.76666666666667\n","Iteration:30,  Accuracy: 99.7741935483871\n","Iteration:31,  Accuracy: 99.78125\n","Iteration:32,  Accuracy: 99.78787878787878\n","Iteration:33,  Accuracy: 99.79411764705883\n","Iteration:34,  Accuracy: 99.8\n","Iteration:35,  Accuracy: 99.77777777777777\n","Iteration:36,  Accuracy: 99.72972972972973\n","Iteration:37,  Accuracy: 99.73684210526316\n","Iteration:38,  Accuracy: 99.74358974358974\n","Iteration:39,  Accuracy: 99.75\n","Iteration:40,  Accuracy: 99.7560975609756\n","Iteration:41,  Accuracy: 99.76190476190476\n","Iteration:42,  Accuracy: 99.76744186046511\n","Iteration:43,  Accuracy: 99.75\n","Iteration:44,  Accuracy: 99.73333333333333\n","Iteration:45,  Accuracy: 99.73913043478261\n","Iteration:46,  Accuracy: 99.72340425531915\n","Iteration:47,  Accuracy: 99.72916666666667\n","Iteration:48,  Accuracy: 99.73469387755102\n","Iteration:49,  Accuracy: 99.74\n","Iteration:50,  Accuracy: 99.74509803921569\n","Iteration:51,  Accuracy: 99.73076923076923\n","Iteration:52,  Accuracy: 99.73584905660377\n","Iteration:53,  Accuracy: 99.74074074074075\n","Iteration:54,  Accuracy: 99.72727272727273\n","Iteration:55,  Accuracy: 99.73214285714286\n","Iteration:56,  Accuracy: 99.73684210526316\n","Iteration:57,  Accuracy: 99.72413793103448\n","Iteration:58,  Accuracy: 99.72881355932203\n","Iteration:59,  Accuracy: 99.73333333333333\n","Iteration:60,  Accuracy: 99.73770491803279\n","Iteration:61,  Accuracy: 99.7258064516129\n","Iteration:62,  Accuracy: 99.71428571428571\n","Iteration:63,  Accuracy: 99.71875\n","Iteration:64,  Accuracy: 99.70769230769231\n","Iteration:65,  Accuracy: 99.6969696969697\n","Iteration:66,  Accuracy: 99.68656716417911\n","Iteration:67,  Accuracy: 99.69117647058823\n","Iteration:68,  Accuracy: 99.68115942028986\n","Iteration:69,  Accuracy: 99.68571428571428\n","Iteration:70,  Accuracy: 99.69014084507042\n","Iteration:71,  Accuracy: 99.69444444444444\n","Iteration:72,  Accuracy: 99.6986301369863\n","Iteration:73,  Accuracy: 99.6891891891892\n","Iteration:74,  Accuracy: 99.69333333333333\n","Iteration:75,  Accuracy: 99.69736842105263\n","Iteration:76,  Accuracy: 99.7012987012987\n","Iteration:77,  Accuracy: 99.7051282051282\n","Iteration:78,  Accuracy: 99.70886075949367\n","Iteration:79,  Accuracy: 99.7\n","Iteration:80,  Accuracy: 99.69135802469135\n","Iteration:81,  Accuracy: 99.6829268292683\n","Iteration:82,  Accuracy: 99.6867469879518\n","Iteration:83,  Accuracy: 99.67857142857143\n","Iteration:84,  Accuracy: 99.68235294117648\n","Iteration:85,  Accuracy: 99.68604651162791\n","Iteration:86,  Accuracy: 99.67816091954023\n","Iteration:87,  Accuracy: 99.68181818181819\n","Iteration:88,  Accuracy: 99.68539325842697\n","Iteration:89,  Accuracy: 99.67777777777778\n","Iteration:90,  Accuracy: 99.68131868131869\n","Iteration:91,  Accuracy: 99.68478260869566\n","Iteration:92,  Accuracy: 99.68817204301075\n","Iteration:93,  Accuracy: 99.68085106382979\n","Iteration:94,  Accuracy: 99.67368421052632\n","Iteration:95,  Accuracy: 99.65625\n","Iteration:96,  Accuracy: 99.65979381443299\n","Iteration:97,  Accuracy: 99.66326530612245\n","Iteration:98,  Accuracy: 99.66666666666667\n","Iteration:99,  Accuracy: 99.67\n","Iteration:100,  Accuracy: 99.66336633663366\n","Iteration:101,  Accuracy: 99.66666666666667\n","Iteration:102,  Accuracy: 99.66990291262135\n","Iteration:103,  Accuracy: 99.66346153846153\n","Iteration:104,  Accuracy: 99.65714285714286\n","Iteration:105,  Accuracy: 99.64150943396227\n","Iteration:106,  Accuracy: 99.64485981308411\n","Iteration:107,  Accuracy: 99.64814814814815\n","Iteration:108,  Accuracy: 99.65137614678899\n","Iteration:109,  Accuracy: 99.64545454545454\n","Iteration:110,  Accuracy: 99.64864864864865\n","Iteration:111,  Accuracy: 99.65178571428571\n","Iteration:112,  Accuracy: 99.65486725663717\n","Iteration:113,  Accuracy: 99.65789473684211\n","Iteration:114,  Accuracy: 99.6608695652174\n","Iteration:115,  Accuracy: 99.65517241379311\n","Iteration:116,  Accuracy: 99.65811965811966\n","Iteration:117,  Accuracy: 99.66101694915254\n","Iteration:118,  Accuracy: 99.66386554621849\n","Iteration:119,  Accuracy: 99.66666666666667\n","Iteration:120,  Accuracy: 99.6694214876033\n","Iteration:121,  Accuracy: 99.67213114754098\n","Iteration:122,  Accuracy: 99.66666666666667\n","Iteration:123,  Accuracy: 99.66129032258064\n","Iteration:124,  Accuracy: 99.664\n","Iteration:125,  Accuracy: 99.66666666666667\n","Iteration:126,  Accuracy: 99.66929133858268\n","Iteration:127,  Accuracy: 99.671875\n","Iteration:128,  Accuracy: 99.67441860465117\n","Iteration:129,  Accuracy: 99.67692307692307\n","Iteration:130,  Accuracy: 99.6793893129771\n","Iteration:131,  Accuracy: 99.68181818181819\n","Iteration:132,  Accuracy: 99.67669172932331\n","Iteration:133,  Accuracy: 99.67910447761194\n","Iteration:134,  Accuracy: 99.68148148148148\n","Iteration:135,  Accuracy: 99.68382352941177\n","Iteration:136,  Accuracy: 99.68613138686132\n","Iteration:137,  Accuracy: 99.68840579710145\n","Iteration:138,  Accuracy: 99.68345323741008\n","Iteration:139,  Accuracy: 99.68571428571428\n","Iteration:140,  Accuracy: 99.68085106382979\n","Iteration:141,  Accuracy: 99.6830985915493\n","Iteration:142,  Accuracy: 99.68531468531468\n","Iteration:143,  Accuracy: 99.6875\n","Iteration:144,  Accuracy: 99.6896551724138\n","Iteration:145,  Accuracy: 99.6917808219178\n","Iteration:146,  Accuracy: 99.68707482993197\n","Iteration:147,  Accuracy: 99.6891891891892\n","Iteration:148,  Accuracy: 99.69127516778524\n","Iteration:149,  Accuracy: 99.68666666666667\n","Iteration:150,  Accuracy: 99.6887417218543\n","Iteration:151,  Accuracy: 99.6842105263158\n","Iteration:152,  Accuracy: 99.68627450980392\n","Iteration:153,  Accuracy: 99.68831168831169\n","Iteration:154,  Accuracy: 99.69032258064516\n","Iteration:155,  Accuracy: 99.6923076923077\n","Iteration:156,  Accuracy: 99.69426751592357\n","Iteration:157,  Accuracy: 99.69620253164557\n","Iteration:158,  Accuracy: 99.69811320754717\n","Iteration:159,  Accuracy: 99.7\n","Iteration:160,  Accuracy: 99.69565217391305\n","Iteration:161,  Accuracy: 99.69753086419753\n","Iteration:162,  Accuracy: 99.69938650306749\n","Iteration:163,  Accuracy: 99.70121951219512\n","Iteration:164,  Accuracy: 99.7030303030303\n","Iteration:165,  Accuracy: 99.6987951807229\n","Iteration:166,  Accuracy: 99.69461077844312\n","Iteration:167,  Accuracy: 99.69642857142857\n","Iteration:168,  Accuracy: 99.6923076923077\n","Iteration:169,  Accuracy: 99.69411764705882\n","Iteration:170,  Accuracy: 99.69005847953217\n","Iteration:171,  Accuracy: 99.69186046511628\n","Iteration:172,  Accuracy: 99.69364161849711\n","Iteration:173,  Accuracy: 99.69540229885058\n","Iteration:174,  Accuracy: 99.69714285714285\n","Iteration:175,  Accuracy: 99.69318181818181\n","Iteration:176,  Accuracy: 99.69491525423729\n","Iteration:177,  Accuracy: 99.69662921348315\n","Iteration:178,  Accuracy: 99.69832402234637\n","Iteration:179,  Accuracy: 99.69444444444444\n","Iteration:180,  Accuracy: 99.69060773480663\n","Iteration:181,  Accuracy: 99.6923076923077\n","Iteration:182,  Accuracy: 99.69398907103825\n","Iteration:183,  Accuracy: 99.69021739130434\n","Iteration:184,  Accuracy: 99.69189189189188\n","Iteration:185,  Accuracy: 99.69354838709677\n","Iteration:186,  Accuracy: 99.68983957219251\n","Iteration:187,  Accuracy: 99.69148936170212\n","Iteration:188,  Accuracy: 99.68783068783068\n","Iteration:189,  Accuracy: 99.68947368421053\n","Iteration:190,  Accuracy: 99.68062827225131\n","Iteration:191,  Accuracy: 99.68229166666667\n","Iteration:192,  Accuracy: 99.67875647668394\n","Iteration:193,  Accuracy: 99.68041237113403\n","Iteration:194,  Accuracy: 99.68205128205128\n","Iteration:195,  Accuracy: 99.68367346938776\n","Iteration:196,  Accuracy: 99.68527918781726\n","Iteration:197,  Accuracy: 99.68686868686869\n","Iteration:198,  Accuracy: 99.68844221105527\n","Iteration:199,  Accuracy: 99.69\n","Iteration:200,  Accuracy: 99.69154228855722\n","Iteration:201,  Accuracy: 99.6930693069307\n","Iteration:202,  Accuracy: 99.69458128078817\n","Iteration:203,  Accuracy: 99.69607843137256\n","Iteration:204,  Accuracy: 99.69756097560976\n","Iteration:205,  Accuracy: 99.69902912621359\n","Iteration:206,  Accuracy: 99.70048309178743\n","Iteration:207,  Accuracy: 99.70192307692308\n","Iteration:208,  Accuracy: 99.69856459330144\n","Iteration:209,  Accuracy: 99.6952380952381\n","Iteration:210,  Accuracy: 99.69668246445498\n","Iteration:211,  Accuracy: 99.69339622641509\n","Iteration:212,  Accuracy: 99.69483568075117\n","Iteration:213,  Accuracy: 99.69626168224299\n","Iteration:214,  Accuracy: 99.69767441860465\n","Iteration:215,  Accuracy: 99.69907407407408\n","Iteration:216,  Accuracy: 99.6958525345622\n","Iteration:217,  Accuracy: 99.69724770642202\n","Iteration:218,  Accuracy: 99.6986301369863\n","Iteration:219,  Accuracy: 99.7\n","Iteration:220,  Accuracy: 99.69683257918552\n","Iteration:221,  Accuracy: 99.6981981981982\n","Iteration:222,  Accuracy: 99.69955156950672\n","Iteration:223,  Accuracy: 99.69642857142857\n","Iteration:224,  Accuracy: 99.69777777777777\n","Iteration:225,  Accuracy: 99.69911504424779\n","Iteration:226,  Accuracy: 99.70044052863436\n","Iteration:227,  Accuracy: 99.70175438596492\n","Iteration:228,  Accuracy: 99.69868995633188\n","Iteration:229,  Accuracy: 99.7\n","Iteration:230,  Accuracy: 99.7012987012987\n","Iteration:231,  Accuracy: 99.70258620689656\n","Iteration:232,  Accuracy: 99.70386266094421\n","Iteration:233,  Accuracy: 99.7051282051282\n","Iteration:234,  Accuracy: 99.70638297872341\n","Iteration:235,  Accuracy: 99.70762711864407\n","Iteration:236,  Accuracy: 99.70042194092827\n","Iteration:237,  Accuracy: 99.7016806722689\n","Iteration:238,  Accuracy: 99.69874476987448\n","Iteration:239,  Accuracy: 99.7\n","Iteration:240,  Accuracy: 99.69709543568464\n","Iteration:241,  Accuracy: 99.68595041322314\n","Iteration:242,  Accuracy: 99.6872427983539\n","Iteration:243,  Accuracy: 99.68442622950819\n","Iteration:244,  Accuracy: 99.68571428571428\n","Iteration:245,  Accuracy: 99.6869918699187\n","Iteration:246,  Accuracy: 99.6842105263158\n","Iteration:247,  Accuracy: 99.68548387096774\n","Iteration:248,  Accuracy: 99.6867469879518\n","Iteration:249,  Accuracy: 99.684\n","Iteration:250,  Accuracy: 99.68525896414343\n","Iteration:251,  Accuracy: 99.68650793650794\n","Iteration:252,  Accuracy: 99.68774703557312\n","Iteration:253,  Accuracy: 99.68897637795276\n","Iteration:254,  Accuracy: 99.69019607843137\n","Iteration:255,  Accuracy: 99.6875\n","Iteration:256,  Accuracy: 99.68482490272373\n","Iteration:257,  Accuracy: 99.68604651162791\n","Iteration:258,  Accuracy: 99.68725868725869\n","Iteration:259,  Accuracy: 99.68461538461538\n","Iteration:260,  Accuracy: 99.68582375478927\n","Iteration:261,  Accuracy: 99.68320610687023\n","Iteration:262,  Accuracy: 99.67300380228137\n","Iteration:263,  Accuracy: 99.67045454545455\n","Iteration:264,  Accuracy: 99.67169811320754\n","Iteration:265,  Accuracy: 99.67293233082707\n","Iteration:266,  Accuracy: 99.67041198501873\n","Iteration:267,  Accuracy: 99.67164179104478\n","Iteration:268,  Accuracy: 99.6728624535316\n","Iteration:269,  Accuracy: 99.67407407407407\n","Iteration:270,  Accuracy: 99.66789667896678\n","Iteration:271,  Accuracy: 99.6654411764706\n","Iteration:272,  Accuracy: 99.66666666666667\n","Iteration:273,  Accuracy: 99.66788321167883\n","Iteration:274,  Accuracy: 99.66181818181818\n","Iteration:275,  Accuracy: 99.66304347826087\n","Iteration:276,  Accuracy: 99.66064981949458\n","Iteration:277,  Accuracy: 99.66187050359713\n","Iteration:278,  Accuracy: 99.6594982078853\n","Iteration:279,  Accuracy: 99.65\n","Iteration:280,  Accuracy: 99.65124555160142\n","Iteration:281,  Accuracy: 99.64539007092199\n","Iteration:282,  Accuracy: 99.64310954063605\n","Iteration:283,  Accuracy: 99.6443661971831\n","Iteration:284,  Accuracy: 99.64561403508772\n","Iteration:285,  Accuracy: 99.64685314685315\n","Iteration:286,  Accuracy: 99.64808362369338\n","Iteration:287,  Accuracy: 99.64930555555556\n","Iteration:288,  Accuracy: 99.65051903114187\n","Iteration:289,  Accuracy: 99.64137931034483\n","Iteration:290,  Accuracy: 99.63573883161511\n","Iteration:291,  Accuracy: 99.63698630136986\n","Iteration:292,  Accuracy: 99.6382252559727\n","Iteration:293,  Accuracy: 99.63945578231292\n","Iteration:294,  Accuracy: 99.64067796610169\n","Iteration:295,  Accuracy: 99.63851351351352\n","Iteration:296,  Accuracy: 99.63973063973064\n","Iteration:297,  Accuracy: 99.63758389261746\n","Iteration:298,  Accuracy: 99.63879598662207\n","Iteration:299,  Accuracy: 99.63666666666667\n","Iteration:300,  Accuracy: 99.63455149501661\n","Iteration:301,  Accuracy: 99.62913907284768\n","Iteration:302,  Accuracy: 99.63036303630363\n","Iteration:303,  Accuracy: 99.6282894736842\n","Iteration:304,  Accuracy: 99.62950819672132\n","Iteration:305,  Accuracy: 99.63071895424837\n","Iteration:306,  Accuracy: 99.62866449511401\n","Iteration:307,  Accuracy: 99.62662337662337\n","Iteration:308,  Accuracy: 99.62783171521036\n","Iteration:309,  Accuracy: 99.62903225806451\n","Iteration:310,  Accuracy: 99.63022508038586\n","Iteration:311,  Accuracy: 99.63141025641026\n","Iteration:312,  Accuracy: 99.63258785942492\n","Iteration:313,  Accuracy: 99.63375796178345\n","Iteration:314,  Accuracy: 99.63492063492063\n","Iteration:315,  Accuracy: 99.63607594936708\n","Iteration:316,  Accuracy: 99.6372239747634\n","Iteration:317,  Accuracy: 99.63836477987421\n","Iteration:318,  Accuracy: 99.63949843260188\n","Iteration:319,  Accuracy: 99.640625\n","Iteration:320,  Accuracy: 99.6386292834891\n","Iteration:321,  Accuracy: 99.63664596273291\n","Iteration:322,  Accuracy: 99.63467492260062\n","Iteration:323,  Accuracy: 99.62962962962963\n","Iteration:324,  Accuracy: 99.63076923076923\n","Iteration:325,  Accuracy: 99.62883435582822\n","Iteration:326,  Accuracy: 99.62996941896024\n","Iteration:327,  Accuracy: 99.6310975609756\n","Iteration:328,  Accuracy: 99.62917933130699\n","Iteration:329,  Accuracy: 99.63030303030303\n","Iteration:330,  Accuracy: 99.62839879154079\n","Iteration:331,  Accuracy: 99.62951807228916\n","Iteration:332,  Accuracy: 99.62762762762763\n","Iteration:333,  Accuracy: 99.62874251497006\n","Iteration:334,  Accuracy: 99.62985074626866\n","Iteration:335,  Accuracy: 99.63095238095238\n","Iteration:336,  Accuracy: 99.6320474777448\n","Iteration:337,  Accuracy: 99.63313609467455\n","Iteration:338,  Accuracy: 99.63126843657817\n","Iteration:339,  Accuracy: 99.63235294117646\n","Iteration:340,  Accuracy: 99.633431085044\n","Iteration:341,  Accuracy: 99.63450292397661\n","Iteration:342,  Accuracy: 99.63556851311954\n","Iteration:343,  Accuracy: 99.63372093023256\n","Iteration:344,  Accuracy: 99.63478260869566\n","Iteration:345,  Accuracy: 99.63583815028902\n","Iteration:346,  Accuracy: 99.63112391930835\n","Iteration:347,  Accuracy: 99.63218390804597\n","Iteration:348,  Accuracy: 99.63323782234957\n","Iteration:349,  Accuracy: 99.63142857142857\n","Iteration:350,  Accuracy: 99.63247863247864\n","Iteration:351,  Accuracy: 99.63352272727273\n","Iteration:352,  Accuracy: 99.63456090651557\n","Iteration:353,  Accuracy: 99.63276836158192\n","Iteration:354,  Accuracy: 99.63380281690141\n","Iteration:355,  Accuracy: 99.63483146067416\n","Iteration:356,  Accuracy: 99.63305322128852\n","Iteration:357,  Accuracy: 99.6340782122905\n","Iteration:358,  Accuracy: 99.63509749303621\n","Iteration:359,  Accuracy: 99.63333333333334\n","Iteration:360,  Accuracy: 99.63434903047091\n","Iteration:361,  Accuracy: 99.6353591160221\n","Iteration:362,  Accuracy: 99.63636363636364\n","Iteration:363,  Accuracy: 99.63736263736264\n","Iteration:364,  Accuracy: 99.63561643835617\n","Iteration:365,  Accuracy: 99.63387978142076\n","Iteration:366,  Accuracy: 99.63215258855585\n","Iteration:367,  Accuracy: 99.63315217391305\n","Iteration:368,  Accuracy: 99.63414634146342\n","Iteration:369,  Accuracy: 99.63513513513513\n","Iteration:370,  Accuracy: 99.63611859838275\n","Iteration:371,  Accuracy: 99.63709677419355\n","Iteration:372,  Accuracy: 99.63538873994638\n","Iteration:373,  Accuracy: 99.63636363636364\n","Iteration:374,  Accuracy: 99.63466666666666\n","Iteration:375,  Accuracy: 99.63031914893617\n","Iteration:376,  Accuracy: 99.63129973474801\n","Iteration:377,  Accuracy: 99.63227513227513\n","Iteration:378,  Accuracy: 99.63324538258576\n","Iteration:379,  Accuracy: 99.63421052631578\n","Iteration:380,  Accuracy: 99.62992125984252\n","Iteration:381,  Accuracy: 99.63089005235602\n","Iteration:382,  Accuracy: 99.62924281984334\n","Iteration:383,  Accuracy: 99.625\n","Iteration:384,  Accuracy: 99.62597402597403\n","Iteration:385,  Accuracy: 99.62435233160622\n","Iteration:386,  Accuracy: 99.62273901808786\n","Iteration:387,  Accuracy: 99.62113402061856\n","Iteration:388,  Accuracy: 99.61953727506426\n","Iteration:389,  Accuracy: 99.62051282051281\n","Iteration:390,  Accuracy: 99.62148337595907\n","Iteration:391,  Accuracy: 99.61989795918367\n","Iteration:392,  Accuracy: 99.6208651399491\n","Iteration:393,  Accuracy: 99.61910566008989\n","Test Accuracy of the model on the 10000 test images: 99.61910566008989 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x-kJTA3EbtOw"},"source":["### 20개 아니고 10개씩 잘라서 진행"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cU_TaAkAbs6l","executionInfo":{"status":"ok","timestamp":1623967301341,"user_tz":-540,"elapsed":1322880,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"7b8bd655-11c7-4ca6-e341-6389fa78facd"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 2\n","batch_size = 400\n","num_epochs = len(y_train[100000:]) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train[100000:], y_train[100000:]) # 400,160\n","train_loader = DataLoader(dataset=train_dataset, batch_size=400, shuffle=True)\n","\n","# Train the model 400*20*8 = 800*10*8\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        images = images[:, 10:]\n","        #print(images.shape)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/143], Step [100/144], Loss: 0.27681219577789307\n","Epoch [2/143], Step [100/144], Loss: 0.21070636808872223\n","Epoch [3/143], Step [100/144], Loss: 0.20594066381454468\n","Epoch [4/143], Step [100/144], Loss: 0.18833181262016296\n","Epoch [5/143], Step [100/144], Loss: 0.1516430526971817\n","Epoch [6/143], Step [100/144], Loss: 0.10275159776210785\n","Epoch [7/143], Step [100/144], Loss: 0.09995552897453308\n","Epoch [8/143], Step [100/144], Loss: 0.11717662215232849\n","Epoch [9/143], Step [100/144], Loss: 0.07111965864896774\n","Epoch [10/143], Step [100/144], Loss: 0.04940161108970642\n","Epoch [11/143], Step [100/144], Loss: 0.06520748138427734\n","Epoch [12/143], Step [100/144], Loss: 0.02650388516485691\n","Epoch [13/143], Step [100/144], Loss: 0.043343354016542435\n","Epoch [14/143], Step [100/144], Loss: 0.042931489646434784\n","Epoch [15/143], Step [100/144], Loss: 0.02375202812254429\n","Epoch [16/143], Step [100/144], Loss: 0.031941719353199005\n","Epoch [17/143], Step [100/144], Loss: 0.026157855987548828\n","Epoch [18/143], Step [100/144], Loss: 0.026232436299324036\n","Epoch [19/143], Step [100/144], Loss: 0.021479357033967972\n","Epoch [20/143], Step [100/144], Loss: 0.012225211597979069\n","Epoch [21/143], Step [100/144], Loss: 0.00946440827101469\n","Epoch [22/143], Step [100/144], Loss: 0.025323698297142982\n","Epoch [23/143], Step [100/144], Loss: 0.007052259519696236\n","Epoch [24/143], Step [100/144], Loss: 0.008844180032610893\n","Epoch [25/143], Step [100/144], Loss: 0.0028862208127975464\n","Epoch [26/143], Step [100/144], Loss: 0.005299708340317011\n","Epoch [27/143], Step [100/144], Loss: 0.014566812664270401\n","Epoch [28/143], Step [100/144], Loss: 0.011283297091722488\n","Epoch [29/143], Step [100/144], Loss: 0.008924676105380058\n","Epoch [30/143], Step [100/144], Loss: 0.010406107641756535\n","Epoch [31/143], Step [100/144], Loss: 0.020177865400910378\n","Epoch [32/143], Step [100/144], Loss: 0.0011937369126826525\n","Epoch [33/143], Step [100/144], Loss: 0.013289812952280045\n","Epoch [34/143], Step [100/144], Loss: 0.02839546836912632\n","Epoch [35/143], Step [100/144], Loss: 0.012060586363077164\n","Epoch [36/143], Step [100/144], Loss: 0.016417870298027992\n","Epoch [37/143], Step [100/144], Loss: 0.01187864039093256\n","Epoch [38/143], Step [100/144], Loss: 0.006760316900908947\n","Epoch [39/143], Step [100/144], Loss: 0.0020009425934404135\n","Epoch [40/143], Step [100/144], Loss: 0.02110978774726391\n","Epoch [41/143], Step [100/144], Loss: 0.008207977749407291\n","Epoch [42/143], Step [100/144], Loss: 0.02122131548821926\n","Epoch [43/143], Step [100/144], Loss: 0.009489846415817738\n","Epoch [44/143], Step [100/144], Loss: 0.008040159940719604\n","Epoch [45/143], Step [100/144], Loss: 0.004662868566811085\n","Epoch [46/143], Step [100/144], Loss: 0.0022608621511608362\n","Epoch [47/143], Step [100/144], Loss: 0.025640038773417473\n","Epoch [48/143], Step [100/144], Loss: 0.018640659749507904\n","Epoch [49/143], Step [100/144], Loss: 0.004464942030608654\n","Epoch [50/143], Step [100/144], Loss: 0.003628681879490614\n","Epoch [51/143], Step [100/144], Loss: 0.013846253044903278\n","Epoch [52/143], Step [100/144], Loss: 0.018218347802758217\n","Epoch [53/143], Step [100/144], Loss: 0.028953665867447853\n","Epoch [54/143], Step [100/144], Loss: 0.004728048108518124\n","Epoch [55/143], Step [100/144], Loss: 0.0014746758388355374\n","Epoch [56/143], Step [100/144], Loss: 0.01907544955611229\n","Epoch [57/143], Step [100/144], Loss: 0.0026797435712069273\n","Epoch [58/143], Step [100/144], Loss: 0.004718795884400606\n","Epoch [59/143], Step [100/144], Loss: 0.036452699452638626\n","Epoch [60/143], Step [100/144], Loss: 0.022761858999729156\n","Epoch [61/143], Step [100/144], Loss: 0.016148995608091354\n","Epoch [62/143], Step [100/144], Loss: 0.0035214403178542852\n","Epoch [63/143], Step [100/144], Loss: 0.0018336630892008543\n","Epoch [64/143], Step [100/144], Loss: 0.012824066914618015\n","Epoch [65/143], Step [100/144], Loss: 0.014718012884259224\n","Epoch [66/143], Step [100/144], Loss: 0.009264194406569004\n","Epoch [67/143], Step [100/144], Loss: 0.005853290669620037\n","Epoch [68/143], Step [100/144], Loss: 0.0018403581343591213\n","Epoch [69/143], Step [100/144], Loss: 0.024725167080760002\n","Epoch [70/143], Step [100/144], Loss: 0.019541634246706963\n","Epoch [71/143], Step [100/144], Loss: 0.011159442365169525\n","Epoch [72/143], Step [100/144], Loss: 0.0010976188350468874\n","Epoch [73/143], Step [100/144], Loss: 0.0015369418542832136\n","Epoch [74/143], Step [100/144], Loss: 0.004494026768952608\n","Epoch [75/143], Step [100/144], Loss: 0.017959553748369217\n","Epoch [76/143], Step [100/144], Loss: 0.003539429046213627\n","Epoch [77/143], Step [100/144], Loss: 0.008686907589435577\n","Epoch [78/143], Step [100/144], Loss: 0.0071083176881074905\n","Epoch [79/143], Step [100/144], Loss: 0.0157170370221138\n","Epoch [80/143], Step [100/144], Loss: 0.004056304227560759\n","Epoch [81/143], Step [100/144], Loss: 0.014221841469407082\n","Epoch [82/143], Step [100/144], Loss: 0.0011681827018037438\n","Epoch [83/143], Step [100/144], Loss: 0.011348932050168514\n","Epoch [84/143], Step [100/144], Loss: 0.011757257394492626\n","Epoch [85/143], Step [100/144], Loss: 0.008755899034440517\n","Epoch [86/143], Step [100/144], Loss: 0.006414840929210186\n","Epoch [87/143], Step [100/144], Loss: 0.014999466016888618\n","Epoch [88/143], Step [100/144], Loss: 0.0028330576606094837\n","Epoch [89/143], Step [100/144], Loss: 0.0028652718756347895\n","Epoch [90/143], Step [100/144], Loss: 0.008766806684434414\n","Epoch [91/143], Step [100/144], Loss: 0.0022568609565496445\n","Epoch [92/143], Step [100/144], Loss: 0.02793528139591217\n","Epoch [93/143], Step [100/144], Loss: 0.005885134916752577\n","Epoch [94/143], Step [100/144], Loss: 0.007467602379620075\n","Epoch [95/143], Step [100/144], Loss: 0.009151737205684185\n","Epoch [96/143], Step [100/144], Loss: 0.014029166661202908\n","Epoch [97/143], Step [100/144], Loss: 0.012007765471935272\n","Epoch [98/143], Step [100/144], Loss: 0.004429374355822802\n","Epoch [99/143], Step [100/144], Loss: 0.021641172468662262\n","Epoch [100/143], Step [100/144], Loss: 0.008632407523691654\n","Epoch [101/143], Step [100/144], Loss: 0.022008292376995087\n","Epoch [102/143], Step [100/144], Loss: 0.004807589575648308\n","Epoch [103/143], Step [100/144], Loss: 0.005751322954893112\n","Epoch [104/143], Step [100/144], Loss: 0.03176078572869301\n","Epoch [105/143], Step [100/144], Loss: 0.004226775374263525\n","Epoch [106/143], Step [100/144], Loss: 0.003743850626051426\n","Epoch [107/143], Step [100/144], Loss: 0.0013217800296843052\n","Epoch [108/143], Step [100/144], Loss: 0.009719996713101864\n","Epoch [109/143], Step [100/144], Loss: 0.007601965684443712\n","Epoch [110/143], Step [100/144], Loss: 0.024516647681593895\n","Epoch [111/143], Step [100/144], Loss: 0.001195924123749137\n","Epoch [112/143], Step [100/144], Loss: 0.019497668370604515\n","Epoch [113/143], Step [100/144], Loss: 0.0064771766774356365\n","Epoch [114/143], Step [100/144], Loss: 0.0057991002686321735\n","Epoch [115/143], Step [100/144], Loss: 0.005531518254429102\n","Epoch [116/143], Step [100/144], Loss: 0.03351695463061333\n","Epoch [117/143], Step [100/144], Loss: 0.0033369138836860657\n","Epoch [118/143], Step [100/144], Loss: 0.024090927094221115\n","Epoch [119/143], Step [100/144], Loss: 0.006244088523089886\n","Epoch [120/143], Step [100/144], Loss: 0.006025655660778284\n","Epoch [121/143], Step [100/144], Loss: 0.003472474403679371\n","Epoch [122/143], Step [100/144], Loss: 0.005621999502182007\n","Epoch [123/143], Step [100/144], Loss: 0.001811638823710382\n","Epoch [124/143], Step [100/144], Loss: 0.009412279352545738\n","Epoch [125/143], Step [100/144], Loss: 0.002107638632878661\n","Epoch [126/143], Step [100/144], Loss: 0.01676364243030548\n","Epoch [127/143], Step [100/144], Loss: 0.0136521365493536\n","Epoch [128/143], Step [100/144], Loss: 0.005220983177423477\n","Epoch [129/143], Step [100/144], Loss: 0.0008511940250173211\n","Epoch [130/143], Step [100/144], Loss: 0.011817540973424911\n","Epoch [131/143], Step [100/144], Loss: 0.00978903379291296\n","Epoch [132/143], Step [100/144], Loss: 0.0009807011811062694\n","Epoch [133/143], Step [100/144], Loss: 0.002637946279719472\n","Epoch [134/143], Step [100/144], Loss: 0.010029519908130169\n","Epoch [135/143], Step [100/144], Loss: 0.0034405875485390425\n","Epoch [136/143], Step [100/144], Loss: 0.0033768864814192057\n","Epoch [137/143], Step [100/144], Loss: 0.003564226906746626\n","Epoch [138/143], Step [100/144], Loss: 0.009605216793715954\n","Epoch [139/143], Step [100/144], Loss: 0.002011500531807542\n","Epoch [140/143], Step [100/144], Loss: 0.012144693173468113\n","Epoch [141/143], Step [100/144], Loss: 0.016884421929717064\n","Epoch [142/143], Step [100/144], Loss: 0.0038422367069870234\n","Epoch [143/143], Step [100/144], Loss: 0.005041071679443121\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpyRCB-Rk0nZ","executionInfo":{"status":"ok","timestamp":1623967302157,"user_tz":-540,"elapsed":827,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"f9858390-4fd3-4cae-edfe-623d3d1ef0f1"},"source":["# Test the model\n","test_dataset = CustomDataset(X_test[30000:], y_test[30000:])\n","test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (images, labels) in enumerate(test_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        images = images[:, 10:]\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 100.0\n","Iteration:1,  Accuracy: 100.0\n","Iteration:2,  Accuracy: 99.66666666666667\n","Iteration:3,  Accuracy: 99.5\n","Iteration:4,  Accuracy: 99.4\n","Iteration:5,  Accuracy: 99.33333333333333\n","Iteration:6,  Accuracy: 99.28571428571429\n","Iteration:7,  Accuracy: 99.125\n","Iteration:8,  Accuracy: 99.0\n","Iteration:9,  Accuracy: 99.1\n","Iteration:10,  Accuracy: 99.18181818181819\n","Iteration:11,  Accuracy: 99.25\n","Iteration:12,  Accuracy: 99.3076923076923\n","Iteration:13,  Accuracy: 99.14285714285714\n","Iteration:14,  Accuracy: 99.2\n","Iteration:15,  Accuracy: 99.25\n","Iteration:16,  Accuracy: 99.29411764705883\n","Iteration:17,  Accuracy: 99.27777777777777\n","Iteration:18,  Accuracy: 99.3157894736842\n","Iteration:19,  Accuracy: 99.35\n","Iteration:20,  Accuracy: 99.38095238095238\n","Iteration:21,  Accuracy: 99.36363636363636\n","Iteration:22,  Accuracy: 99.3913043478261\n","Iteration:23,  Accuracy: 99.41666666666667\n","Iteration:24,  Accuracy: 99.36\n","Iteration:25,  Accuracy: 99.3076923076923\n","Iteration:26,  Accuracy: 99.29629629629629\n","Iteration:27,  Accuracy: 99.28571428571429\n","Iteration:28,  Accuracy: 99.27586206896552\n","Iteration:29,  Accuracy: 99.23333333333333\n","Iteration:30,  Accuracy: 99.2258064516129\n","Iteration:31,  Accuracy: 99.21875\n","Iteration:32,  Accuracy: 99.18181818181819\n","Iteration:33,  Accuracy: 99.1470588235294\n","Iteration:34,  Accuracy: 99.17142857142858\n","Iteration:35,  Accuracy: 99.16666666666667\n","Iteration:36,  Accuracy: 99.13513513513513\n","Iteration:37,  Accuracy: 99.15789473684211\n","Iteration:38,  Accuracy: 99.15384615384616\n","Iteration:39,  Accuracy: 99.175\n","Iteration:40,  Accuracy: 99.1951219512195\n","Iteration:41,  Accuracy: 99.21428571428571\n","Iteration:42,  Accuracy: 99.23255813953489\n","Iteration:43,  Accuracy: 99.25\n","Iteration:44,  Accuracy: 99.26666666666667\n","Iteration:45,  Accuracy: 99.26086956521739\n","Iteration:46,  Accuracy: 99.27659574468085\n","Iteration:47,  Accuracy: 99.27083333333333\n","Iteration:48,  Accuracy: 99.28571428571429\n","Iteration:49,  Accuracy: 99.3\n","Iteration:50,  Accuracy: 99.31372549019608\n","Iteration:51,  Accuracy: 99.3076923076923\n","Iteration:52,  Accuracy: 99.32075471698113\n","Iteration:53,  Accuracy: 99.33333333333333\n","Iteration:54,  Accuracy: 99.34545454545454\n","Iteration:55,  Accuracy: 99.33928571428571\n","Iteration:56,  Accuracy: 99.35087719298245\n","Iteration:57,  Accuracy: 99.36206896551724\n","Iteration:58,  Accuracy: 99.37288135593221\n","Iteration:59,  Accuracy: 99.38333333333334\n","Iteration:60,  Accuracy: 99.39344262295081\n","Iteration:61,  Accuracy: 99.40322580645162\n","Iteration:62,  Accuracy: 99.41269841269842\n","Iteration:63,  Accuracy: 99.421875\n","Iteration:64,  Accuracy: 99.38461538461539\n","Iteration:65,  Accuracy: 99.37878787878788\n","Iteration:66,  Accuracy: 99.3731343283582\n","Iteration:67,  Accuracy: 99.38235294117646\n","Iteration:68,  Accuracy: 99.3623188405797\n","Iteration:69,  Accuracy: 99.37142857142857\n","Iteration:70,  Accuracy: 99.36619718309859\n","Iteration:71,  Accuracy: 99.375\n","Iteration:72,  Accuracy: 99.38356164383562\n","Iteration:73,  Accuracy: 99.37837837837837\n","Iteration:74,  Accuracy: 99.38666666666667\n","Iteration:75,  Accuracy: 99.38157894736842\n","Iteration:76,  Accuracy: 99.36363636363636\n","Iteration:77,  Accuracy: 99.33333333333333\n","Iteration:78,  Accuracy: 99.34177215189874\n","Iteration:79,  Accuracy: 99.325\n","Iteration:80,  Accuracy: 99.32098765432099\n","Iteration:81,  Accuracy: 99.3048780487805\n","Iteration:82,  Accuracy: 99.27710843373494\n","Iteration:83,  Accuracy: 99.26190476190476\n","Iteration:84,  Accuracy: 99.27058823529411\n","Iteration:85,  Accuracy: 99.25581395348837\n","Iteration:86,  Accuracy: 99.25287356321839\n","Iteration:87,  Accuracy: 99.26136363636364\n","Iteration:88,  Accuracy: 99.26966292134831\n","Iteration:89,  Accuracy: 99.27777777777777\n","Iteration:90,  Accuracy: 99.27472527472527\n","Iteration:91,  Accuracy: 99.26086956521739\n","Iteration:92,  Accuracy: 99.25806451612904\n","Iteration:93,  Accuracy: 99.25381089436094\n","Test Accuracy of the model on the 10000 test images: 99.25381089436094 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ock9DEh1QO_1"},"source":["# Multi Class Classification\n"]},{"cell_type":"code","metadata":{"id":"O3JquLdnVdyE","executionInfo":{"status":"ok","timestamp":1624277181167,"user_tz":-540,"elapsed":323,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}}},"source":["X_train_0721 = np.load(\"X_train_0721.npy\")\n","y_train_0721 = np.load(\"y_train_0721.npy\")\n","X_test_0721 = np.load(\"X_test_0721.npy\")\n","y_test_0721 = np.load(\"y_test_0721.npy\")\n","\n","X_train_0723 = np.load(\"X_train_0723.npy\")\n","y_train_0723 = np.load(\"y_train_0723.npy\")\n","X_test_0723 = np.load(\"X_test_0723.npy\")\n","y_test_0723 = np.load(\"y_test_0723.npy\")"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufw_wBbDKA8_","executionInfo":{"elapsed":662,"status":"ok","timestamp":1623651267494,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"25d3d01a-ae72-4114-82f2-5855f46348e7"},"source":["X_test_0721.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5965, 160)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"JRaKoAKlQko5","executionInfo":{"status":"ok","timestamp":1624277176838,"user_tz":-540,"elapsed":13,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}}},"source":["X_train = np.concatenate(( X_train_0721, X_train_0723), axis=0)\n","y_train = np.concatenate(( y_train_0721, y_train_0723), axis=0)\n","X_test = np.concatenate(( X_test_0721, X_test_0723), axis=0)\n","y_test = np.concatenate((y_test_0721, y_test_0723), axis=0)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PbqyBVY_UZjv","executionInfo":{"elapsed":6,"status":"ok","timestamp":1623121389860,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"62fac4b7-8438-42fa-ab03-a3a354625951"},"source":["print(X_train.shape)\n","print(X_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(49669, 160)\n","(12419, 160)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_LYh7XEfR_Ke","executionInfo":{"elapsed":492,"status":"ok","timestamp":1623133278901,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"8d34c918-f251-4ca8-999f-4e8029093e46"},"source":["print(np.bincount(y_test_0721))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2944  485  295  299  299  311  306  323  703]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MgWUcyJcUd5j","executionInfo":{"elapsed":611,"status":"ok","timestamp":1623121393246,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"805fe203-042d-4065-84a0-8cfccacc310e"},"source":["print(np.bincount(y_train))\n","print(np.bincount(y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[25324  3960  2326  2359  2379  2449  2464  2589  5819]\n","[6331  991  581  591  595  612  616  647 1455]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LQo0IWBXvEF9"},"source":["### 아래는 20개짜리"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJqHemjFUnZM","executionInfo":{"elapsed":5692043,"status":"ok","timestamp":1623127319728,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"a20fc91c-dc43-4bed-894c-d96240637e71"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 9\n","batch_size = 512\n","num_epochs = len(y_train) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train, y_train)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=512, shuffle=True)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/331], Step [100/332], Loss: 0.561526894569397\n","Epoch [1/331], Step [200/332], Loss: 0.41244256496429443\n","Epoch [1/331], Step [300/332], Loss: 0.5402791500091553\n","Epoch [2/331], Step [100/332], Loss: 0.3541595935821533\n","Epoch [2/331], Step [200/332], Loss: 0.3199812173843384\n","Epoch [2/331], Step [300/332], Loss: 0.3521575927734375\n","Epoch [3/331], Step [100/332], Loss: 0.18315131962299347\n","Epoch [3/331], Step [200/332], Loss: 0.16422076523303986\n","Epoch [3/331], Step [300/332], Loss: 0.17044837772846222\n","Epoch [4/331], Step [100/332], Loss: 0.0695042535662651\n","Epoch [4/331], Step [200/332], Loss: 0.20512594282627106\n","Epoch [4/331], Step [300/332], Loss: 0.07928928732872009\n","Epoch [5/331], Step [100/332], Loss: 0.03247762471437454\n","Epoch [5/331], Step [200/332], Loss: 0.05850009247660637\n","Epoch [5/331], Step [300/332], Loss: 0.04632242023944855\n","Epoch [6/331], Step [100/332], Loss: 0.04940245673060417\n","Epoch [6/331], Step [200/332], Loss: 0.05454302579164505\n","Epoch [6/331], Step [300/332], Loss: 0.10311055183410645\n","Epoch [7/331], Step [100/332], Loss: 0.0188857764005661\n","Epoch [7/331], Step [200/332], Loss: 0.06265817582607269\n","Epoch [7/331], Step [300/332], Loss: 0.04217376187443733\n","Epoch [8/331], Step [100/332], Loss: 0.03782106935977936\n","Epoch [8/331], Step [200/332], Loss: 0.003748188493773341\n","Epoch [8/331], Step [300/332], Loss: 0.02964966371655464\n","Epoch [9/331], Step [100/332], Loss: 0.027854932472109795\n","Epoch [9/331], Step [200/332], Loss: 0.03461162745952606\n","Epoch [9/331], Step [300/332], Loss: 0.009368502534925938\n","Epoch [10/331], Step [100/332], Loss: 0.022236518561840057\n","Epoch [10/331], Step [200/332], Loss: 0.04298508167266846\n","Epoch [10/331], Step [300/332], Loss: 0.0072363875806331635\n","Epoch [11/331], Step [100/332], Loss: 0.010924759320914745\n","Epoch [11/331], Step [200/332], Loss: 0.031463876366615295\n","Epoch [11/331], Step [300/332], Loss: 0.017752282321453094\n","Epoch [12/331], Step [100/332], Loss: 0.00737238023430109\n","Epoch [12/331], Step [200/332], Loss: 0.03794717416167259\n","Epoch [12/331], Step [300/332], Loss: 0.002617762889713049\n","Epoch [13/331], Step [100/332], Loss: 0.00769030349329114\n","Epoch [13/331], Step [200/332], Loss: 0.017742332071065903\n","Epoch [13/331], Step [300/332], Loss: 0.0036880739498883486\n","Epoch [14/331], Step [100/332], Loss: 0.02934373915195465\n","Epoch [14/331], Step [200/332], Loss: 0.022526150569319725\n","Epoch [14/331], Step [300/332], Loss: 0.023692278191447258\n","Epoch [15/331], Step [100/332], Loss: 0.012979493476450443\n","Epoch [15/331], Step [200/332], Loss: 0.04841849207878113\n","Epoch [15/331], Step [300/332], Loss: 0.06242722272872925\n","Epoch [16/331], Step [100/332], Loss: 0.004339995328336954\n","Epoch [16/331], Step [200/332], Loss: 0.05144451931118965\n","Epoch [16/331], Step [300/332], Loss: 0.014301275834441185\n","Epoch [17/331], Step [100/332], Loss: 0.019565757364034653\n","Epoch [17/331], Step [200/332], Loss: 0.0030571662355214357\n","Epoch [17/331], Step [300/332], Loss: 0.03612875938415527\n","Epoch [18/331], Step [100/332], Loss: 0.002430595923215151\n","Epoch [18/331], Step [200/332], Loss: 0.002290944801643491\n","Epoch [18/331], Step [300/332], Loss: 0.011103259399533272\n","Epoch [19/331], Step [100/332], Loss: 0.0027041323482990265\n","Epoch [19/331], Step [200/332], Loss: 0.046275217086076736\n","Epoch [19/331], Step [300/332], Loss: 0.006263370160013437\n","Epoch [20/331], Step [100/332], Loss: 0.00677002128213644\n","Epoch [20/331], Step [200/332], Loss: 0.010820259340107441\n","Epoch [20/331], Step [300/332], Loss: 0.10563281178474426\n","Epoch [21/331], Step [100/332], Loss: 0.008508564904332161\n","Epoch [21/331], Step [200/332], Loss: 0.019996680319309235\n","Epoch [21/331], Step [300/332], Loss: 0.0028500899206846952\n","Epoch [22/331], Step [100/332], Loss: 0.022558271884918213\n","Epoch [22/331], Step [200/332], Loss: 0.003029439365491271\n","Epoch [22/331], Step [300/332], Loss: 0.009690227918326855\n","Epoch [23/331], Step [100/332], Loss: 0.009534751065075397\n","Epoch [23/331], Step [200/332], Loss: 0.025596756488084793\n","Epoch [23/331], Step [300/332], Loss: 0.016478706151247025\n","Epoch [24/331], Step [100/332], Loss: 0.0027137110009789467\n","Epoch [24/331], Step [200/332], Loss: 0.004124391358345747\n","Epoch [24/331], Step [300/332], Loss: 0.00919071864336729\n","Epoch [25/331], Step [100/332], Loss: 0.02761095017194748\n","Epoch [25/331], Step [200/332], Loss: 0.009646665304899216\n","Epoch [25/331], Step [300/332], Loss: 0.04886331409215927\n","Epoch [26/331], Step [100/332], Loss: 0.0031714895740151405\n","Epoch [26/331], Step [200/332], Loss: 0.023185277357697487\n","Epoch [26/331], Step [300/332], Loss: 0.002560631837695837\n","Epoch [27/331], Step [100/332], Loss: 0.016544176265597343\n","Epoch [27/331], Step [200/332], Loss: 0.06293320655822754\n","Epoch [27/331], Step [300/332], Loss: 0.020728886127471924\n","Epoch [28/331], Step [100/332], Loss: 0.0056962682865560055\n","Epoch [28/331], Step [200/332], Loss: 0.0036819390952587128\n","Epoch [28/331], Step [300/332], Loss: 0.04458867013454437\n","Epoch [29/331], Step [100/332], Loss: 0.0036003589630126953\n","Epoch [29/331], Step [200/332], Loss: 0.02348608709871769\n","Epoch [29/331], Step [300/332], Loss: 0.012475626543164253\n","Epoch [30/331], Step [100/332], Loss: 0.02138388901948929\n","Epoch [30/331], Step [200/332], Loss: 0.014994945377111435\n","Epoch [30/331], Step [300/332], Loss: 0.023516841232776642\n","Epoch [31/331], Step [100/332], Loss: 0.044435836374759674\n","Epoch [31/331], Step [200/332], Loss: 0.027900241315364838\n","Epoch [31/331], Step [300/332], Loss: 0.0015566275687888265\n","Epoch [32/331], Step [100/332], Loss: 0.0008161164005286992\n","Epoch [32/331], Step [200/332], Loss: 0.0029674042016267776\n","Epoch [32/331], Step [300/332], Loss: 0.02812563255429268\n","Epoch [33/331], Step [100/332], Loss: 0.017201529815793037\n","Epoch [33/331], Step [200/332], Loss: 0.008420080877840519\n","Epoch [33/331], Step [300/332], Loss: 0.017239192500710487\n","Epoch [34/331], Step [100/332], Loss: 0.05253784731030464\n","Epoch [34/331], Step [200/332], Loss: 0.028038565069437027\n","Epoch [34/331], Step [300/332], Loss: 0.001548434840515256\n","Epoch [35/331], Step [100/332], Loss: 0.05869840458035469\n","Epoch [35/331], Step [200/332], Loss: 0.0014730473048985004\n","Epoch [35/331], Step [300/332], Loss: 0.02747867815196514\n","Epoch [36/331], Step [100/332], Loss: 0.03441843017935753\n","Epoch [36/331], Step [200/332], Loss: 0.004588794894516468\n","Epoch [36/331], Step [300/332], Loss: 0.03511876240372658\n","Epoch [37/331], Step [100/332], Loss: 0.042118873447179794\n","Epoch [37/331], Step [200/332], Loss: 0.06125984713435173\n","Epoch [37/331], Step [300/332], Loss: 0.002908561145886779\n","Epoch [38/331], Step [100/332], Loss: 0.0006809252663515508\n","Epoch [38/331], Step [200/332], Loss: 0.03208876773715019\n","Epoch [38/331], Step [300/332], Loss: 0.10095875710248947\n","Epoch [39/331], Step [100/332], Loss: 0.009138469584286213\n","Epoch [39/331], Step [200/332], Loss: 0.00527922110632062\n","Epoch [39/331], Step [300/332], Loss: 0.0029003932140767574\n","Epoch [40/331], Step [100/332], Loss: 0.019354837015271187\n","Epoch [40/331], Step [200/332], Loss: 0.01114883553236723\n","Epoch [40/331], Step [300/332], Loss: 0.014171346090734005\n","Epoch [41/331], Step [100/332], Loss: 0.0020092963241040707\n","Epoch [41/331], Step [200/332], Loss: 0.0008625552291050553\n","Epoch [41/331], Step [300/332], Loss: 0.02410171739757061\n","Epoch [42/331], Step [100/332], Loss: 0.0024143510963767767\n","Epoch [42/331], Step [200/332], Loss: 0.00588990980759263\n","Epoch [42/331], Step [300/332], Loss: 0.0018335591303184628\n","Epoch [43/331], Step [100/332], Loss: 0.0025877978187054396\n","Epoch [43/331], Step [200/332], Loss: 0.0011965956073254347\n","Epoch [43/331], Step [300/332], Loss: 0.0010100874351337552\n","Epoch [44/331], Step [100/332], Loss: 0.006960341241210699\n","Epoch [44/331], Step [200/332], Loss: 0.019771112129092216\n","Epoch [44/331], Step [300/332], Loss: 0.046110957860946655\n","Epoch [45/331], Step [100/332], Loss: 0.02920250967144966\n","Epoch [45/331], Step [200/332], Loss: 0.00482534896582365\n","Epoch [45/331], Step [300/332], Loss: 0.020027529448270798\n","Epoch [46/331], Step [100/332], Loss: 0.02270418591797352\n","Epoch [46/331], Step [200/332], Loss: 0.04441392049193382\n","Epoch [46/331], Step [300/332], Loss: 0.0013930050190538168\n","Epoch [47/331], Step [100/332], Loss: 0.004714540671557188\n","Epoch [47/331], Step [200/332], Loss: 0.01110418513417244\n","Epoch [47/331], Step [300/332], Loss: 0.0011872766772285104\n","Epoch [48/331], Step [100/332], Loss: 0.010064929723739624\n","Epoch [48/331], Step [200/332], Loss: 0.009299170225858688\n","Epoch [48/331], Step [300/332], Loss: 0.004001994617283344\n","Epoch [49/331], Step [100/332], Loss: 0.013848433271050453\n","Epoch [49/331], Step [200/332], Loss: 0.008558317087590694\n","Epoch [49/331], Step [300/332], Loss: 0.009584976360201836\n","Epoch [50/331], Step [100/332], Loss: 0.05088941380381584\n","Epoch [50/331], Step [200/332], Loss: 0.010221067816019058\n","Epoch [50/331], Step [300/332], Loss: 0.034530892968177795\n","Epoch [51/331], Step [100/332], Loss: 0.02943764068186283\n","Epoch [51/331], Step [200/332], Loss: 0.00612410856410861\n","Epoch [51/331], Step [300/332], Loss: 0.037846945226192474\n","Epoch [52/331], Step [100/332], Loss: 0.002335543045774102\n","Epoch [52/331], Step [200/332], Loss: 0.0017033261246979237\n","Epoch [52/331], Step [300/332], Loss: 0.02412388101220131\n","Epoch [53/331], Step [100/332], Loss: 0.022271450608968735\n","Epoch [53/331], Step [200/332], Loss: 0.018348341807723045\n","Epoch [53/331], Step [300/332], Loss: 0.005110940895974636\n","Epoch [54/331], Step [100/332], Loss: 0.0023914428893476725\n","Epoch [54/331], Step [200/332], Loss: 0.008114617317914963\n","Epoch [54/331], Step [300/332], Loss: 0.051506973803043365\n","Epoch [55/331], Step [100/332], Loss: 0.05605306103825569\n","Epoch [55/331], Step [200/332], Loss: 0.007485209498554468\n","Epoch [55/331], Step [300/332], Loss: 0.00595132727175951\n","Epoch [56/331], Step [100/332], Loss: 0.01033273059874773\n","Epoch [56/331], Step [200/332], Loss: 0.0009268671274185181\n","Epoch [56/331], Step [300/332], Loss: 0.00916854478418827\n","Epoch [57/331], Step [100/332], Loss: 0.004632632248103619\n","Epoch [57/331], Step [200/332], Loss: 0.004052362870424986\n","Epoch [57/331], Step [300/332], Loss: 0.0015649745473638177\n","Epoch [58/331], Step [100/332], Loss: 0.010798239149153233\n","Epoch [58/331], Step [200/332], Loss: 0.006688928697258234\n","Epoch [58/331], Step [300/332], Loss: 0.034334663301706314\n","Epoch [59/331], Step [100/332], Loss: 0.003928986843675375\n","Epoch [59/331], Step [200/332], Loss: 0.022948559373617172\n","Epoch [59/331], Step [300/332], Loss: 0.06248980760574341\n","Epoch [60/331], Step [100/332], Loss: 0.048477496951818466\n","Epoch [60/331], Step [200/332], Loss: 0.003378338413313031\n","Epoch [60/331], Step [300/332], Loss: 0.052356913685798645\n","Epoch [61/331], Step [100/332], Loss: 0.0021608166862279177\n","Epoch [61/331], Step [200/332], Loss: 0.03929629921913147\n","Epoch [61/331], Step [300/332], Loss: 0.005364388693124056\n","Epoch [62/331], Step [100/332], Loss: 0.003743852023035288\n","Epoch [62/331], Step [200/332], Loss: 0.01757904887199402\n","Epoch [62/331], Step [300/332], Loss: 0.007905124686658382\n","Epoch [63/331], Step [100/332], Loss: 0.010721687227487564\n","Epoch [63/331], Step [200/332], Loss: 0.0030895571690052748\n","Epoch [63/331], Step [300/332], Loss: 0.010486610233783722\n","Epoch [64/331], Step [100/332], Loss: 0.0008625907939858735\n","Epoch [64/331], Step [200/332], Loss: 0.021044503897428513\n","Epoch [64/331], Step [300/332], Loss: 0.030298404395580292\n","Epoch [65/331], Step [100/332], Loss: 0.007082940079271793\n","Epoch [65/331], Step [200/332], Loss: 0.058997612446546555\n","Epoch [65/331], Step [300/332], Loss: 0.01888609677553177\n","Epoch [66/331], Step [100/332], Loss: 0.001477177836932242\n","Epoch [66/331], Step [200/332], Loss: 0.0014284460339695215\n","Epoch [66/331], Step [300/332], Loss: 0.031472913920879364\n","Epoch [67/331], Step [100/332], Loss: 0.014673673547804356\n","Epoch [67/331], Step [200/332], Loss: 0.027950191870331764\n","Epoch [67/331], Step [300/332], Loss: 0.00639133108779788\n","Epoch [68/331], Step [100/332], Loss: 0.0032476948108524084\n","Epoch [68/331], Step [200/332], Loss: 0.0010804341873154044\n","Epoch [68/331], Step [300/332], Loss: 0.03536200150847435\n","Epoch [69/331], Step [100/332], Loss: 0.0144191300496459\n","Epoch [69/331], Step [200/332], Loss: 0.00491683604195714\n","Epoch [69/331], Step [300/332], Loss: 0.004184095188975334\n","Epoch [70/331], Step [100/332], Loss: 0.0046478197909891605\n","Epoch [70/331], Step [200/332], Loss: 0.0070204599760472775\n","Epoch [70/331], Step [300/332], Loss: 0.01565396599471569\n","Epoch [71/331], Step [100/332], Loss: 0.0010205308208242059\n","Epoch [71/331], Step [200/332], Loss: 0.0082403514534235\n","Epoch [71/331], Step [300/332], Loss: 0.008099839091300964\n","Epoch [72/331], Step [100/332], Loss: 0.005234148818999529\n","Epoch [72/331], Step [200/332], Loss: 0.007829721085727215\n","Epoch [72/331], Step [300/332], Loss: 0.00020699251035694033\n","Epoch [73/331], Step [100/332], Loss: 0.0065841819159686565\n","Epoch [73/331], Step [200/332], Loss: 0.0011545892339199781\n","Epoch [73/331], Step [300/332], Loss: 0.005010917782783508\n","Epoch [74/331], Step [100/332], Loss: 0.01306884828954935\n","Epoch [74/331], Step [200/332], Loss: 0.004494888242334127\n","Epoch [74/331], Step [300/332], Loss: 0.009533341974020004\n","Epoch [75/331], Step [100/332], Loss: 0.01653883047401905\n","Epoch [75/331], Step [200/332], Loss: 0.013504174537956715\n","Epoch [75/331], Step [300/332], Loss: 0.004312311299145222\n","Epoch [76/331], Step [100/332], Loss: 0.0955275148153305\n","Epoch [76/331], Step [200/332], Loss: 0.027209850028157234\n","Epoch [76/331], Step [300/332], Loss: 0.0010683173313736916\n","Epoch [77/331], Step [100/332], Loss: 0.00657490873709321\n","Epoch [77/331], Step [200/332], Loss: 0.00959695503115654\n","Epoch [77/331], Step [300/332], Loss: 0.010230663232505322\n","Epoch [78/331], Step [100/332], Loss: 0.002112947404384613\n","Epoch [78/331], Step [200/332], Loss: 0.002665631240233779\n","Epoch [78/331], Step [300/332], Loss: 0.04352318495512009\n","Epoch [79/331], Step [100/332], Loss: 0.009374176152050495\n","Epoch [79/331], Step [200/332], Loss: 0.005349369253963232\n","Epoch [79/331], Step [300/332], Loss: 0.00548152718693018\n","Epoch [80/331], Step [100/332], Loss: 0.1134311705827713\n","Epoch [80/331], Step [200/332], Loss: 0.007062888238579035\n","Epoch [80/331], Step [300/332], Loss: 0.002748830709606409\n","Epoch [81/331], Step [100/332], Loss: 0.005022330209612846\n","Epoch [81/331], Step [200/332], Loss: 0.0015972531400620937\n","Epoch [81/331], Step [300/332], Loss: 0.0072969780303537846\n","Epoch [82/331], Step [100/332], Loss: 0.0037804758176207542\n","Epoch [82/331], Step [200/332], Loss: 0.00042208353988826275\n","Epoch [82/331], Step [300/332], Loss: 0.006851118989288807\n","Epoch [83/331], Step [100/332], Loss: 0.0008210341911762953\n","Epoch [83/331], Step [200/332], Loss: 0.008711989969015121\n","Epoch [83/331], Step [300/332], Loss: 0.011193796060979366\n","Epoch [84/331], Step [100/332], Loss: 0.01759304292500019\n","Epoch [84/331], Step [200/332], Loss: 0.0050116050988435745\n","Epoch [84/331], Step [300/332], Loss: 0.007266189903020859\n","Epoch [85/331], Step [100/332], Loss: 0.015741579234600067\n","Epoch [85/331], Step [200/332], Loss: 0.0047582522965967655\n","Epoch [85/331], Step [300/332], Loss: 0.0028172365855425596\n","Epoch [86/331], Step [100/332], Loss: 0.012487593106925488\n","Epoch [86/331], Step [200/332], Loss: 0.005237750709056854\n","Epoch [86/331], Step [300/332], Loss: 0.0038348198868334293\n","Epoch [87/331], Step [100/332], Loss: 0.031441185623407364\n","Epoch [87/331], Step [200/332], Loss: 0.011318420059978962\n","Epoch [87/331], Step [300/332], Loss: 0.02525724098086357\n","Epoch [88/331], Step [100/332], Loss: 0.02130996435880661\n","Epoch [88/331], Step [200/332], Loss: 0.001831195899285376\n","Epoch [88/331], Step [300/332], Loss: 0.011895080097019672\n","Epoch [89/331], Step [100/332], Loss: 0.0011655609123408794\n","Epoch [89/331], Step [200/332], Loss: 0.02816932089626789\n","Epoch [89/331], Step [300/332], Loss: 0.000815739156678319\n","Epoch [90/331], Step [100/332], Loss: 0.0005972242215648293\n","Epoch [90/331], Step [200/332], Loss: 0.011560099199414253\n","Epoch [90/331], Step [300/332], Loss: 0.004361777100712061\n","Epoch [91/331], Step [100/332], Loss: 0.008302707225084305\n","Epoch [91/331], Step [200/332], Loss: 0.019648103043437004\n","Epoch [91/331], Step [300/332], Loss: 0.008093367330729961\n","Epoch [92/331], Step [100/332], Loss: 0.027983715757727623\n","Epoch [92/331], Step [200/332], Loss: 0.02794564887881279\n","Epoch [92/331], Step [300/332], Loss: 0.04293537884950638\n","Epoch [93/331], Step [100/332], Loss: 0.006239060312509537\n","Epoch [93/331], Step [200/332], Loss: 0.0018545858329162002\n","Epoch [93/331], Step [300/332], Loss: 0.028392527252435684\n","Epoch [94/331], Step [100/332], Loss: 0.01904379017651081\n","Epoch [94/331], Step [200/332], Loss: 0.008205288089811802\n","Epoch [94/331], Step [300/332], Loss: 0.0008831950835883617\n","Epoch [95/331], Step [100/332], Loss: 0.003923485521227121\n","Epoch [95/331], Step [200/332], Loss: 0.00039348326390609145\n","Epoch [95/331], Step [300/332], Loss: 0.0006586491363123059\n","Epoch [96/331], Step [100/332], Loss: 0.001422713277861476\n","Epoch [96/331], Step [200/332], Loss: 0.05203213542699814\n","Epoch [96/331], Step [300/332], Loss: 0.0004468423721846193\n","Epoch [97/331], Step [100/332], Loss: 0.0007985278498381376\n","Epoch [97/331], Step [200/332], Loss: 0.002853445475921035\n","Epoch [97/331], Step [300/332], Loss: 0.00028464727802202106\n","Epoch [98/331], Step [100/332], Loss: 0.011306551285088062\n","Epoch [98/331], Step [200/332], Loss: 0.030568456277251244\n","Epoch [98/331], Step [300/332], Loss: 0.0005181496380828321\n","Epoch [99/331], Step [100/332], Loss: 0.0006604369846172631\n","Epoch [99/331], Step [200/332], Loss: 0.02294476144015789\n","Epoch [99/331], Step [300/332], Loss: 0.011885079555213451\n","Epoch [100/331], Step [100/332], Loss: 0.036197297275066376\n","Epoch [100/331], Step [200/332], Loss: 0.024845775216817856\n","Epoch [100/331], Step [300/332], Loss: 0.025704508647322655\n","Epoch [101/331], Step [100/332], Loss: 0.04403179511427879\n","Epoch [101/331], Step [200/332], Loss: 0.02088666893541813\n","Epoch [101/331], Step [300/332], Loss: 0.02611403353512287\n","Epoch [102/331], Step [100/332], Loss: 0.0009560998296365142\n","Epoch [102/331], Step [200/332], Loss: 0.007714895531535149\n","Epoch [102/331], Step [300/332], Loss: 0.007284242194145918\n","Epoch [103/331], Step [100/332], Loss: 0.003947420511394739\n","Epoch [103/331], Step [200/332], Loss: 0.000972093956079334\n","Epoch [103/331], Step [300/332], Loss: 0.000596633879467845\n","Epoch [104/331], Step [100/332], Loss: 0.001958634005859494\n","Epoch [104/331], Step [200/332], Loss: 0.01252412423491478\n","Epoch [104/331], Step [300/332], Loss: 0.02510579116642475\n","Epoch [105/331], Step [100/332], Loss: 0.007966757752001286\n","Epoch [105/331], Step [200/332], Loss: 0.02577611245214939\n","Epoch [105/331], Step [300/332], Loss: 0.020567184314131737\n","Epoch [106/331], Step [100/332], Loss: 0.02885720320045948\n","Epoch [106/331], Step [200/332], Loss: 0.013246918097138405\n","Epoch [106/331], Step [300/332], Loss: 0.011626094579696655\n","Epoch [107/331], Step [100/332], Loss: 0.0029959455132484436\n","Epoch [107/331], Step [200/332], Loss: 0.04579053074121475\n","Epoch [107/331], Step [300/332], Loss: 0.019871870055794716\n","Epoch [108/331], Step [100/332], Loss: 0.014103198423981667\n","Epoch [108/331], Step [200/332], Loss: 0.0032041852828115225\n","Epoch [108/331], Step [300/332], Loss: 0.027854284271597862\n","Epoch [109/331], Step [100/332], Loss: 0.0008367638802155852\n","Epoch [109/331], Step [200/332], Loss: 0.0060305120423436165\n","Epoch [109/331], Step [300/332], Loss: 0.0017773661529645324\n","Epoch [110/331], Step [100/332], Loss: 0.0005408854922279716\n","Epoch [110/331], Step [200/332], Loss: 0.016235284507274628\n","Epoch [110/331], Step [300/332], Loss: 0.021786412224173546\n","Epoch [111/331], Step [100/332], Loss: 0.0019232917111366987\n","Epoch [111/331], Step [200/332], Loss: 0.015087170526385307\n","Epoch [111/331], Step [300/332], Loss: 0.004276295658200979\n","Epoch [112/331], Step [100/332], Loss: 0.0008752690046094358\n","Epoch [112/331], Step [200/332], Loss: 0.013856973499059677\n","Epoch [112/331], Step [300/332], Loss: 0.0610116645693779\n","Epoch [113/331], Step [100/332], Loss: 0.010202991776168346\n","Epoch [113/331], Step [200/332], Loss: 0.0008668035152368248\n","Epoch [113/331], Step [300/332], Loss: 0.07170028984546661\n","Epoch [114/331], Step [100/332], Loss: 0.0032778847962617874\n","Epoch [114/331], Step [200/332], Loss: 0.010191615670919418\n","Epoch [114/331], Step [300/332], Loss: 0.0016741291619837284\n","Epoch [115/331], Step [100/332], Loss: 0.016011517494916916\n","Epoch [115/331], Step [200/332], Loss: 0.0459870845079422\n","Epoch [115/331], Step [300/332], Loss: 0.05906731262803078\n","Epoch [116/331], Step [100/332], Loss: 0.006955131888389587\n","Epoch [116/331], Step [200/332], Loss: 0.0513489805161953\n","Epoch [116/331], Step [300/332], Loss: 0.03836710751056671\n","Epoch [117/331], Step [100/332], Loss: 0.028225861489772797\n","Epoch [117/331], Step [200/332], Loss: 0.002439237665385008\n","Epoch [117/331], Step [300/332], Loss: 0.00518708536401391\n","Epoch [118/331], Step [100/332], Loss: 0.0052420590072870255\n","Epoch [118/331], Step [200/332], Loss: 0.005382285453379154\n","Epoch [118/331], Step [300/332], Loss: 0.0026842150837183\n","Epoch [119/331], Step [100/332], Loss: 0.0022643485572189093\n","Epoch [119/331], Step [200/332], Loss: 0.02754978835582733\n","Epoch [119/331], Step [300/332], Loss: 0.00049495481653139\n","Epoch [120/331], Step [100/332], Loss: 0.00694509269669652\n","Epoch [120/331], Step [200/332], Loss: 0.024666346609592438\n","Epoch [120/331], Step [300/332], Loss: 0.013956047594547272\n","Epoch [121/331], Step [100/332], Loss: 0.03238546475768089\n","Epoch [121/331], Step [200/332], Loss: 0.016890522092580795\n","Epoch [121/331], Step [300/332], Loss: 0.006684470921754837\n","Epoch [122/331], Step [100/332], Loss: 0.027294842526316643\n","Epoch [122/331], Step [200/332], Loss: 0.008429412730038166\n","Epoch [122/331], Step [300/332], Loss: 0.007347143720835447\n","Epoch [123/331], Step [100/332], Loss: 0.007183635141700506\n","Epoch [123/331], Step [200/332], Loss: 0.008264439180493355\n","Epoch [123/331], Step [300/332], Loss: 0.0032140323892235756\n","Epoch [124/331], Step [100/332], Loss: 0.0029355487786233425\n","Epoch [124/331], Step [200/332], Loss: 0.005722518544644117\n","Epoch [124/331], Step [300/332], Loss: 0.0050184763967990875\n","Epoch [125/331], Step [100/332], Loss: 0.002939211903139949\n","Epoch [125/331], Step [200/332], Loss: 0.014784223400056362\n","Epoch [125/331], Step [300/332], Loss: 0.012912721373140812\n","Epoch [126/331], Step [100/332], Loss: 0.015925228595733643\n","Epoch [126/331], Step [200/332], Loss: 0.03414778783917427\n","Epoch [126/331], Step [300/332], Loss: 0.001066063647158444\n","Epoch [127/331], Step [100/332], Loss: 0.0142212500795722\n","Epoch [127/331], Step [200/332], Loss: 0.0017890024464577436\n","Epoch [127/331], Step [300/332], Loss: 0.004360169637948275\n","Epoch [128/331], Step [100/332], Loss: 0.004133162088692188\n","Epoch [128/331], Step [200/332], Loss: 0.008279317989945412\n","Epoch [128/331], Step [300/332], Loss: 0.0041833193972706795\n","Epoch [129/331], Step [100/332], Loss: 0.0028837542049586773\n","Epoch [129/331], Step [200/332], Loss: 0.1239510253071785\n","Epoch [129/331], Step [300/332], Loss: 0.00368120358325541\n","Epoch [130/331], Step [100/332], Loss: 0.002391511108726263\n","Epoch [130/331], Step [200/332], Loss: 0.0015281355008482933\n","Epoch [130/331], Step [300/332], Loss: 0.02564837411046028\n","Epoch [131/331], Step [100/332], Loss: 0.04050393030047417\n","Epoch [131/331], Step [200/332], Loss: 0.003294304246082902\n","Epoch [131/331], Step [300/332], Loss: 0.00928101222962141\n","Epoch [132/331], Step [100/332], Loss: 0.005769290495663881\n","Epoch [132/331], Step [200/332], Loss: 0.05671335756778717\n","Epoch [132/331], Step [300/332], Loss: 0.0189468152821064\n","Epoch [133/331], Step [100/332], Loss: 0.031013965606689453\n","Epoch [133/331], Step [200/332], Loss: 0.019720343872904778\n","Epoch [133/331], Step [300/332], Loss: 0.015546628274023533\n","Epoch [134/331], Step [100/332], Loss: 0.00992276705801487\n","Epoch [134/331], Step [200/332], Loss: 0.07639887183904648\n","Epoch [134/331], Step [300/332], Loss: 0.048080410808324814\n","Epoch [135/331], Step [100/332], Loss: 0.008394049480557442\n","Epoch [135/331], Step [200/332], Loss: 0.018199903890490532\n","Epoch [135/331], Step [300/332], Loss: 0.07553958892822266\n","Epoch [136/331], Step [100/332], Loss: 0.02178305946290493\n","Epoch [136/331], Step [200/332], Loss: 0.0043786964379251\n","Epoch [136/331], Step [300/332], Loss: 0.015210160985589027\n","Epoch [137/331], Step [100/332], Loss: 0.0029289619997143745\n","Epoch [137/331], Step [200/332], Loss: 0.007585492916405201\n","Epoch [137/331], Step [300/332], Loss: 0.003844398306682706\n","Epoch [138/331], Step [100/332], Loss: 0.008020167239010334\n","Epoch [138/331], Step [200/332], Loss: 0.006515389773994684\n","Epoch [138/331], Step [300/332], Loss: 0.06806861609220505\n","Epoch [139/331], Step [100/332], Loss: 0.040862198919057846\n","Epoch [139/331], Step [200/332], Loss: 0.02450929582118988\n","Epoch [139/331], Step [300/332], Loss: 0.04141265153884888\n","Epoch [140/331], Step [100/332], Loss: 0.03936426714062691\n","Epoch [140/331], Step [200/332], Loss: 0.017352331429719925\n","Epoch [140/331], Step [300/332], Loss: 0.02665654942393303\n","Epoch [141/331], Step [100/332], Loss: 0.01014323253184557\n","Epoch [141/331], Step [200/332], Loss: 0.03174178674817085\n","Epoch [141/331], Step [300/332], Loss: 0.022176844999194145\n","Epoch [142/331], Step [100/332], Loss: 0.001292644883506\n","Epoch [142/331], Step [200/332], Loss: 0.07957896590232849\n","Epoch [142/331], Step [300/332], Loss: 0.010561644099652767\n","Epoch [143/331], Step [100/332], Loss: 0.0021922446321696043\n","Epoch [143/331], Step [200/332], Loss: 0.001019189367070794\n","Epoch [143/331], Step [300/332], Loss: 0.03954494372010231\n","Epoch [144/331], Step [100/332], Loss: 0.0017234607366845012\n","Epoch [144/331], Step [200/332], Loss: 0.010606693103909492\n","Epoch [144/331], Step [300/332], Loss: 0.007873065769672394\n","Epoch [145/331], Step [100/332], Loss: 0.01388197299093008\n","Epoch [145/331], Step [200/332], Loss: 0.027948668226599693\n","Epoch [145/331], Step [300/332], Loss: 0.0006249280413612723\n","Epoch [146/331], Step [100/332], Loss: 0.005077714566141367\n","Epoch [146/331], Step [200/332], Loss: 0.0008053786586970091\n","Epoch [146/331], Step [300/332], Loss: 0.0021592448465526104\n","Epoch [147/331], Step [100/332], Loss: 0.001007052487693727\n","Epoch [147/331], Step [200/332], Loss: 0.0008814648608677089\n","Epoch [147/331], Step [300/332], Loss: 0.0013152312021702528\n","Epoch [148/331], Step [100/332], Loss: 0.0012665223330259323\n","Epoch [148/331], Step [200/332], Loss: 0.01609683968126774\n","Epoch [148/331], Step [300/332], Loss: 0.005807255394756794\n","Epoch [149/331], Step [100/332], Loss: 0.002633317606523633\n","Epoch [149/331], Step [200/332], Loss: 0.008354362100362778\n","Epoch [149/331], Step [300/332], Loss: 0.002183164469897747\n","Epoch [150/331], Step [100/332], Loss: 0.014858582988381386\n","Epoch [150/331], Step [200/332], Loss: 0.0016171331517398357\n","Epoch [150/331], Step [300/332], Loss: 0.05944721773266792\n","Epoch [151/331], Step [100/332], Loss: 0.023747816681861877\n","Epoch [151/331], Step [200/332], Loss: 0.008157583884894848\n","Epoch [151/331], Step [300/332], Loss: 0.03573928028345108\n","Epoch [152/331], Step [100/332], Loss: 0.001291403197683394\n","Epoch [152/331], Step [200/332], Loss: 0.016667211428284645\n","Epoch [152/331], Step [300/332], Loss: 0.0034288978204131126\n","Epoch [153/331], Step [100/332], Loss: 0.03116462752223015\n","Epoch [153/331], Step [200/332], Loss: 0.02470714971423149\n","Epoch [153/331], Step [300/332], Loss: 0.041566744446754456\n","Epoch [154/331], Step [100/332], Loss: 0.07099549472332001\n","Epoch [154/331], Step [200/332], Loss: 0.09837207198143005\n","Epoch [154/331], Step [300/332], Loss: 0.01023288257420063\n","Epoch [155/331], Step [100/332], Loss: 0.02572554349899292\n","Epoch [155/331], Step [200/332], Loss: 0.02075727842748165\n","Epoch [155/331], Step [300/332], Loss: 0.03992597386240959\n","Epoch [156/331], Step [100/332], Loss: 0.05101380497217178\n","Epoch [156/331], Step [200/332], Loss: 0.012603712268173695\n","Epoch [156/331], Step [300/332], Loss: 0.0020919612143188715\n","Epoch [157/331], Step [100/332], Loss: 0.001617095316760242\n","Epoch [157/331], Step [200/332], Loss: 0.005592593923211098\n","Epoch [157/331], Step [300/332], Loss: 0.014080967754125595\n","Epoch [158/331], Step [100/332], Loss: 0.0034245122224092484\n","Epoch [158/331], Step [200/332], Loss: 0.0021921361330896616\n","Epoch [158/331], Step [300/332], Loss: 0.014011380262672901\n","Epoch [159/331], Step [100/332], Loss: 0.003978403750807047\n","Epoch [159/331], Step [200/332], Loss: 0.011399771086871624\n","Epoch [159/331], Step [300/332], Loss: 0.0011181037407368422\n","Epoch [160/331], Step [100/332], Loss: 0.006222084164619446\n","Epoch [160/331], Step [200/332], Loss: 0.014671804383397102\n","Epoch [160/331], Step [300/332], Loss: 0.010149886831641197\n","Epoch [161/331], Step [100/332], Loss: 0.007692376617342234\n","Epoch [161/331], Step [200/332], Loss: 0.006443023681640625\n","Epoch [161/331], Step [300/332], Loss: 0.02652304247021675\n","Epoch [162/331], Step [100/332], Loss: 0.008171089924871922\n","Epoch [162/331], Step [200/332], Loss: 0.006172996014356613\n","Epoch [162/331], Step [300/332], Loss: 0.09594062715768814\n","Epoch [163/331], Step [100/332], Loss: 0.006395981181412935\n","Epoch [163/331], Step [200/332], Loss: 0.015895772725343704\n","Epoch [163/331], Step [300/332], Loss: 0.005973184481263161\n","Epoch [164/331], Step [100/332], Loss: 0.0024147864896804094\n","Epoch [164/331], Step [200/332], Loss: 0.02497737482190132\n","Epoch [164/331], Step [300/332], Loss: 0.0024948674254119396\n","Epoch [165/331], Step [100/332], Loss: 0.005464558489620686\n","Epoch [165/331], Step [200/332], Loss: 0.12591661512851715\n","Epoch [165/331], Step [300/332], Loss: 0.006662373431026936\n","Epoch [166/331], Step [100/332], Loss: 0.0020160058047622442\n","Epoch [166/331], Step [200/332], Loss: 0.014046842232346535\n","Epoch [166/331], Step [300/332], Loss: 0.004238100256770849\n","Epoch [167/331], Step [100/332], Loss: 0.0018185657681897283\n","Epoch [167/331], Step [200/332], Loss: 0.011977511458098888\n","Epoch [167/331], Step [300/332], Loss: 0.028324006125330925\n","Epoch [168/331], Step [100/332], Loss: 0.0008688389207236469\n","Epoch [168/331], Step [200/332], Loss: 0.009175985120236874\n","Epoch [168/331], Step [300/332], Loss: 0.0014397972263395786\n","Epoch [169/331], Step [100/332], Loss: 0.000732612912543118\n","Epoch [169/331], Step [200/332], Loss: 0.0009911776287481189\n","Epoch [169/331], Step [300/332], Loss: 0.0008471459150314331\n","Epoch [170/331], Step [100/332], Loss: 0.00790795125067234\n","Epoch [170/331], Step [200/332], Loss: 0.029708020389080048\n","Epoch [170/331], Step [300/332], Loss: 0.0010467038955539465\n","Epoch [171/331], Step [100/332], Loss: 0.001358501031063497\n","Epoch [171/331], Step [200/332], Loss: 0.025547033175826073\n","Epoch [171/331], Step [300/332], Loss: 0.037948790937662125\n","Epoch [172/331], Step [100/332], Loss: 0.031198501586914062\n","Epoch [172/331], Step [200/332], Loss: 0.07261321693658829\n","Epoch [172/331], Step [300/332], Loss: 0.05021804943680763\n","Epoch [173/331], Step [100/332], Loss: 0.0086134672164917\n","Epoch [173/331], Step [200/332], Loss: 0.00990693736821413\n","Epoch [173/331], Step [300/332], Loss: 0.011329111643135548\n","Epoch [174/331], Step [100/332], Loss: 0.07156864553689957\n","Epoch [174/331], Step [200/332], Loss: 0.08403880149126053\n","Epoch [174/331], Step [300/332], Loss: 0.01631110906600952\n","Epoch [175/331], Step [100/332], Loss: 0.02645319327712059\n","Epoch [175/331], Step [200/332], Loss: 0.02188154123723507\n","Epoch [175/331], Step [300/332], Loss: 0.0017436238704249263\n","Epoch [176/331], Step [100/332], Loss: 0.003636846551671624\n","Epoch [176/331], Step [200/332], Loss: 0.007896593771874905\n","Epoch [176/331], Step [300/332], Loss: 0.009022883139550686\n","Epoch [177/331], Step [100/332], Loss: 0.01782950758934021\n","Epoch [177/331], Step [200/332], Loss: 0.030614538118243217\n","Epoch [177/331], Step [300/332], Loss: 0.06750478595495224\n","Epoch [178/331], Step [100/332], Loss: 0.002065393142402172\n","Epoch [178/331], Step [200/332], Loss: 0.001744190463796258\n","Epoch [178/331], Step [300/332], Loss: 0.014136163517832756\n","Epoch [179/331], Step [100/332], Loss: 0.0034779561683535576\n","Epoch [179/331], Step [200/332], Loss: 0.0016018088208511472\n","Epoch [179/331], Step [300/332], Loss: 0.003470883471891284\n","Epoch [180/331], Step [100/332], Loss: 0.0023068911395967007\n","Epoch [180/331], Step [200/332], Loss: 0.0024926902260631323\n","Epoch [180/331], Step [300/332], Loss: 0.004739546682685614\n","Epoch [181/331], Step [100/332], Loss: 0.0013526607071980834\n","Epoch [181/331], Step [200/332], Loss: 0.020143473520874977\n","Epoch [181/331], Step [300/332], Loss: 0.0013448357349261642\n","Epoch [182/331], Step [100/332], Loss: 0.025211989879608154\n","Epoch [182/331], Step [200/332], Loss: 0.03553026542067528\n","Epoch [182/331], Step [300/332], Loss: 0.010516251437366009\n","Epoch [183/331], Step [100/332], Loss: 0.03610655292868614\n","Epoch [183/331], Step [200/332], Loss: 0.041165996342897415\n","Epoch [183/331], Step [300/332], Loss: 0.014696633443236351\n","Epoch [184/331], Step [100/332], Loss: 0.017896320670843124\n","Epoch [184/331], Step [200/332], Loss: 0.013632467947900295\n","Epoch [184/331], Step [300/332], Loss: 0.0051470291800796986\n","Epoch [185/331], Step [100/332], Loss: 0.0009736398933455348\n","Epoch [185/331], Step [200/332], Loss: 0.0006416555261239409\n","Epoch [185/331], Step [300/332], Loss: 0.013491951860487461\n","Epoch [186/331], Step [100/332], Loss: 0.04524345323443413\n","Epoch [186/331], Step [200/332], Loss: 0.00590609572827816\n","Epoch [186/331], Step [300/332], Loss: 0.010258548893034458\n","Epoch [187/331], Step [100/332], Loss: 0.009830939583480358\n","Epoch [187/331], Step [200/332], Loss: 0.0016972601879388094\n","Epoch [187/331], Step [300/332], Loss: 0.005689671728760004\n","Epoch [188/331], Step [100/332], Loss: 0.04792006313800812\n","Epoch [188/331], Step [200/332], Loss: 0.01578734628856182\n","Epoch [188/331], Step [300/332], Loss: 0.03214398771524429\n","Epoch [189/331], Step [100/332], Loss: 0.0044426871463656425\n","Epoch [189/331], Step [200/332], Loss: 0.005823974031955004\n","Epoch [189/331], Step [300/332], Loss: 0.0031021691393107176\n","Epoch [190/331], Step [100/332], Loss: 0.019695129245519638\n","Epoch [190/331], Step [200/332], Loss: 0.023103538900613785\n","Epoch [190/331], Step [300/332], Loss: 0.026454983279109\n","Epoch [191/331], Step [100/332], Loss: 0.014080781489610672\n","Epoch [191/331], Step [200/332], Loss: 0.01267439778894186\n","Epoch [191/331], Step [300/332], Loss: 0.055884722620248795\n","Epoch [192/331], Step [100/332], Loss: 0.020350167527794838\n","Epoch [192/331], Step [200/332], Loss: 0.008797337301075459\n","Epoch [192/331], Step [300/332], Loss: 0.006849875673651695\n","Epoch [193/331], Step [100/332], Loss: 0.003930193372070789\n","Epoch [193/331], Step [200/332], Loss: 0.0011366574326530099\n","Epoch [193/331], Step [300/332], Loss: 0.0053022075444459915\n","Epoch [194/331], Step [100/332], Loss: 0.007523367181420326\n","Epoch [194/331], Step [200/332], Loss: 0.00045328662963584065\n","Epoch [194/331], Step [300/332], Loss: 0.0007744153263047338\n","Epoch [195/331], Step [100/332], Loss: 0.03991315886378288\n","Epoch [195/331], Step [200/332], Loss: 0.00549468444660306\n","Epoch [195/331], Step [300/332], Loss: 0.008688353933393955\n","Epoch [196/331], Step [100/332], Loss: 0.016023293137550354\n","Epoch [196/331], Step [200/332], Loss: 0.012134792283177376\n","Epoch [196/331], Step [300/332], Loss: 0.009510066360235214\n","Epoch [197/331], Step [100/332], Loss: 0.011784229427576065\n","Epoch [197/331], Step [200/332], Loss: 0.011671798303723335\n","Epoch [197/331], Step [300/332], Loss: 0.003345355624333024\n","Epoch [198/331], Step [100/332], Loss: 0.0026075986679643393\n","Epoch [198/331], Step [200/332], Loss: 0.006587857846170664\n","Epoch [198/331], Step [300/332], Loss: 0.015152337960898876\n","Epoch [199/331], Step [100/332], Loss: 0.00256382767111063\n","Epoch [199/331], Step [200/332], Loss: 0.0019503821386024356\n","Epoch [199/331], Step [300/332], Loss: 0.03209393098950386\n","Epoch [200/331], Step [100/332], Loss: 0.002333638956770301\n","Epoch [200/331], Step [200/332], Loss: 0.009151393547654152\n","Epoch [200/331], Step [300/332], Loss: 0.016378110274672508\n","Epoch [201/331], Step [100/332], Loss: 0.026352394372224808\n","Epoch [201/331], Step [200/332], Loss: 0.02125588059425354\n","Epoch [201/331], Step [300/332], Loss: 0.0012329176533967257\n","Epoch [202/331], Step [100/332], Loss: 0.0015589776448905468\n","Epoch [202/331], Step [200/332], Loss: 0.015139762312173843\n","Epoch [202/331], Step [300/332], Loss: 0.022019118070602417\n","Epoch [203/331], Step [100/332], Loss: 0.03214997425675392\n","Epoch [203/331], Step [200/332], Loss: 0.017553869634866714\n","Epoch [203/331], Step [300/332], Loss: 0.06813948601484299\n","Epoch [204/331], Step [100/332], Loss: 0.04515352472662926\n","Epoch [204/331], Step [200/332], Loss: 0.03641714155673981\n","Epoch [204/331], Step [300/332], Loss: 0.019255971536040306\n","Epoch [205/331], Step [100/332], Loss: 0.04866227135062218\n","Epoch [205/331], Step [200/332], Loss: 0.025761546567082405\n","Epoch [205/331], Step [300/332], Loss: 0.056263670325279236\n","Epoch [206/331], Step [100/332], Loss: 0.0038910137955099344\n","Epoch [206/331], Step [200/332], Loss: 0.006808501202613115\n","Epoch [206/331], Step [300/332], Loss: 0.08089817315340042\n","Epoch [207/331], Step [100/332], Loss: 0.025870848447084427\n","Epoch [207/331], Step [200/332], Loss: 0.004086983855813742\n","Epoch [207/331], Step [300/332], Loss: 0.010518032126128674\n","Epoch [208/331], Step [100/332], Loss: 0.01132116001099348\n","Epoch [208/331], Step [200/332], Loss: 0.010395302437245846\n","Epoch [208/331], Step [300/332], Loss: 0.0021948032081127167\n","Epoch [209/331], Step [100/332], Loss: 0.051901184022426605\n","Epoch [209/331], Step [200/332], Loss: 0.0049095358699560165\n","Epoch [209/331], Step [300/332], Loss: 0.04458090290427208\n","Epoch [210/331], Step [100/332], Loss: 0.0013044726802036166\n","Epoch [210/331], Step [200/332], Loss: 0.0062737418338656425\n","Epoch [210/331], Step [300/332], Loss: 0.0018423827132210135\n","Epoch [211/331], Step [100/332], Loss: 0.0005067868041805923\n","Epoch [211/331], Step [200/332], Loss: 0.0015304931439459324\n","Epoch [211/331], Step [300/332], Loss: 0.002905613975599408\n","Epoch [212/331], Step [100/332], Loss: 0.0006029362557455897\n","Epoch [212/331], Step [200/332], Loss: 0.02079591527581215\n","Epoch [212/331], Step [300/332], Loss: 0.026109253987669945\n","Epoch [213/331], Step [100/332], Loss: 0.0032175248488783836\n","Epoch [213/331], Step [200/332], Loss: 0.0053572584874928\n","Epoch [213/331], Step [300/332], Loss: 0.002075665397569537\n","Epoch [214/331], Step [100/332], Loss: 0.0006300632958300412\n","Epoch [214/331], Step [200/332], Loss: 0.01194066647440195\n","Epoch [214/331], Step [300/332], Loss: 0.004748508334159851\n","Epoch [215/331], Step [100/332], Loss: 0.012493432499468327\n","Epoch [215/331], Step [200/332], Loss: 0.015367353335022926\n","Epoch [215/331], Step [300/332], Loss: 0.006722510792315006\n","Epoch [216/331], Step [100/332], Loss: 0.022515064105391502\n","Epoch [216/331], Step [200/332], Loss: 0.00769278546795249\n","Epoch [216/331], Step [300/332], Loss: 0.00413801334798336\n","Epoch [217/331], Step [100/332], Loss: 0.016936354339122772\n","Epoch [217/331], Step [200/332], Loss: 0.03165543079376221\n","Epoch [217/331], Step [300/332], Loss: 0.038206569850444794\n","Epoch [218/331], Step [100/332], Loss: 0.043090127408504486\n","Epoch [218/331], Step [200/332], Loss: 0.023053109645843506\n","Epoch [218/331], Step [300/332], Loss: 0.036709271371364594\n","Epoch [219/331], Step [100/332], Loss: 0.016074370592832565\n","Epoch [219/331], Step [200/332], Loss: 0.013665667735040188\n","Epoch [219/331], Step [300/332], Loss: 0.008517596870660782\n","Epoch [220/331], Step [100/332], Loss: 0.005678586196154356\n","Epoch [220/331], Step [200/332], Loss: 0.0023580745328217745\n","Epoch [220/331], Step [300/332], Loss: 0.0005824098479934037\n","Epoch [221/331], Step [100/332], Loss: 0.017938971519470215\n","Epoch [221/331], Step [200/332], Loss: 0.04825970157980919\n","Epoch [221/331], Step [300/332], Loss: 0.005788865033537149\n","Epoch [222/331], Step [100/332], Loss: 0.014274975284934044\n","Epoch [222/331], Step [200/332], Loss: 0.0008882766705937684\n","Epoch [222/331], Step [300/332], Loss: 0.007009550929069519\n","Epoch [223/331], Step [100/332], Loss: 0.04300987720489502\n","Epoch [223/331], Step [200/332], Loss: 0.01605464704334736\n","Epoch [223/331], Step [300/332], Loss: 0.0523083470761776\n","Epoch [224/331], Step [100/332], Loss: 0.007488455157727003\n","Epoch [224/331], Step [200/332], Loss: 0.026436274871230125\n","Epoch [224/331], Step [300/332], Loss: 0.013711769133806229\n","Epoch [225/331], Step [100/332], Loss: 0.0037286956794559956\n","Epoch [225/331], Step [200/332], Loss: 0.01563601940870285\n","Epoch [225/331], Step [300/332], Loss: 0.01991868205368519\n","Epoch [226/331], Step [100/332], Loss: 0.009276596829295158\n","Epoch [226/331], Step [200/332], Loss: 0.007036054041236639\n","Epoch [226/331], Step [300/332], Loss: 0.04778324440121651\n","Epoch [227/331], Step [100/332], Loss: 0.0029710715170949697\n","Epoch [227/331], Step [200/332], Loss: 0.0011614427203312516\n","Epoch [227/331], Step [300/332], Loss: 0.03699103370308876\n","Epoch [228/331], Step [100/332], Loss: 0.01152031496167183\n","Epoch [228/331], Step [200/332], Loss: 0.0196327306330204\n","Epoch [228/331], Step [300/332], Loss: 0.021249860525131226\n","Epoch [229/331], Step [100/332], Loss: 0.01667340286076069\n","Epoch [229/331], Step [200/332], Loss: 0.004308709409087896\n","Epoch [229/331], Step [300/332], Loss: 0.01919618621468544\n","Epoch [230/331], Step [100/332], Loss: 0.03664956986904144\n","Epoch [230/331], Step [200/332], Loss: 0.018193550407886505\n","Epoch [230/331], Step [300/332], Loss: 0.005702067632228136\n","Epoch [231/331], Step [100/332], Loss: 0.0030814402271062136\n","Epoch [231/331], Step [200/332], Loss: 0.018871258944272995\n","Epoch [231/331], Step [300/332], Loss: 0.02144545502960682\n","Epoch [232/331], Step [100/332], Loss: 0.0050108726136386395\n","Epoch [232/331], Step [200/332], Loss: 0.030620025470852852\n","Epoch [232/331], Step [300/332], Loss: 0.0021256585605442524\n","Epoch [233/331], Step [100/332], Loss: 0.02749454416334629\n","Epoch [233/331], Step [200/332], Loss: 0.016940295696258545\n","Epoch [233/331], Step [300/332], Loss: 0.005214901175349951\n","Epoch [234/331], Step [100/332], Loss: 0.04041793569922447\n","Epoch [234/331], Step [200/332], Loss: 0.025536131113767624\n","Epoch [234/331], Step [300/332], Loss: 0.006578731816262007\n","Epoch [235/331], Step [100/332], Loss: 0.010097295045852661\n","Epoch [235/331], Step [200/332], Loss: 0.01455694530159235\n","Epoch [235/331], Step [300/332], Loss: 0.030010564252734184\n","Epoch [236/331], Step [100/332], Loss: 0.0058719743974506855\n","Epoch [236/331], Step [200/332], Loss: 0.0061819483526051044\n","Epoch [236/331], Step [300/332], Loss: 0.047293394804000854\n","Epoch [237/331], Step [100/332], Loss: 0.01724056899547577\n","Epoch [237/331], Step [200/332], Loss: 0.08551450073719025\n","Epoch [237/331], Step [300/332], Loss: 0.016311168670654297\n","Epoch [238/331], Step [100/332], Loss: 0.007932485081255436\n","Epoch [238/331], Step [200/332], Loss: 0.016274522989988327\n","Epoch [238/331], Step [300/332], Loss: 0.06596224009990692\n","Epoch [239/331], Step [100/332], Loss: 0.009548291563987732\n","Epoch [239/331], Step [200/332], Loss: 0.004758951719850302\n","Epoch [239/331], Step [300/332], Loss: 0.003061244497075677\n","Epoch [240/331], Step [100/332], Loss: 0.014970822259783745\n","Epoch [240/331], Step [200/332], Loss: 0.004760666284710169\n","Epoch [240/331], Step [300/332], Loss: 0.006422804202884436\n","Epoch [241/331], Step [100/332], Loss: 0.011323857121169567\n","Epoch [241/331], Step [200/332], Loss: 0.0007809335365891457\n","Epoch [241/331], Step [300/332], Loss: 0.007753587793558836\n","Epoch [242/331], Step [100/332], Loss: 0.004162792582064867\n","Epoch [242/331], Step [200/332], Loss: 0.0085580600425601\n","Epoch [242/331], Step [300/332], Loss: 0.014617602340877056\n","Epoch [243/331], Step [100/332], Loss: 0.004771282896399498\n","Epoch [243/331], Step [200/332], Loss: 0.0160666573792696\n","Epoch [243/331], Step [300/332], Loss: 0.011620080098509789\n","Epoch [244/331], Step [100/332], Loss: 0.006332226097583771\n","Epoch [244/331], Step [200/332], Loss: 0.027552343904972076\n","Epoch [244/331], Step [300/332], Loss: 0.007293758448213339\n","Epoch [245/331], Step [100/332], Loss: 0.025512512773275375\n","Epoch [245/331], Step [200/332], Loss: 0.04477684944868088\n","Epoch [245/331], Step [300/332], Loss: 0.00199701520614326\n","Epoch [246/331], Step [100/332], Loss: 0.007472148165106773\n","Epoch [246/331], Step [200/332], Loss: 0.00041524486732669175\n","Epoch [246/331], Step [300/332], Loss: 0.01913389563560486\n","Epoch [247/331], Step [100/332], Loss: 0.02734134905040264\n","Epoch [247/331], Step [200/332], Loss: 0.008091283030807972\n","Epoch [247/331], Step [300/332], Loss: 0.04777756333351135\n","Epoch [248/331], Step [100/332], Loss: 0.00809355080127716\n","Epoch [248/331], Step [200/332], Loss: 0.01172950491309166\n","Epoch [248/331], Step [300/332], Loss: 0.03836268559098244\n","Epoch [249/331], Step [100/332], Loss: 0.01435946300625801\n","Epoch [249/331], Step [200/332], Loss: 0.022908497601747513\n","Epoch [249/331], Step [300/332], Loss: 0.009649815037846565\n","Epoch [250/331], Step [100/332], Loss: 0.015942968428134918\n","Epoch [250/331], Step [200/332], Loss: 0.041723255068063736\n","Epoch [250/331], Step [300/332], Loss: 0.001245900522917509\n","Epoch [251/331], Step [100/332], Loss: 0.02852674201130867\n","Epoch [251/331], Step [200/332], Loss: 0.022674880921840668\n","Epoch [251/331], Step [300/332], Loss: 0.0019456601003184915\n","Epoch [252/331], Step [100/332], Loss: 0.014069419354200363\n","Epoch [252/331], Step [200/332], Loss: 0.00375500600785017\n","Epoch [252/331], Step [300/332], Loss: 0.00635455222800374\n","Epoch [253/331], Step [100/332], Loss: 0.027862221002578735\n","Epoch [253/331], Step [200/332], Loss: 0.0014430698938667774\n","Epoch [253/331], Step [300/332], Loss: 0.0021755602210760117\n","Epoch [254/331], Step [100/332], Loss: 0.004346564877778292\n","Epoch [254/331], Step [200/332], Loss: 0.005170544609427452\n","Epoch [254/331], Step [300/332], Loss: 0.006270568817853928\n","Epoch [255/331], Step [100/332], Loss: 0.007474897895008326\n","Epoch [255/331], Step [200/332], Loss: 0.04426833614706993\n","Epoch [255/331], Step [300/332], Loss: 0.00678241578862071\n","Epoch [256/331], Step [100/332], Loss: 0.021183917298913002\n","Epoch [256/331], Step [200/332], Loss: 0.008236851543188095\n","Epoch [256/331], Step [300/332], Loss: 0.0579642690718174\n","Epoch [257/331], Step [100/332], Loss: 0.00815663207322359\n","Epoch [257/331], Step [200/332], Loss: 0.00885744672268629\n","Epoch [257/331], Step [300/332], Loss: 0.014740786515176296\n","Epoch [258/331], Step [100/332], Loss: 0.022902455180883408\n","Epoch [258/331], Step [200/332], Loss: 0.05147271603345871\n","Epoch [258/331], Step [300/332], Loss: 0.002743696328252554\n","Epoch [259/331], Step [100/332], Loss: 0.0010033819125965238\n","Epoch [259/331], Step [200/332], Loss: 0.004443342797458172\n","Epoch [259/331], Step [300/332], Loss: 0.002550121396780014\n","Epoch [260/331], Step [100/332], Loss: 0.015263017266988754\n","Epoch [260/331], Step [200/332], Loss: 0.013686101883649826\n","Epoch [260/331], Step [300/332], Loss: 0.0010875563602894545\n","Epoch [261/331], Step [100/332], Loss: 0.010386998765170574\n","Epoch [261/331], Step [200/332], Loss: 0.02323128469288349\n","Epoch [261/331], Step [300/332], Loss: 0.020828459411859512\n","Epoch [262/331], Step [100/332], Loss: 0.03197041153907776\n","Epoch [262/331], Step [200/332], Loss: 0.0025031014811247587\n","Epoch [262/331], Step [300/332], Loss: 0.027473043650388718\n","Epoch [263/331], Step [100/332], Loss: 0.016508786007761955\n","Epoch [263/331], Step [200/332], Loss: 0.009508084505796432\n","Epoch [263/331], Step [300/332], Loss: 0.01444418914616108\n","Epoch [264/331], Step [100/332], Loss: 0.014465748332440853\n","Epoch [264/331], Step [200/332], Loss: 0.004605088848620653\n","Epoch [264/331], Step [300/332], Loss: 0.008374590426683426\n","Epoch [265/331], Step [100/332], Loss: 0.005443225614726543\n","Epoch [265/331], Step [200/332], Loss: 0.0010318235727027059\n","Epoch [265/331], Step [300/332], Loss: 0.013075812719762325\n","Epoch [266/331], Step [100/332], Loss: 0.002105270279571414\n","Epoch [266/331], Step [200/332], Loss: 0.0015532077522948384\n","Epoch [266/331], Step [300/332], Loss: 0.010495808906853199\n","Epoch [267/331], Step [100/332], Loss: 0.002777990186586976\n","Epoch [267/331], Step [200/332], Loss: 0.017817702144384384\n","Epoch [267/331], Step [300/332], Loss: 0.000650330854114145\n","Epoch [268/331], Step [100/332], Loss: 0.0054445695132017136\n","Epoch [268/331], Step [200/332], Loss: 0.005309267435222864\n","Epoch [268/331], Step [300/332], Loss: 0.0033482983708381653\n","Epoch [269/331], Step [100/332], Loss: 0.011763077229261398\n","Epoch [269/331], Step [200/332], Loss: 0.039427176117897034\n","Epoch [269/331], Step [300/332], Loss: 0.05181002989411354\n","Epoch [270/331], Step [100/332], Loss: 0.001739045139402151\n","Epoch [270/331], Step [200/332], Loss: 0.010199377313256264\n","Epoch [270/331], Step [300/332], Loss: 0.05370641127228737\n","Epoch [271/331], Step [100/332], Loss: 0.034199655055999756\n","Epoch [271/331], Step [200/332], Loss: 0.014111766591668129\n","Epoch [271/331], Step [300/332], Loss: 0.010500929318368435\n","Epoch [272/331], Step [100/332], Loss: 0.009632701054215431\n","Epoch [272/331], Step [200/332], Loss: 0.003396163461729884\n","Epoch [272/331], Step [300/332], Loss: 0.0037467486690729856\n","Epoch [273/331], Step [100/332], Loss: 0.003232714021578431\n","Epoch [273/331], Step [200/332], Loss: 0.002618823666125536\n","Epoch [273/331], Step [300/332], Loss: 0.00352981174364686\n","Epoch [274/331], Step [100/332], Loss: 0.009401385672390461\n","Epoch [274/331], Step [200/332], Loss: 0.054709650576114655\n","Epoch [274/331], Step [300/332], Loss: 0.010123802348971367\n","Epoch [275/331], Step [100/332], Loss: 0.004085869994014502\n","Epoch [275/331], Step [200/332], Loss: 0.005603575613349676\n","Epoch [275/331], Step [300/332], Loss: 0.0024154288694262505\n","Epoch [276/331], Step [100/332], Loss: 0.005872619338333607\n","Epoch [276/331], Step [200/332], Loss: 0.1044841855764389\n","Epoch [276/331], Step [300/332], Loss: 0.01931421458721161\n","Epoch [277/331], Step [100/332], Loss: 0.04255491867661476\n","Epoch [277/331], Step [200/332], Loss: 0.004483168479055166\n","Epoch [277/331], Step [300/332], Loss: 0.037274155765771866\n","Epoch [278/331], Step [100/332], Loss: 0.011386399157345295\n","Epoch [278/331], Step [200/332], Loss: 0.015533186495304108\n","Epoch [278/331], Step [300/332], Loss: 0.009156002663075924\n","Epoch [279/331], Step [100/332], Loss: 0.005302371457219124\n","Epoch [279/331], Step [200/332], Loss: 0.009087775833904743\n","Epoch [279/331], Step [300/332], Loss: 0.0021511956583708525\n","Epoch [280/331], Step [100/332], Loss: 0.011255433782935143\n","Epoch [280/331], Step [200/332], Loss: 0.03870975971221924\n","Epoch [280/331], Step [300/332], Loss: 0.01082687359303236\n","Epoch [281/331], Step [100/332], Loss: 0.03787960112094879\n","Epoch [281/331], Step [200/332], Loss: 0.023616045713424683\n","Epoch [281/331], Step [300/332], Loss: 0.0005039076786488295\n","Epoch [282/331], Step [100/332], Loss: 0.020188698545098305\n","Epoch [282/331], Step [200/332], Loss: 0.009465789422392845\n","Epoch [282/331], Step [300/332], Loss: 0.006369902286678553\n","Epoch [283/331], Step [100/332], Loss: 0.0050069959834218025\n","Epoch [283/331], Step [200/332], Loss: 0.0014608792262151837\n","Epoch [283/331], Step [300/332], Loss: 0.012278965674340725\n","Epoch [284/331], Step [100/332], Loss: 0.07699093967676163\n","Epoch [284/331], Step [200/332], Loss: 0.011400975286960602\n","Epoch [284/331], Step [300/332], Loss: 0.002313970820978284\n","Epoch [285/331], Step [100/332], Loss: 0.0028297670651227236\n","Epoch [285/331], Step [200/332], Loss: 0.0143833477050066\n","Epoch [285/331], Step [300/332], Loss: 0.0032160100527107716\n","Epoch [286/331], Step [100/332], Loss: 0.0027908484917134047\n","Epoch [286/331], Step [200/332], Loss: 0.011678074486553669\n","Epoch [286/331], Step [300/332], Loss: 0.003918850794434547\n","Epoch [287/331], Step [100/332], Loss: 0.011240960098803043\n","Epoch [287/331], Step [200/332], Loss: 0.028045514598488808\n","Epoch [287/331], Step [300/332], Loss: 0.09821876883506775\n","Epoch [288/331], Step [100/332], Loss: 0.011119626462459564\n","Epoch [288/331], Step [200/332], Loss: 0.0033195174764841795\n","Epoch [288/331], Step [300/332], Loss: 0.0018454967066645622\n","Epoch [289/331], Step [100/332], Loss: 0.0005193686229176819\n","Epoch [289/331], Step [200/332], Loss: 0.008090119808912277\n","Epoch [289/331], Step [300/332], Loss: 0.023636750876903534\n","Epoch [290/331], Step [100/332], Loss: 0.00874734204262495\n","Epoch [290/331], Step [200/332], Loss: 0.0023448842111974955\n","Epoch [290/331], Step [300/332], Loss: 0.005070080980658531\n","Epoch [291/331], Step [100/332], Loss: 0.0041579375974833965\n","Epoch [291/331], Step [200/332], Loss: 0.0029215251561254263\n","Epoch [291/331], Step [300/332], Loss: 0.026354897767305374\n","Epoch [292/331], Step [100/332], Loss: 0.018556861206889153\n","Epoch [292/331], Step [200/332], Loss: 0.009034480899572372\n","Epoch [292/331], Step [300/332], Loss: 0.005150303244590759\n","Epoch [293/331], Step [100/332], Loss: 0.015779009088873863\n","Epoch [293/331], Step [200/332], Loss: 0.01898551918566227\n","Epoch [293/331], Step [300/332], Loss: 0.004620061721652746\n","Epoch [294/331], Step [100/332], Loss: 0.01359159778803587\n","Epoch [294/331], Step [200/332], Loss: 0.0015617234166711569\n","Epoch [294/331], Step [300/332], Loss: 0.007342942524701357\n","Epoch [295/331], Step [100/332], Loss: 0.0013311542570590973\n","Epoch [295/331], Step [200/332], Loss: 0.0003588878025766462\n","Epoch [295/331], Step [300/332], Loss: 0.0007645256700925529\n","Epoch [296/331], Step [100/332], Loss: 0.000777753593865782\n","Epoch [296/331], Step [200/332], Loss: 0.0011578158009797335\n","Epoch [296/331], Step [300/332], Loss: 0.0009537754231132567\n","Epoch [297/331], Step [100/332], Loss: 0.03360168635845184\n","Epoch [297/331], Step [200/332], Loss: 0.0007515683537349105\n","Epoch [297/331], Step [300/332], Loss: 0.030535640195012093\n","Epoch [298/331], Step [100/332], Loss: 0.008633112534880638\n","Epoch [298/331], Step [200/332], Loss: 0.009615914896130562\n","Epoch [298/331], Step [300/332], Loss: 0.05050646513700485\n","Epoch [299/331], Step [100/332], Loss: 0.09869775176048279\n","Epoch [299/331], Step [200/332], Loss: 0.020254172384738922\n","Epoch [299/331], Step [300/332], Loss: 0.01588086597621441\n","Epoch [300/331], Step [100/332], Loss: 0.02090250886976719\n","Epoch [300/331], Step [200/332], Loss: 0.016517838463187218\n","Epoch [300/331], Step [300/332], Loss: 0.005395941901952028\n","Epoch [301/331], Step [100/332], Loss: 0.0023568999022245407\n","Epoch [301/331], Step [200/332], Loss: 0.0015797240193933249\n","Epoch [301/331], Step [300/332], Loss: 0.009618367068469524\n","Epoch [302/331], Step [100/332], Loss: 0.002541907597333193\n","Epoch [302/331], Step [200/332], Loss: 0.004556369502097368\n","Epoch [302/331], Step [300/332], Loss: 0.0020624317694455385\n","Epoch [303/331], Step [100/332], Loss: 0.007203930057585239\n","Epoch [303/331], Step [200/332], Loss: 0.0007396818255074322\n","Epoch [303/331], Step [300/332], Loss: 0.02220781520009041\n","Epoch [304/331], Step [100/332], Loss: 0.0031891376711428165\n","Epoch [304/331], Step [200/332], Loss: 0.01116865873336792\n","Epoch [304/331], Step [300/332], Loss: 0.008819032460451126\n","Epoch [305/331], Step [100/332], Loss: 0.00898849405348301\n","Epoch [305/331], Step [200/332], Loss: 0.0030812490731477737\n","Epoch [305/331], Step [300/332], Loss: 0.004454343114048243\n","Epoch [306/331], Step [100/332], Loss: 0.002206538338214159\n","Epoch [306/331], Step [200/332], Loss: 0.06468725204467773\n","Epoch [306/331], Step [300/332], Loss: 0.01031548622995615\n","Epoch [307/331], Step [100/332], Loss: 0.007076373789459467\n","Epoch [307/331], Step [200/332], Loss: 0.01166228111833334\n","Epoch [307/331], Step [300/332], Loss: 0.01351090706884861\n","Epoch [308/331], Step [100/332], Loss: 0.0020192721858620644\n","Epoch [308/331], Step [200/332], Loss: 0.018645543605089188\n","Epoch [308/331], Step [300/332], Loss: 0.011014830321073532\n","Epoch [309/331], Step [100/332], Loss: 0.0128554105758667\n","Epoch [309/331], Step [200/332], Loss: 0.0026324146892875433\n","Epoch [309/331], Step [300/332], Loss: 0.013713687658309937\n","Epoch [310/331], Step [100/332], Loss: 0.021286655217409134\n","Epoch [310/331], Step [200/332], Loss: 0.003654081840068102\n","Epoch [310/331], Step [300/332], Loss: 0.0005604520556516945\n","Epoch [311/331], Step [100/332], Loss: 0.008115298114717007\n","Epoch [311/331], Step [200/332], Loss: 0.0027336529456079006\n","Epoch [311/331], Step [300/332], Loss: 0.0014810721622779965\n","Epoch [312/331], Step [100/332], Loss: 0.008693092502653599\n","Epoch [312/331], Step [200/332], Loss: 0.002104412531480193\n","Epoch [312/331], Step [300/332], Loss: 0.001067602657712996\n","Epoch [313/331], Step [100/332], Loss: 0.005392446182668209\n","Epoch [313/331], Step [200/332], Loss: 0.002848976757377386\n","Epoch [313/331], Step [300/332], Loss: 0.002063706750050187\n","Epoch [314/331], Step [100/332], Loss: 0.008945456705987453\n","Epoch [314/331], Step [200/332], Loss: 0.0006407731561921537\n","Epoch [314/331], Step [300/332], Loss: 0.00029285208438523114\n","Epoch [315/331], Step [100/332], Loss: 0.045920953154563904\n","Epoch [315/331], Step [200/332], Loss: 0.01952389068901539\n","Epoch [315/331], Step [300/332], Loss: 0.03090999275445938\n","Epoch [316/331], Step [100/332], Loss: 0.07536018639802933\n","Epoch [316/331], Step [200/332], Loss: 0.007023687474429607\n","Epoch [316/331], Step [300/332], Loss: 0.013010368682444096\n","Epoch [317/331], Step [100/332], Loss: 0.0021706989500671625\n","Epoch [317/331], Step [200/332], Loss: 0.0010956745827570558\n","Epoch [317/331], Step [300/332], Loss: 0.004322804044932127\n","Epoch [318/331], Step [100/332], Loss: 0.0027473263908177614\n","Epoch [318/331], Step [200/332], Loss: 0.03153163939714432\n","Epoch [318/331], Step [300/332], Loss: 0.0010003993520513177\n","Epoch [319/331], Step [100/332], Loss: 0.0004892028518952429\n","Epoch [319/331], Step [200/332], Loss: 0.014408759772777557\n","Epoch [319/331], Step [300/332], Loss: 0.0021225411910563707\n","Epoch [320/331], Step [100/332], Loss: 0.00036940077552571893\n","Epoch [320/331], Step [200/332], Loss: 0.0022536010947078466\n","Epoch [320/331], Step [300/332], Loss: 0.0011325652012601495\n","Epoch [321/331], Step [100/332], Loss: 0.0014514863723888993\n","Epoch [321/331], Step [200/332], Loss: 0.0019334441749379039\n","Epoch [321/331], Step [300/332], Loss: 0.02395007200539112\n","Epoch [322/331], Step [100/332], Loss: 0.001010076142847538\n","Epoch [322/331], Step [200/332], Loss: 0.011698106303811073\n","Epoch [322/331], Step [300/332], Loss: 0.013081112876534462\n","Epoch [323/331], Step [100/332], Loss: 0.002512008650228381\n","Epoch [323/331], Step [200/332], Loss: 0.017183639109134674\n","Epoch [323/331], Step [300/332], Loss: 0.020152609795331955\n","Epoch [324/331], Step [100/332], Loss: 0.006987851578742266\n","Epoch [324/331], Step [200/332], Loss: 0.00987132079899311\n","Epoch [324/331], Step [300/332], Loss: 0.016845963895320892\n","Epoch [325/331], Step [100/332], Loss: 0.002172912238165736\n","Epoch [325/331], Step [200/332], Loss: 0.00478121405467391\n","Epoch [325/331], Step [300/332], Loss: 0.007616082206368446\n","Epoch [326/331], Step [100/332], Loss: 0.009783404879271984\n","Epoch [326/331], Step [200/332], Loss: 0.004399328958243132\n","Epoch [326/331], Step [300/332], Loss: 0.0369882807135582\n","Epoch [327/331], Step [100/332], Loss: 0.0038834591396152973\n","Epoch [327/331], Step [200/332], Loss: 0.0006406569737009704\n","Epoch [327/331], Step [300/332], Loss: 0.007989883422851562\n","Epoch [328/331], Step [100/332], Loss: 0.004791566636413336\n","Epoch [328/331], Step [200/332], Loss: 0.038338225334882736\n","Epoch [328/331], Step [300/332], Loss: 0.001856792951002717\n","Epoch [329/331], Step [100/332], Loss: 0.008176082745194435\n","Epoch [329/331], Step [200/332], Loss: 0.00642826221883297\n","Epoch [329/331], Step [300/332], Loss: 0.007848065346479416\n","Epoch [330/331], Step [100/332], Loss: 0.009520209394395351\n","Epoch [330/331], Step [200/332], Loss: 0.02823427878320217\n","Epoch [330/331], Step [300/332], Loss: 0.007799426559358835\n","Epoch [331/331], Step [100/332], Loss: 0.005200213752686977\n","Epoch [331/331], Step [200/332], Loss: 0.00990463700145483\n","Epoch [331/331], Step [300/332], Loss: 0.041635505855083466\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YYD0uottk6rr","executionInfo":{"elapsed":3130,"status":"ok","timestamp":1623127322845,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"b674ab8d-612e-49cc-db8f-54a146f73c73"},"source":["# Test the model\n","test_dataset = CustomDataset(X_test, y_test)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (images, labels) in enumerate(test_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device) # 이런게 실제 동작 부위보다 시간이 오래걸림\n","        outputs = model(images) # 연산 시간이라 볼수도있는데\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 100.0\n","Iteration:1,  Accuracy: 98.4375\n","Iteration:2,  Accuracy: 98.95833333333333\n","Iteration:3,  Accuracy: 98.4375\n","Iteration:4,  Accuracy: 98.75\n","Iteration:5,  Accuracy: 98.4375\n","Iteration:6,  Accuracy: 98.21428571428571\n","Iteration:7,  Accuracy: 98.4375\n","Iteration:8,  Accuracy: 98.61111111111111\n","Iteration:9,  Accuracy: 98.75\n","Iteration:10,  Accuracy: 98.86363636363636\n","Iteration:11,  Accuracy: 98.69791666666667\n","Iteration:12,  Accuracy: 98.5576923076923\n","Iteration:13,  Accuracy: 98.66071428571429\n","Iteration:14,  Accuracy: 98.54166666666667\n","Iteration:15,  Accuracy: 98.4375\n","Iteration:16,  Accuracy: 98.52941176470588\n","Iteration:17,  Accuracy: 98.61111111111111\n","Iteration:18,  Accuracy: 98.6842105263158\n","Iteration:19,  Accuracy: 98.75\n","Iteration:20,  Accuracy: 98.66071428571429\n","Iteration:21,  Accuracy: 98.7215909090909\n","Iteration:22,  Accuracy: 98.77717391304348\n","Iteration:23,  Accuracy: 98.828125\n","Iteration:24,  Accuracy: 98.875\n","Iteration:25,  Accuracy: 98.91826923076923\n","Iteration:26,  Accuracy: 98.95833333333333\n","Iteration:27,  Accuracy: 98.99553571428571\n","Iteration:28,  Accuracy: 98.92241379310344\n","Iteration:29,  Accuracy: 98.95833333333333\n","Iteration:30,  Accuracy: 98.79032258064517\n","Iteration:31,  Accuracy: 98.828125\n","Iteration:32,  Accuracy: 98.86363636363636\n","Iteration:33,  Accuracy: 98.8970588235294\n","Iteration:34,  Accuracy: 98.92857142857143\n","Iteration:35,  Accuracy: 98.87152777777777\n","Iteration:36,  Accuracy: 98.81756756756756\n","Iteration:37,  Accuracy: 98.76644736842105\n","Iteration:38,  Accuracy: 98.79807692307692\n","Iteration:39,  Accuracy: 98.828125\n","Iteration:40,  Accuracy: 98.85670731707317\n","Iteration:41,  Accuracy: 98.88392857142857\n","Iteration:42,  Accuracy: 98.90988372093024\n","Iteration:43,  Accuracy: 98.86363636363636\n","Iteration:44,  Accuracy: 98.75\n","Iteration:45,  Accuracy: 98.70923913043478\n","Iteration:46,  Accuracy: 98.67021276595744\n","Iteration:47,  Accuracy: 98.69791666666667\n","Iteration:48,  Accuracy: 98.72448979591837\n","Iteration:49,  Accuracy: 98.75\n","Iteration:50,  Accuracy: 98.65196078431373\n","Iteration:51,  Accuracy: 98.67788461538461\n","Iteration:52,  Accuracy: 98.64386792452831\n","Iteration:53,  Accuracy: 98.61111111111111\n","Iteration:54,  Accuracy: 98.63636363636364\n","Iteration:55,  Accuracy: 98.60491071428571\n","Iteration:56,  Accuracy: 98.57456140350877\n","Iteration:57,  Accuracy: 98.54525862068965\n","Iteration:58,  Accuracy: 98.51694915254237\n","Iteration:59,  Accuracy: 98.54166666666667\n","Iteration:60,  Accuracy: 98.56557377049181\n","Iteration:61,  Accuracy: 98.58870967741936\n","Iteration:62,  Accuracy: 98.61111111111111\n","Iteration:63,  Accuracy: 98.6328125\n","Iteration:64,  Accuracy: 98.65384615384616\n","Iteration:65,  Accuracy: 98.67424242424242\n","Iteration:66,  Accuracy: 98.55410447761194\n","Iteration:67,  Accuracy: 98.57536764705883\n","Iteration:68,  Accuracy: 98.59601449275362\n","Iteration:69,  Accuracy: 98.61607142857143\n","Iteration:70,  Accuracy: 98.6355633802817\n","Iteration:71,  Accuracy: 98.61111111111111\n","Iteration:72,  Accuracy: 98.58732876712328\n","Iteration:73,  Accuracy: 98.60641891891892\n","Iteration:74,  Accuracy: 98.625\n","Iteration:75,  Accuracy: 98.60197368421052\n","Iteration:76,  Accuracy: 98.57954545454545\n","Iteration:77,  Accuracy: 98.59775641025641\n","Iteration:78,  Accuracy: 98.61550632911393\n","Iteration:79,  Accuracy: 98.59375\n","Iteration:80,  Accuracy: 98.61111111111111\n","Iteration:81,  Accuracy: 98.58993902439025\n","Iteration:82,  Accuracy: 98.60692771084338\n","Iteration:83,  Accuracy: 98.6235119047619\n","Iteration:84,  Accuracy: 98.63970588235294\n","Iteration:85,  Accuracy: 98.65552325581395\n","Iteration:86,  Accuracy: 98.67097701149426\n","Iteration:87,  Accuracy: 98.68607954545455\n","Iteration:88,  Accuracy: 98.63061797752809\n","Iteration:89,  Accuracy: 98.61111111111111\n","Iteration:90,  Accuracy: 98.59203296703296\n","Iteration:91,  Accuracy: 98.57336956521739\n","Iteration:92,  Accuracy: 98.58870967741936\n","Iteration:93,  Accuracy: 98.60372340425532\n","Iteration:94,  Accuracy: 98.58552631578948\n","Iteration:95,  Accuracy: 98.60026041666667\n","Iteration:96,  Accuracy: 98.55025773195877\n","Iteration:97,  Accuracy: 98.53316326530613\n","Iteration:98,  Accuracy: 98.48484848484848\n","Iteration:99,  Accuracy: 98.46875\n","Iteration:100,  Accuracy: 98.48391089108911\n","Iteration:101,  Accuracy: 98.49877450980392\n","Iteration:102,  Accuracy: 98.5133495145631\n","Iteration:103,  Accuracy: 98.52764423076923\n","Iteration:104,  Accuracy: 98.51190476190476\n","Iteration:105,  Accuracy: 98.52594339622641\n","Iteration:106,  Accuracy: 98.53971962616822\n","Iteration:107,  Accuracy: 98.55324074074075\n","Iteration:108,  Accuracy: 98.56651376146789\n","Iteration:109,  Accuracy: 98.57954545454545\n","Iteration:110,  Accuracy: 98.59234234234235\n","Iteration:111,  Accuracy: 98.60491071428571\n","Iteration:112,  Accuracy: 98.56194690265487\n","Iteration:113,  Accuracy: 98.57456140350877\n","Iteration:114,  Accuracy: 98.55978260869566\n","Iteration:115,  Accuracy: 98.57219827586206\n","Iteration:116,  Accuracy: 98.5576923076923\n","Iteration:117,  Accuracy: 98.56991525423729\n","Iteration:118,  Accuracy: 98.58193277310924\n","Iteration:119,  Accuracy: 98.59375\n","Iteration:120,  Accuracy: 98.57954545454545\n","Iteration:121,  Accuracy: 98.56557377049181\n","Iteration:122,  Accuracy: 98.55182926829268\n","Iteration:123,  Accuracy: 98.56350806451613\n","Iteration:124,  Accuracy: 98.55\n","Iteration:125,  Accuracy: 98.53670634920636\n","Iteration:126,  Accuracy: 98.5482283464567\n","Iteration:127,  Accuracy: 98.53515625\n","Iteration:128,  Accuracy: 98.52228682170542\n","Iteration:129,  Accuracy: 98.53365384615384\n","Iteration:130,  Accuracy: 98.54484732824427\n","Iteration:131,  Accuracy: 98.53219696969697\n","Iteration:132,  Accuracy: 98.54323308270676\n","Iteration:133,  Accuracy: 98.55410447761194\n","Iteration:134,  Accuracy: 98.56481481481481\n","Iteration:135,  Accuracy: 98.55238970588235\n","Iteration:136,  Accuracy: 98.54014598540147\n","Iteration:137,  Accuracy: 98.55072463768116\n","Iteration:138,  Accuracy: 98.56115107913669\n","Iteration:139,  Accuracy: 98.57142857142857\n","Iteration:140,  Accuracy: 98.58156028368795\n","Iteration:141,  Accuracy: 98.59154929577464\n","Iteration:142,  Accuracy: 98.5576923076923\n","Iteration:143,  Accuracy: 98.56770833333333\n","Iteration:144,  Accuracy: 98.57758620689656\n","Iteration:145,  Accuracy: 98.58732876712328\n","Iteration:146,  Accuracy: 98.59693877551021\n","Iteration:147,  Accuracy: 98.60641891891892\n","Iteration:148,  Accuracy: 98.61577181208054\n","Iteration:149,  Accuracy: 98.58333333333333\n","Iteration:150,  Accuracy: 98.59271523178808\n","Iteration:151,  Accuracy: 98.5814144736842\n","Iteration:152,  Accuracy: 98.5906862745098\n","Iteration:153,  Accuracy: 98.57954545454545\n","Iteration:154,  Accuracy: 98.58870967741936\n","Iteration:155,  Accuracy: 98.59775641025641\n","Iteration:156,  Accuracy: 98.60668789808918\n","Iteration:157,  Accuracy: 98.59572784810126\n","Iteration:158,  Accuracy: 98.60455974842768\n","Iteration:159,  Accuracy: 98.61328125\n","Iteration:160,  Accuracy: 98.6218944099379\n","Iteration:161,  Accuracy: 98.61111111111111\n","Iteration:162,  Accuracy: 98.60046012269939\n","Iteration:163,  Accuracy: 98.60899390243902\n","Iteration:164,  Accuracy: 98.61742424242425\n","Iteration:165,  Accuracy: 98.6257530120482\n","Iteration:166,  Accuracy: 98.63398203592814\n","Iteration:167,  Accuracy: 98.6421130952381\n","Iteration:168,  Accuracy: 98.65014792899409\n","Iteration:169,  Accuracy: 98.65808823529412\n","Iteration:170,  Accuracy: 98.64766081871345\n","Iteration:171,  Accuracy: 98.6373546511628\n","Iteration:172,  Accuracy: 98.64523121387283\n","Iteration:173,  Accuracy: 98.63505747126437\n","Iteration:174,  Accuracy: 98.64285714285714\n","Iteration:175,  Accuracy: 98.65056818181819\n","Iteration:176,  Accuracy: 98.64053672316385\n","Iteration:177,  Accuracy: 98.64817415730337\n","Iteration:178,  Accuracy: 98.65572625698324\n","Iteration:179,  Accuracy: 98.66319444444444\n","Iteration:180,  Accuracy: 98.67058011049724\n","Iteration:181,  Accuracy: 98.66071428571429\n","Iteration:182,  Accuracy: 98.66803278688525\n","Iteration:183,  Accuracy: 98.65828804347827\n","Iteration:184,  Accuracy: 98.66554054054055\n","Iteration:185,  Accuracy: 98.67271505376344\n","Iteration:186,  Accuracy: 98.66310160427807\n","Iteration:187,  Accuracy: 98.67021276595744\n","Iteration:188,  Accuracy: 98.66071428571429\n","Iteration:189,  Accuracy: 98.66776315789474\n","Iteration:190,  Accuracy: 98.67473821989529\n","Iteration:191,  Accuracy: 98.681640625\n","Iteration:192,  Accuracy: 98.68847150259067\n","Iteration:193,  Accuracy: 98.67912371134021\n","Iteration:194,  Accuracy: 98.68589743589743\n","Iteration:195,  Accuracy: 98.69260204081633\n","Iteration:196,  Accuracy: 98.69923857868021\n","Iteration:197,  Accuracy: 98.70580808080808\n","Iteration:198,  Accuracy: 98.696608040201\n","Iteration:199,  Accuracy: 98.6875\n","Iteration:200,  Accuracy: 98.67848258706468\n","Iteration:201,  Accuracy: 98.68502475247524\n","Iteration:202,  Accuracy: 98.69150246305419\n","Iteration:203,  Accuracy: 98.69791666666667\n","Iteration:204,  Accuracy: 98.70426829268293\n","Iteration:205,  Accuracy: 98.71055825242719\n","Iteration:206,  Accuracy: 98.70169082125604\n","Iteration:207,  Accuracy: 98.7079326923077\n","Iteration:208,  Accuracy: 98.6842105263158\n","Iteration:209,  Accuracy: 98.67559523809524\n","Iteration:210,  Accuracy: 98.68187203791469\n","Iteration:211,  Accuracy: 98.67334905660377\n","Iteration:212,  Accuracy: 98.65023474178403\n","Iteration:213,  Accuracy: 98.65654205607477\n","Iteration:214,  Accuracy: 98.66279069767442\n","Iteration:215,  Accuracy: 98.66898148148148\n","Iteration:216,  Accuracy: 98.67511520737327\n","Iteration:217,  Accuracy: 98.68119266055047\n","Iteration:218,  Accuracy: 98.68721461187215\n","Iteration:219,  Accuracy: 98.69318181818181\n","Iteration:220,  Accuracy: 98.69909502262443\n","Iteration:221,  Accuracy: 98.70495495495496\n","Iteration:222,  Accuracy: 98.71076233183857\n","Iteration:223,  Accuracy: 98.71651785714286\n","Iteration:224,  Accuracy: 98.69444444444444\n","Iteration:225,  Accuracy: 98.70022123893806\n","Iteration:226,  Accuracy: 98.69218061674009\n","Iteration:227,  Accuracy: 98.67050438596492\n","Iteration:228,  Accuracy: 98.67631004366812\n","Iteration:229,  Accuracy: 98.6820652173913\n","Iteration:230,  Accuracy: 98.68777056277057\n","Iteration:231,  Accuracy: 98.69342672413794\n","Iteration:232,  Accuracy: 98.69903433476395\n","Iteration:233,  Accuracy: 98.70459401709402\n","Iteration:234,  Accuracy: 98.69680851063829\n","Iteration:235,  Accuracy: 98.67584745762711\n","Iteration:236,  Accuracy: 98.65506329113924\n","Iteration:237,  Accuracy: 98.66071428571429\n","Iteration:238,  Accuracy: 98.6663179916318\n","Iteration:239,  Accuracy: 98.6328125\n","Iteration:240,  Accuracy: 98.62551867219916\n","Iteration:241,  Accuracy: 98.63119834710744\n","Iteration:242,  Accuracy: 98.62397119341564\n","Iteration:243,  Accuracy: 98.62961065573771\n","Iteration:244,  Accuracy: 98.63520408163265\n","Iteration:245,  Accuracy: 98.6280487804878\n","Iteration:246,  Accuracy: 98.6336032388664\n","Iteration:247,  Accuracy: 98.63911290322581\n","Iteration:248,  Accuracy: 98.6320281124498\n","Iteration:249,  Accuracy: 98.6375\n","Iteration:250,  Accuracy: 98.6429282868526\n","Iteration:251,  Accuracy: 98.6359126984127\n","Iteration:252,  Accuracy: 98.6413043478261\n","Iteration:253,  Accuracy: 98.64665354330708\n","Iteration:254,  Accuracy: 98.63970588235294\n","Iteration:255,  Accuracy: 98.64501953125\n","Iteration:256,  Accuracy: 98.63813229571984\n","Iteration:257,  Accuracy: 98.64341085271317\n","Iteration:258,  Accuracy: 98.62451737451738\n","Iteration:259,  Accuracy: 98.61778846153847\n","Iteration:260,  Accuracy: 98.62308429118774\n","Iteration:261,  Accuracy: 98.62833969465649\n","Iteration:262,  Accuracy: 98.62167300380229\n","Iteration:263,  Accuracy: 98.62689393939394\n","Iteration:264,  Accuracy: 98.63207547169812\n","Iteration:265,  Accuracy: 98.62546992481202\n","Iteration:266,  Accuracy: 98.63061797752809\n","Iteration:267,  Accuracy: 98.6357276119403\n","Iteration:268,  Accuracy: 98.64079925650557\n","Iteration:269,  Accuracy: 98.64583333333333\n","Iteration:270,  Accuracy: 98.65083025830258\n","Iteration:271,  Accuracy: 98.65579044117646\n","Iteration:272,  Accuracy: 98.63782051282051\n","Iteration:273,  Accuracy: 98.64279197080292\n","Iteration:274,  Accuracy: 98.64772727272727\n","Iteration:275,  Accuracy: 98.6526268115942\n","Iteration:276,  Accuracy: 98.64620938628158\n","Iteration:277,  Accuracy: 98.65107913669065\n","Iteration:278,  Accuracy: 98.65591397849462\n","Iteration:279,  Accuracy: 98.66071428571429\n","Iteration:280,  Accuracy: 98.6432384341637\n","Iteration:281,  Accuracy: 98.63696808510639\n","Iteration:282,  Accuracy: 98.63074204946996\n","Iteration:283,  Accuracy: 98.62455985915493\n","Iteration:284,  Accuracy: 98.61842105263158\n","Iteration:285,  Accuracy: 98.62325174825175\n","Iteration:286,  Accuracy: 98.61716027874564\n","Iteration:287,  Accuracy: 98.62196180555556\n","Iteration:288,  Accuracy: 98.6159169550173\n","Iteration:289,  Accuracy: 98.62068965517241\n","Iteration:290,  Accuracy: 98.60395189003437\n","Iteration:291,  Accuracy: 98.60873287671232\n","Iteration:292,  Accuracy: 98.61348122866895\n","Iteration:293,  Accuracy: 98.61819727891157\n","Iteration:294,  Accuracy: 98.61228813559322\n","Iteration:295,  Accuracy: 98.61697635135135\n","Iteration:296,  Accuracy: 98.621632996633\n","Iteration:297,  Accuracy: 98.62625838926175\n","Iteration:298,  Accuracy: 98.62040133779264\n","Iteration:299,  Accuracy: 98.61458333333333\n","Iteration:300,  Accuracy: 98.61918604651163\n","Iteration:301,  Accuracy: 98.6237582781457\n","Iteration:302,  Accuracy: 98.61798679867987\n","Iteration:303,  Accuracy: 98.62253289473684\n","Iteration:304,  Accuracy: 98.62704918032787\n","Iteration:305,  Accuracy: 98.63153594771242\n","Iteration:306,  Accuracy: 98.63599348534201\n","Iteration:307,  Accuracy: 98.63027597402598\n","Iteration:308,  Accuracy: 98.63470873786407\n","Iteration:309,  Accuracy: 98.63911290322581\n","Iteration:310,  Accuracy: 98.6434887459807\n","Iteration:311,  Accuracy: 98.63782051282051\n","Iteration:312,  Accuracy: 98.63218849840256\n","Iteration:313,  Accuracy: 98.63654458598727\n","Iteration:314,  Accuracy: 98.64087301587301\n","Iteration:315,  Accuracy: 98.62539556962025\n","Iteration:316,  Accuracy: 98.62973186119874\n","Iteration:317,  Accuracy: 98.63404088050315\n","Iteration:318,  Accuracy: 98.63832288401254\n","Iteration:319,  Accuracy: 98.642578125\n","Iteration:320,  Accuracy: 98.64680685358256\n","Iteration:321,  Accuracy: 98.65100931677019\n","Iteration:322,  Accuracy: 98.64551083591331\n","Iteration:323,  Accuracy: 98.6496913580247\n","Iteration:324,  Accuracy: 98.65384615384616\n","Iteration:325,  Accuracy: 98.64838957055214\n","Iteration:326,  Accuracy: 98.65252293577981\n","Iteration:327,  Accuracy: 98.64710365853658\n","Iteration:328,  Accuracy: 98.65121580547113\n","Iteration:329,  Accuracy: 98.65530303030303\n","Iteration:330,  Accuracy: 98.65936555891238\n","Iteration:331,  Accuracy: 98.66340361445783\n","Iteration:332,  Accuracy: 98.66741741741741\n","Iteration:333,  Accuracy: 98.67140718562874\n","Iteration:334,  Accuracy: 98.6660447761194\n","Iteration:335,  Accuracy: 98.66071428571429\n","Iteration:336,  Accuracy: 98.66468842729971\n","Iteration:337,  Accuracy: 98.65939349112426\n","Iteration:338,  Accuracy: 98.64491150442478\n","Iteration:339,  Accuracy: 98.64889705882354\n","Iteration:340,  Accuracy: 98.65285923753666\n","Iteration:341,  Accuracy: 98.65679824561404\n","Iteration:342,  Accuracy: 98.66071428571429\n","Iteration:343,  Accuracy: 98.66460755813954\n","Iteration:344,  Accuracy: 98.65942028985508\n","Iteration:345,  Accuracy: 98.66329479768787\n","Iteration:346,  Accuracy: 98.65814121037464\n","Iteration:347,  Accuracy: 98.66199712643679\n","Iteration:348,  Accuracy: 98.65687679083095\n","Iteration:349,  Accuracy: 98.66071428571429\n","Iteration:350,  Accuracy: 98.64672364672364\n","Iteration:351,  Accuracy: 98.65056818181819\n","Iteration:352,  Accuracy: 98.64553824362606\n","Iteration:353,  Accuracy: 98.64053672316385\n","Iteration:354,  Accuracy: 98.6443661971831\n","Iteration:355,  Accuracy: 98.63939606741573\n","Iteration:356,  Accuracy: 98.64320728291317\n","Iteration:357,  Accuracy: 98.63826815642459\n","Iteration:358,  Accuracy: 98.633356545961\n","Iteration:359,  Accuracy: 98.62847222222223\n","Iteration:360,  Accuracy: 98.61495844875347\n","Iteration:361,  Accuracy: 98.61878453038673\n","Iteration:362,  Accuracy: 98.62258953168045\n","Iteration:363,  Accuracy: 98.62637362637362\n","Iteration:364,  Accuracy: 98.63013698630137\n","Iteration:365,  Accuracy: 98.63387978142076\n","Iteration:366,  Accuracy: 98.62908719346049\n","Iteration:367,  Accuracy: 98.6328125\n","Iteration:368,  Accuracy: 98.6280487804878\n","Iteration:369,  Accuracy: 98.63175675675676\n","Iteration:370,  Accuracy: 98.63544474393531\n","Iteration:371,  Accuracy: 98.63911290322581\n","Iteration:372,  Accuracy: 98.64276139410188\n","Iteration:373,  Accuracy: 98.64639037433155\n","Iteration:374,  Accuracy: 98.63333333333334\n","Iteration:375,  Accuracy: 98.63696808510639\n","Iteration:376,  Accuracy: 98.64058355437666\n","Iteration:377,  Accuracy: 98.6441798941799\n","Iteration:378,  Accuracy: 98.64775725593668\n","Iteration:379,  Accuracy: 98.65131578947368\n","Iteration:380,  Accuracy: 98.64665354330708\n","Iteration:381,  Accuracy: 98.65019633507853\n","Iteration:382,  Accuracy: 98.64556135770235\n","Iteration:383,  Accuracy: 98.64095052083333\n","Iteration:384,  Accuracy: 98.62012987012987\n","Iteration:385,  Accuracy: 98.61560880829016\n","Iteration:386,  Accuracy: 98.59496124031008\n","Iteration:387,  Accuracy: 98.59052835051547\n","Iteration:388,  Accuracy: 98.59086883001852\n","Test Accuracy of the model on the 10000 test images: 98.59086883001852 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RmJkiiOKvIbM"},"source":["### 10개짜리"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7bm4VmZvJ30","executionInfo":{"status":"ok","timestamp":1623964182947,"user_tz":-540,"elapsed":714746,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"c3e1d540-6869-4003-a140-e362a91476d4"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 9\n","batch_size = 150\n","num_epochs = len(y_train[25000:]) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train[25000:], y_train[25000:]) # 400,160\n","train_loader = DataLoader(dataset=train_dataset, batch_size=150, shuffle=True)\n","\n","# Train the model 400*20*8 = 800*10*8\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        images = images[:, 10:]\n","        #print(images.shape)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/164], Step [100/165], Loss: 0.7253389954566956\n","Epoch [2/164], Step [100/165], Loss: 0.458177387714386\n","Epoch [3/164], Step [100/165], Loss: 0.27630671858787537\n","Epoch [4/164], Step [100/165], Loss: 0.3011864125728607\n","Epoch [5/164], Step [100/165], Loss: 0.24990302324295044\n","Epoch [6/164], Step [100/165], Loss: 0.15201915800571442\n","Epoch [7/164], Step [100/165], Loss: 0.13964848220348358\n","Epoch [8/164], Step [100/165], Loss: 0.16478371620178223\n","Epoch [9/164], Step [100/165], Loss: 0.12459197640419006\n","Epoch [10/164], Step [100/165], Loss: 0.07041636109352112\n","Epoch [11/164], Step [100/165], Loss: 0.06382420659065247\n","Epoch [12/164], Step [100/165], Loss: 0.04761909320950508\n","Epoch [13/164], Step [100/165], Loss: 0.07167492061853409\n","Epoch [14/164], Step [100/165], Loss: 0.020317669957876205\n","Epoch [15/164], Step [100/165], Loss: 0.020299185067415237\n","Epoch [16/164], Step [100/165], Loss: 0.06359891593456268\n","Epoch [17/164], Step [100/165], Loss: 0.01221522968262434\n","Epoch [18/164], Step [100/165], Loss: 0.044197313487529755\n","Epoch [19/164], Step [100/165], Loss: 0.015768524259328842\n","Epoch [20/164], Step [100/165], Loss: 0.022405879572033882\n","Epoch [21/164], Step [100/165], Loss: 0.06121949478983879\n","Epoch [22/164], Step [100/165], Loss: 0.022512372583150864\n","Epoch [23/164], Step [100/165], Loss: 0.03602716326713562\n","Epoch [24/164], Step [100/165], Loss: 0.033241238445043564\n","Epoch [25/164], Step [100/165], Loss: 0.005001038312911987\n","Epoch [26/164], Step [100/165], Loss: 0.02094665914773941\n","Epoch [27/164], Step [100/165], Loss: 0.027443045750260353\n","Epoch [28/164], Step [100/165], Loss: 0.0029840709175914526\n","Epoch [29/164], Step [100/165], Loss: 0.0031859534792602062\n","Epoch [30/164], Step [100/165], Loss: 0.0226017776876688\n","Epoch [31/164], Step [100/165], Loss: 0.027185238897800446\n","Epoch [32/164], Step [100/165], Loss: 0.038883086293935776\n","Epoch [33/164], Step [100/165], Loss: 0.00583370216190815\n","Epoch [34/164], Step [100/165], Loss: 0.005062670446932316\n","Epoch [35/164], Step [100/165], Loss: 0.022307157516479492\n","Epoch [36/164], Step [100/165], Loss: 0.0013927866239100695\n","Epoch [37/164], Step [100/165], Loss: 0.007326010148972273\n","Epoch [38/164], Step [100/165], Loss: 0.053577493876218796\n","Epoch [39/164], Step [100/165], Loss: 0.011928971856832504\n","Epoch [40/164], Step [100/165], Loss: 0.013914672657847404\n","Epoch [41/164], Step [100/165], Loss: 0.0026157370302826166\n","Epoch [42/164], Step [100/165], Loss: 0.0019342930754646659\n","Epoch [43/164], Step [100/165], Loss: 0.002253294223919511\n","Epoch [44/164], Step [100/165], Loss: 0.015185297466814518\n","Epoch [45/164], Step [100/165], Loss: 0.009514699690043926\n","Epoch [46/164], Step [100/165], Loss: 0.06623909622430801\n","Epoch [47/164], Step [100/165], Loss: 0.04084448888897896\n","Epoch [48/164], Step [100/165], Loss: 0.026063721626996994\n","Epoch [49/164], Step [100/165], Loss: 0.002753639128059149\n","Epoch [50/164], Step [100/165], Loss: 0.005912497639656067\n","Epoch [51/164], Step [100/165], Loss: 0.022181527689099312\n","Epoch [52/164], Step [100/165], Loss: 0.020998988300561905\n","Epoch [53/164], Step [100/165], Loss: 0.0009345848229713738\n","Epoch [54/164], Step [100/165], Loss: 0.04394206777215004\n","Epoch [55/164], Step [100/165], Loss: 0.01005566492676735\n","Epoch [56/164], Step [100/165], Loss: 0.019507640972733498\n","Epoch [57/164], Step [100/165], Loss: 0.021032122895121574\n","Epoch [58/164], Step [100/165], Loss: 0.0032556767109781504\n","Epoch [59/164], Step [100/165], Loss: 0.038820672780275345\n","Epoch [60/164], Step [100/165], Loss: 0.024031318724155426\n","Epoch [61/164], Step [100/165], Loss: 0.02794991247355938\n","Epoch [62/164], Step [100/165], Loss: 0.05052046850323677\n","Epoch [63/164], Step [100/165], Loss: 0.02600756101310253\n","Epoch [64/164], Step [100/165], Loss: 0.016174938529729843\n","Epoch [65/164], Step [100/165], Loss: 0.006699792575091124\n","Epoch [66/164], Step [100/165], Loss: 0.0015749019803479314\n","Epoch [67/164], Step [100/165], Loss: 0.0014195641269907355\n","Epoch [68/164], Step [100/165], Loss: 0.009057519026100636\n","Epoch [69/164], Step [100/165], Loss: 0.017445554956793785\n","Epoch [70/164], Step [100/165], Loss: 0.0067113786935806274\n","Epoch [71/164], Step [100/165], Loss: 0.002976360497996211\n","Epoch [72/164], Step [100/165], Loss: 0.020030764862895012\n","Epoch [73/164], Step [100/165], Loss: 0.002492474392056465\n","Epoch [74/164], Step [100/165], Loss: 0.03484760597348213\n","Epoch [75/164], Step [100/165], Loss: 0.0053705177269876\n","Epoch [76/164], Step [100/165], Loss: 0.005388645920902491\n","Epoch [77/164], Step [100/165], Loss: 0.02259904146194458\n","Epoch [78/164], Step [100/165], Loss: 0.012543019838631153\n","Epoch [79/164], Step [100/165], Loss: 0.008282775059342384\n","Epoch [80/164], Step [100/165], Loss: 0.002483982825651765\n","Epoch [81/164], Step [100/165], Loss: 0.003385507268831134\n","Epoch [82/164], Step [100/165], Loss: 0.001220286008901894\n","Epoch [83/164], Step [100/165], Loss: 0.008346166461706161\n","Epoch [84/164], Step [100/165], Loss: 0.008425980806350708\n","Epoch [85/164], Step [100/165], Loss: 0.054824650287628174\n","Epoch [86/164], Step [100/165], Loss: 0.02362644486129284\n","Epoch [87/164], Step [100/165], Loss: 0.001965756295248866\n","Epoch [88/164], Step [100/165], Loss: 0.010087249800562859\n","Epoch [89/164], Step [100/165], Loss: 0.014979805797338486\n","Epoch [90/164], Step [100/165], Loss: 0.0035910937003791332\n","Epoch [91/164], Step [100/165], Loss: 0.00830827560275793\n","Epoch [92/164], Step [100/165], Loss: 0.003889321582391858\n","Epoch [93/164], Step [100/165], Loss: 0.002357671270146966\n","Epoch [94/164], Step [100/165], Loss: 0.012813577428460121\n","Epoch [95/164], Step [100/165], Loss: 0.0036812513135373592\n","Epoch [96/164], Step [100/165], Loss: 0.01576998643577099\n","Epoch [97/164], Step [100/165], Loss: 0.004121657460927963\n","Epoch [98/164], Step [100/165], Loss: 0.038781601935625076\n","Epoch [99/164], Step [100/165], Loss: 0.006999776232987642\n","Epoch [100/164], Step [100/165], Loss: 0.015078553929924965\n","Epoch [101/164], Step [100/165], Loss: 0.0020446290727704763\n","Epoch [102/164], Step [100/165], Loss: 0.001354070147499442\n","Epoch [103/164], Step [100/165], Loss: 0.0013524185633286834\n","Epoch [104/164], Step [100/165], Loss: 0.07408260554075241\n","Epoch [105/164], Step [100/165], Loss: 0.004290346521884203\n","Epoch [106/164], Step [100/165], Loss: 0.003075233893468976\n","Epoch [107/164], Step [100/165], Loss: 0.016055401414632797\n","Epoch [108/164], Step [100/165], Loss: 0.053656697273254395\n","Epoch [109/164], Step [100/165], Loss: 0.0013296218821778893\n","Epoch [110/164], Step [100/165], Loss: 0.14228278398513794\n","Epoch [111/164], Step [100/165], Loss: 0.01949986070394516\n","Epoch [112/164], Step [100/165], Loss: 0.026070138439536095\n","Epoch [113/164], Step [100/165], Loss: 0.002434151479974389\n","Epoch [114/164], Step [100/165], Loss: 0.0017532813362777233\n","Epoch [115/164], Step [100/165], Loss: 0.0023431291338056326\n","Epoch [116/164], Step [100/165], Loss: 0.012469719164073467\n","Epoch [117/164], Step [100/165], Loss: 0.008898090571165085\n","Epoch [118/164], Step [100/165], Loss: 0.04683762416243553\n","Epoch [119/164], Step [100/165], Loss: 0.028522122651338577\n","Epoch [120/164], Step [100/165], Loss: 0.014409780502319336\n","Epoch [121/164], Step [100/165], Loss: 0.0016061479691416025\n","Epoch [122/164], Step [100/165], Loss: 0.003463835222646594\n","Epoch [123/164], Step [100/165], Loss: 0.004341801628470421\n","Epoch [124/164], Step [100/165], Loss: 0.0017236541025340557\n","Epoch [125/164], Step [100/165], Loss: 0.030558153986930847\n","Epoch [126/164], Step [100/165], Loss: 0.03190283104777336\n","Epoch [127/164], Step [100/165], Loss: 0.010500558651983738\n","Epoch [128/164], Step [100/165], Loss: 0.0003662344242911786\n","Epoch [129/164], Step [100/165], Loss: 0.0005167850176803768\n","Epoch [130/164], Step [100/165], Loss: 0.003383709117770195\n","Epoch [131/164], Step [100/165], Loss: 0.0007023775251582265\n","Epoch [132/164], Step [100/165], Loss: 0.04973534122109413\n","Epoch [133/164], Step [100/165], Loss: 0.0045429011806845665\n","Epoch [134/164], Step [100/165], Loss: 0.03685850277543068\n","Epoch [135/164], Step [100/165], Loss: 0.06289854645729065\n","Epoch [136/164], Step [100/165], Loss: 0.01650805026292801\n","Epoch [137/164], Step [100/165], Loss: 0.0021506703924387693\n","Epoch [138/164], Step [100/165], Loss: 0.002011488424614072\n","Epoch [139/164], Step [100/165], Loss: 0.002778171095997095\n","Epoch [140/164], Step [100/165], Loss: 0.01394660770893097\n","Epoch [141/164], Step [100/165], Loss: 0.008057364262640476\n","Epoch [142/164], Step [100/165], Loss: 0.002325883135199547\n","Epoch [143/164], Step [100/165], Loss: 0.01194552518427372\n","Epoch [144/164], Step [100/165], Loss: 0.044270794838666916\n","Epoch [145/164], Step [100/165], Loss: 0.0014107686001807451\n","Epoch [146/164], Step [100/165], Loss: 0.0025931885465979576\n","Epoch [147/164], Step [100/165], Loss: 0.0014964501606300473\n","Epoch [148/164], Step [100/165], Loss: 0.009300078265368938\n","Epoch [149/164], Step [100/165], Loss: 0.01750086061656475\n","Epoch [150/164], Step [100/165], Loss: 0.07248028367757797\n","Epoch [151/164], Step [100/165], Loss: 0.001299236319027841\n","Epoch [152/164], Step [100/165], Loss: 0.011709707789123058\n","Epoch [153/164], Step [100/165], Loss: 0.043029408901929855\n","Epoch [154/164], Step [100/165], Loss: 0.0020220030564814806\n","Epoch [155/164], Step [100/165], Loss: 0.0751078799366951\n","Epoch [156/164], Step [100/165], Loss: 0.038799628615379333\n","Epoch [157/164], Step [100/165], Loss: 0.002495527733117342\n","Epoch [158/164], Step [100/165], Loss: 0.0015596066368743777\n","Epoch [159/164], Step [100/165], Loss: 0.00828484259545803\n","Epoch [160/164], Step [100/165], Loss: 0.0022781151346862316\n","Epoch [161/164], Step [100/165], Loss: 0.03520432114601135\n","Epoch [162/164], Step [100/165], Loss: 0.0029386193491518497\n","Epoch [163/164], Step [100/165], Loss: 0.030076907947659492\n","Epoch [164/164], Step [100/165], Loss: 0.0023006396368145943\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WkdhTuuMvKmW","executionInfo":{"status":"ok","timestamp":1623965892432,"user_tz":-540,"elapsed":338,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"6a83846a-6954-4ad7-8bff-61f6f956a505"},"source":["# Test the model\n","test_dataset = CustomDataset(X_test[6000:], y_test[6000:])\n","test_loader = DataLoader(dataset=test_dataset, batch_size=60, shuffle=False)\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (images, labels) in enumerate(test_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        images = images[:, 10:]\n","        labels = labels.to(device) # 이런게 실제 동작 부위보다 시간이 오래걸림\n","        outputs = model(images) # 연산 시간이라 볼수도있는데\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 96.66666666666667\n","Iteration:1,  Accuracy: 96.66666666666667\n","Iteration:2,  Accuracy: 97.22222222222223\n","Iteration:3,  Accuracy: 97.91666666666667\n","Iteration:4,  Accuracy: 98.33333333333333\n","Iteration:5,  Accuracy: 98.33333333333333\n","Iteration:6,  Accuracy: 98.57142857142857\n","Iteration:7,  Accuracy: 98.33333333333333\n","Iteration:8,  Accuracy: 98.51851851851852\n","Iteration:9,  Accuracy: 98.5\n","Iteration:10,  Accuracy: 98.18181818181819\n","Iteration:11,  Accuracy: 98.33333333333333\n","Iteration:12,  Accuracy: 98.2051282051282\n","Iteration:13,  Accuracy: 97.97619047619048\n","Iteration:14,  Accuracy: 98.11111111111111\n","Iteration:15,  Accuracy: 98.02083333333333\n","Iteration:16,  Accuracy: 98.13725490196079\n","Iteration:17,  Accuracy: 98.05555555555556\n","Iteration:18,  Accuracy: 97.98245614035088\n","Iteration:19,  Accuracy: 97.83333333333333\n","Iteration:20,  Accuracy: 97.93650793650794\n","Iteration:21,  Accuracy: 97.95454545454545\n","Iteration:22,  Accuracy: 97.89855072463769\n","Iteration:23,  Accuracy: 97.91666666666667\n","Iteration:24,  Accuracy: 98.0\n","Iteration:25,  Accuracy: 98.07692307692308\n","Iteration:26,  Accuracy: 98.08641975308642\n","Iteration:27,  Accuracy: 98.03571428571429\n","Iteration:28,  Accuracy: 98.04597701149426\n","Iteration:29,  Accuracy: 98.05555555555556\n","Iteration:30,  Accuracy: 98.01075268817205\n","Iteration:31,  Accuracy: 98.07291666666667\n","Iteration:32,  Accuracy: 98.08080808080808\n","Iteration:33,  Accuracy: 98.03921568627452\n","Iteration:34,  Accuracy: 98.0\n","Iteration:35,  Accuracy: 97.96296296296296\n","Iteration:36,  Accuracy: 97.97297297297297\n","Iteration:37,  Accuracy: 98.02631578947368\n","Iteration:38,  Accuracy: 98.03418803418803\n","Iteration:39,  Accuracy: 98.0\n","Iteration:40,  Accuracy: 97.96747967479675\n","Iteration:41,  Accuracy: 98.01587301587301\n","Iteration:42,  Accuracy: 98.02325581395348\n","Iteration:43,  Accuracy: 98.06818181818181\n","Iteration:44,  Accuracy: 98.07407407407408\n","Iteration:45,  Accuracy: 98.1159420289855\n","Iteration:46,  Accuracy: 98.15602836879432\n","Iteration:47,  Accuracy: 98.05555555555556\n","Iteration:48,  Accuracy: 98.0952380952381\n","Iteration:49,  Accuracy: 98.1\n","Iteration:50,  Accuracy: 98.13725490196079\n","Iteration:51,  Accuracy: 98.14102564102564\n","Iteration:52,  Accuracy: 98.17610062893081\n","Iteration:53,  Accuracy: 98.05555555555556\n","Iteration:54,  Accuracy: 98.0909090909091\n","Iteration:55,  Accuracy: 98.125\n","Iteration:56,  Accuracy: 98.15789473684211\n","Iteration:57,  Accuracy: 98.16091954022988\n","Iteration:58,  Accuracy: 98.1638418079096\n","Iteration:59,  Accuracy: 98.19444444444444\n","Iteration:60,  Accuracy: 98.22404371584699\n","Iteration:61,  Accuracy: 98.1989247311828\n","Iteration:62,  Accuracy: 98.17460317460318\n","Iteration:63,  Accuracy: 98.17708333333333\n","Iteration:64,  Accuracy: 98.2051282051282\n","Iteration:65,  Accuracy: 98.15656565656566\n","Iteration:66,  Accuracy: 98.08457711442786\n","Iteration:67,  Accuracy: 98.08823529411765\n","Iteration:68,  Accuracy: 98.06763285024155\n","Iteration:69,  Accuracy: 98.07142857142857\n","Iteration:70,  Accuracy: 98.07511737089202\n","Iteration:71,  Accuracy: 98.07870370370371\n","Iteration:72,  Accuracy: 98.08219178082192\n","Iteration:73,  Accuracy: 98.10810810810811\n","Iteration:74,  Accuracy: 98.13333333333334\n","Iteration:75,  Accuracy: 98.1140350877193\n","Iteration:76,  Accuracy: 98.13852813852814\n","Iteration:77,  Accuracy: 98.11965811965813\n","Iteration:78,  Accuracy: 98.10126582278481\n","Iteration:79,  Accuracy: 98.08333333333333\n","Iteration:80,  Accuracy: 98.08641975308642\n","Iteration:81,  Accuracy: 98.10975609756098\n","Iteration:82,  Accuracy: 98.11244979919678\n","Iteration:83,  Accuracy: 98.11507936507937\n","Iteration:84,  Accuracy: 98.13725490196079\n","Iteration:85,  Accuracy: 98.15891472868218\n","Iteration:86,  Accuracy: 98.16091954022988\n","Iteration:87,  Accuracy: 98.16287878787878\n","Iteration:88,  Accuracy: 98.16479400749064\n","Iteration:89,  Accuracy: 98.16666666666667\n","Iteration:90,  Accuracy: 98.13186813186813\n","Iteration:91,  Accuracy: 98.15217391304348\n","Iteration:92,  Accuracy: 98.17204301075269\n","Iteration:93,  Accuracy: 98.17375886524823\n","Iteration:94,  Accuracy: 98.17543859649123\n","Iteration:95,  Accuracy: 98.17708333333333\n","Iteration:96,  Accuracy: 98.1786941580756\n","Iteration:97,  Accuracy: 98.19727891156462\n","Iteration:98,  Accuracy: 98.14814814814815\n","Iteration:99,  Accuracy: 98.13333333333334\n","Iteration:100,  Accuracy: 98.1023102310231\n","Iteration:101,  Accuracy: 98.0718954248366\n","Iteration:102,  Accuracy: 98.07443365695792\n","Iteration:103,  Accuracy: 98.09294871794872\n","Iteration:104,  Accuracy: 98.11111111111111\n","Iteration:105,  Accuracy: 98.11320754716981\n","Iteration:106,  Accuracy: 98.11497117931142\n","Test Accuracy of the model: 98.11497117931142 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TRze_y74GwS2"},"source":["# 논문과 직접적 비교를 위한 학습\n","## Binary Classification"]},{"cell_type":"markdown","metadata":{"id":"Qq9bD7wsG2Kk"},"source":["### 1. 2020-07-12 데이터로만 학습 및 테스트\n","##### 논문에서의 결과) 99%"]},{"cell_type":"code","metadata":{"id":"T7fd3OlrPzSb"},"source":["X_train_0712 = np.load(\"X_train_0712.npy\")\n","y_train_0712 = np.load(\"y_train_0712.npy\")\n","X_test_0712 = np.load(\"X_test_0712.npy\")\n","y_test_0712 = np.load(\"y_test_0712.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sY0NKHIBHoO7","executionInfo":{"elapsed":6,"status":"ok","timestamp":1623301013438,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"7bb37d99-8dc0-415c-f686-ba3462f33f41"},"source":["print(X_train_0712.shape)\n","print(X_test_0712.shape)\n","print(np.bincount(y_train_0712))\n","print(np.bincount(y_test_0712))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(7193, 160)\n","(1799, 160)\n","[3371 3822]\n","[843 956]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yor1L5IBlULe","executionInfo":{"elapsed":780759,"status":"ok","timestamp":1623301796723,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"1d1c5bc3-cfc9-49fb-fc1d-7899bec278f2"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 2\n","batch_size = 32\n","num_epochs = len(y_train_0712) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train_0712, y_train_0712)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/224], Step [100/225], Loss: 0.16171139478683472\n","Epoch [1/224], Step [200/225], Loss: 0.11455738544464111\n","Epoch [2/224], Step [100/225], Loss: 0.058471713215112686\n","Epoch [2/224], Step [200/225], Loss: 0.018002161756157875\n","Epoch [3/224], Step [100/225], Loss: 0.2823757827281952\n","Epoch [3/224], Step [200/225], Loss: 0.08304735273122787\n","Epoch [4/224], Step [100/225], Loss: 0.0025273743085563183\n","Epoch [4/224], Step [200/225], Loss: 0.004826669581234455\n","Epoch [5/224], Step [100/225], Loss: 0.0034316948149353266\n","Epoch [5/224], Step [200/225], Loss: 0.024067139253020287\n","Epoch [6/224], Step [100/225], Loss: 0.009466120973229408\n","Epoch [6/224], Step [200/225], Loss: 0.0032103373669087887\n","Epoch [7/224], Step [100/225], Loss: 0.18426887691020966\n","Epoch [7/224], Step [200/225], Loss: 0.0359390527009964\n","Epoch [8/224], Step [100/225], Loss: 0.15726642310619354\n","Epoch [8/224], Step [200/225], Loss: 0.0031849767547100782\n","Epoch [9/224], Step [100/225], Loss: 0.001125242910347879\n","Epoch [9/224], Step [200/225], Loss: 0.0006286355201154947\n","Epoch [10/224], Step [100/225], Loss: 0.0007353681139647961\n","Epoch [10/224], Step [200/225], Loss: 0.0014921757392585278\n","Epoch [11/224], Step [100/225], Loss: 0.043931782245635986\n","Epoch [11/224], Step [200/225], Loss: 0.0010308782802894711\n","Epoch [12/224], Step [100/225], Loss: 0.0010780961019918323\n","Epoch [12/224], Step [200/225], Loss: 0.00059179199161008\n","Epoch [13/224], Step [100/225], Loss: 0.0004880871274508536\n","Epoch [13/224], Step [200/225], Loss: 0.02257203496992588\n","Epoch [14/224], Step [100/225], Loss: 0.0017455387860536575\n","Epoch [14/224], Step [200/225], Loss: 0.00046709051821380854\n","Epoch [15/224], Step [100/225], Loss: 0.001024729572236538\n","Epoch [15/224], Step [200/225], Loss: 0.0010845829965546727\n","Epoch [16/224], Step [100/225], Loss: 0.02746729925274849\n","Epoch [16/224], Step [200/225], Loss: 0.003733663586899638\n","Epoch [17/224], Step [100/225], Loss: 0.06280263513326645\n","Epoch [17/224], Step [200/225], Loss: 0.0009556474396958947\n","Epoch [18/224], Step [100/225], Loss: 0.008157302625477314\n","Epoch [18/224], Step [200/225], Loss: 0.0016018298920243979\n","Epoch [19/224], Step [100/225], Loss: 0.0004178365343250334\n","Epoch [19/224], Step [200/225], Loss: 0.0003849538043141365\n","Epoch [20/224], Step [100/225], Loss: 0.0003770480107050389\n","Epoch [20/224], Step [200/225], Loss: 0.0018360426183789968\n","Epoch [21/224], Step [100/225], Loss: 0.00028872358961962163\n","Epoch [21/224], Step [200/225], Loss: 7.414293941110373e-05\n","Epoch [22/224], Step [100/225], Loss: 4.682423605117947e-05\n","Epoch [22/224], Step [200/225], Loss: 7.273418304976076e-05\n","Epoch [23/224], Step [100/225], Loss: 0.0009592778515070677\n","Epoch [23/224], Step [200/225], Loss: 7.953449676278979e-05\n","Epoch [24/224], Step [100/225], Loss: 0.00023776074522174895\n","Epoch [24/224], Step [200/225], Loss: 7.810172974132001e-05\n","Epoch [25/224], Step [100/225], Loss: 3.873303285217844e-05\n","Epoch [25/224], Step [200/225], Loss: 9.243816748494282e-05\n","Epoch [26/224], Step [100/225], Loss: 6.478396244347095e-05\n","Epoch [26/224], Step [200/225], Loss: 8.2881459093187e-05\n","Epoch [27/224], Step [100/225], Loss: 2.109146589646116e-05\n","Epoch [27/224], Step [200/225], Loss: 0.0005122664151713252\n","Epoch [28/224], Step [100/225], Loss: 6.855275569250807e-05\n","Epoch [28/224], Step [200/225], Loss: 2.1124264094396494e-05\n","Epoch [29/224], Step [100/225], Loss: 0.0011717143934220076\n","Epoch [29/224], Step [200/225], Loss: 0.007777871564030647\n","Epoch [30/224], Step [100/225], Loss: 0.0029661813750863075\n","Epoch [30/224], Step [200/225], Loss: 0.004658864811062813\n","Epoch [31/224], Step [100/225], Loss: 0.003177161794155836\n","Epoch [31/224], Step [200/225], Loss: 0.03528345748782158\n","Epoch [32/224], Step [100/225], Loss: 0.0003511738032102585\n","Epoch [32/224], Step [200/225], Loss: 0.00589112238958478\n","Epoch [33/224], Step [100/225], Loss: 0.000807094038464129\n","Epoch [33/224], Step [200/225], Loss: 0.00019657783559523523\n","Epoch [34/224], Step [100/225], Loss: 0.00021246547112241387\n","Epoch [34/224], Step [200/225], Loss: 0.000394319009501487\n","Epoch [35/224], Step [100/225], Loss: 0.0002899050014093518\n","Epoch [35/224], Step [200/225], Loss: 0.00017879364895634353\n","Epoch [36/224], Step [100/225], Loss: 0.00014199278666637838\n","Epoch [36/224], Step [200/225], Loss: 0.0004605896247085184\n","Epoch [37/224], Step [100/225], Loss: 0.00041911969310604036\n","Epoch [37/224], Step [200/225], Loss: 0.00014373795420397073\n","Epoch [38/224], Step [100/225], Loss: 0.0001855887530837208\n","Epoch [38/224], Step [200/225], Loss: 0.00011140795686515048\n","Epoch [39/224], Step [100/225], Loss: 4.020117921754718e-05\n","Epoch [39/224], Step [200/225], Loss: 2.1416104573290795e-05\n","Epoch [40/224], Step [100/225], Loss: 3.670321893878281e-05\n","Epoch [40/224], Step [200/225], Loss: 3.0273173251771368e-05\n","Epoch [41/224], Step [100/225], Loss: 3.1659117667004466e-05\n","Epoch [41/224], Step [200/225], Loss: 1.862835415522568e-05\n","Epoch [42/224], Step [100/225], Loss: 5.98275255470071e-06\n","Epoch [42/224], Step [200/225], Loss: 0.00028982371441088617\n","Epoch [43/224], Step [100/225], Loss: 6.951329851290211e-06\n","Epoch [43/224], Step [200/225], Loss: 5.4239758355834056e-06\n","Epoch [44/224], Step [100/225], Loss: 3.315492222100147e-06\n","Epoch [44/224], Step [200/225], Loss: 1.7489579477114603e-05\n","Epoch [45/224], Step [100/225], Loss: 3.1515705813944805e-06\n","Epoch [45/224], Step [200/225], Loss: 5.185541795071913e-06\n","Epoch [46/224], Step [100/225], Loss: 3.270786692155525e-06\n","Epoch [46/224], Step [200/225], Loss: 8.90307182999095e-06\n","Epoch [47/224], Step [100/225], Loss: 4.838984295929549e-06\n","Epoch [47/224], Step [200/225], Loss: 2.5145507152046775e-06\n","Epoch [48/224], Step [100/225], Loss: 3.222348141207476e-06\n","Epoch [48/224], Step [200/225], Loss: 1.6465747876281966e-06\n","Epoch [49/224], Step [100/225], Loss: 1.8775406260829186e-06\n","Epoch [49/224], Step [200/225], Loss: 1.911066874527023e-06\n","Epoch [50/224], Step [100/225], Loss: 1.8700872033150517e-06\n","Epoch [50/224], Step [200/225], Loss: 6.659572682110593e-05\n","Epoch [51/224], Step [100/225], Loss: 2.9056975563435117e-06\n","Epoch [51/224], Step [200/225], Loss: 1.2777721849488444e-06\n","Epoch [52/224], Step [100/225], Loss: 1.3038485349170514e-06\n","Epoch [52/224], Step [200/225], Loss: 1.6577433825659682e-06\n","Epoch [53/224], Step [100/225], Loss: 6.183972232065571e-07\n","Epoch [53/224], Step [200/225], Loss: 8.009366183614475e-07\n","Epoch [54/224], Step [100/225], Loss: 2.3494654669775628e-05\n","Epoch [54/224], Step [200/225], Loss: 5.103643161419313e-07\n","Epoch [55/224], Step [100/225], Loss: 7.003538939898135e-07\n","Epoch [55/224], Step [200/225], Loss: 1.12130919660558e-06\n","Epoch [56/224], Step [100/225], Loss: 7.040791274448566e-07\n","Epoch [56/224], Step [200/225], Loss: 2.9822254873579368e-05\n","Epoch [57/224], Step [100/225], Loss: 5.476172191265505e-07\n","Epoch [57/224], Step [200/225], Loss: 1.2554182831081562e-06\n","Epoch [58/224], Step [100/225], Loss: 1.3597119732366991e-06\n","Epoch [58/224], Step [200/225], Loss: 8.866134066920495e-07\n","Epoch [59/224], Step [100/225], Loss: 3.390012466297776e-07\n","Epoch [59/224], Step [200/225], Loss: 7.978710527822841e-06\n","Epoch [60/224], Step [100/225], Loss: 2.1979204234412464e-07\n","Epoch [60/224], Step [200/225], Loss: 1.8626448650138627e-07\n","Epoch [61/224], Step [100/225], Loss: 1.7881382063933415e-07\n","Epoch [61/224], Step [200/225], Loss: 2.0489088115027698e-07\n","Epoch [62/224], Step [100/225], Loss: 5.10363634020905e-07\n","Epoch [62/224], Step [200/225], Loss: 3.799787577918323e-07\n","Epoch [63/224], Step [100/225], Loss: 2.011656192735245e-07\n","Epoch [63/224], Step [200/225], Loss: 2.809118086588569e-05\n","Epoch [64/224], Step [100/225], Loss: 2.905721885326784e-07\n","Epoch [64/224], Step [200/225], Loss: 5.476143769556074e-07\n","Epoch [65/224], Step [100/225], Loss: 4.24682355060213e-07\n","Epoch [65/224], Step [200/225], Loss: 1.2293453721667902e-07\n","Epoch [66/224], Step [100/225], Loss: 1.8737887330644298e-06\n","Epoch [66/224], Step [200/225], Loss: 3.9487932212978194e-07\n","Epoch [67/224], Step [100/225], Loss: 1.8626434439283912e-07\n","Epoch [67/224], Step [200/225], Loss: 1.266596996174485e-07\n","Epoch [68/224], Step [100/225], Loss: 1.1920921849650767e-07\n","Epoch [68/224], Step [200/225], Loss: 5.5879347371501353e-08\n","Epoch [69/224], Step [100/225], Loss: 8.940695295223122e-08\n","Epoch [69/224], Step [200/225], Loss: 5.960463766996327e-08\n","Epoch [70/224], Step [100/225], Loss: 6.332992796842518e-08\n","Epoch [70/224], Step [200/225], Loss: 2.16066638358825e-07\n","Epoch [71/224], Step [100/225], Loss: 5.587933316064664e-08\n","Epoch [71/224], Step [200/225], Loss: 8.195635814445268e-08\n","Epoch [72/224], Step [100/225], Loss: 2.9802318834981634e-08\n","Epoch [72/224], Step [200/225], Loss: 3.725289943190546e-08\n","Epoch [73/224], Step [100/225], Loss: 2.8684686981250707e-07\n","Epoch [73/224], Step [200/225], Loss: 9.313223614526578e-08\n","Epoch [74/224], Step [100/225], Loss: 2.607702853651972e-08\n","Epoch [74/224], Step [200/225], Loss: 2.607703031287656e-08\n","Epoch [75/224], Step [100/225], Loss: 8.195635103902532e-08\n","Epoch [75/224], Step [200/225], Loss: 4.8428766774577525e-08\n","Epoch [76/224], Step [100/225], Loss: 3.6507702816379606e-07\n","Epoch [76/224], Step [200/225], Loss: 1.2293449458411487e-07\n","Epoch [77/224], Step [100/225], Loss: 6.370186156345881e-07\n","Epoch [77/224], Step [200/225], Loss: 3.352760913344355e-08\n","Epoch [78/224], Step [100/225], Loss: 6.332989954671575e-08\n","Epoch [78/224], Step [200/225], Loss: 1.1175870007207322e-08\n","Epoch [79/224], Step [100/225], Loss: 1.4901160305669237e-08\n","Epoch [79/224], Step [200/225], Loss: 4.8428741905581774e-08\n","Epoch [80/224], Step [100/225], Loss: 3.725290076417309e-09\n","Epoch [80/224], Step [200/225], Loss: 3.725290076417309e-09\n","Epoch [81/224], Step [100/225], Loss: 1.1175870007207322e-08\n","Epoch [81/224], Step [200/225], Loss: 1.1175870007207322e-08\n","Epoch [82/224], Step [100/225], Loss: 1.1175869119028903e-08\n","Epoch [82/224], Step [200/225], Loss: 7.450574912581942e-08\n","Epoch [83/224], Step [100/225], Loss: 3.725290076417309e-09\n","Epoch [83/224], Step [200/225], Loss: 0.0\n","Epoch [84/224], Step [100/225], Loss: 1.1175870007207322e-08\n","Epoch [84/224], Step [200/225], Loss: 1.4901159417490817e-08\n","Epoch [85/224], Step [100/225], Loss: 7.450580152834618e-09\n","Epoch [85/224], Step [200/225], Loss: 3.9487895264755934e-07\n","Epoch [86/224], Step [100/225], Loss: 7.450580152834618e-09\n","Epoch [86/224], Step [200/225], Loss: 1.4901157641133977e-08\n","Epoch [87/224], Step [100/225], Loss: 4.023288227017474e-07\n","Epoch [87/224], Step [200/225], Loss: 1.4901157641133977e-08\n","Epoch [88/224], Step [100/225], Loss: 6.332991375757047e-08\n","Epoch [88/224], Step [200/225], Loss: 1.1175869119028903e-08\n","Epoch [89/224], Step [100/225], Loss: 7.4505797087454084e-09\n","Epoch [89/224], Step [200/225], Loss: 3.725288166833707e-08\n","Epoch [90/224], Step [100/225], Loss: 3.725290076417309e-09\n","Epoch [90/224], Step [200/225], Loss: 0.0\n","Epoch [91/224], Step [100/225], Loss: 1.4901159417490817e-08\n","Epoch [91/224], Step [200/225], Loss: 0.0\n","Epoch [92/224], Step [100/225], Loss: 0.0\n","Epoch [92/224], Step [200/225], Loss: 7.4505797087454084e-09\n","Epoch [93/224], Step [100/225], Loss: 3.725290076417309e-09\n","Epoch [93/224], Step [200/225], Loss: 0.0\n","Epoch [94/224], Step [100/225], Loss: 3.725290076417309e-09\n","Epoch [94/224], Step [200/225], Loss: 3.725290076417309e-09\n","Epoch [95/224], Step [100/225], Loss: 1.1175870007207322e-08\n","Epoch [95/224], Step [200/225], Loss: 0.0\n","Epoch [96/224], Step [100/225], Loss: 0.0\n","Epoch [96/224], Step [200/225], Loss: 0.0\n","Epoch [97/224], Step [100/225], Loss: 0.0\n","Epoch [97/224], Step [200/225], Loss: 0.0\n","Epoch [98/224], Step [100/225], Loss: 0.0\n","Epoch [98/224], Step [200/225], Loss: 0.0\n","Epoch [99/224], Step [100/225], Loss: 3.725290076417309e-09\n","Epoch [99/224], Step [200/225], Loss: 0.0\n","Epoch [100/224], Step [100/225], Loss: 0.0\n","Epoch [100/224], Step [200/225], Loss: 7.4505797087454084e-09\n","Epoch [101/224], Step [100/225], Loss: 4.470345160711986e-08\n","Epoch [101/224], Step [200/225], Loss: 0.0\n","Epoch [102/224], Step [100/225], Loss: 2.9802308176840597e-08\n","Epoch [102/224], Step [200/225], Loss: 0.0\n","Epoch [103/224], Step [100/225], Loss: 0.0\n","Epoch [103/224], Step [200/225], Loss: 0.0\n","Epoch [104/224], Step [100/225], Loss: 0.0\n","Epoch [104/224], Step [200/225], Loss: 3.725290076417309e-09\n","Epoch [105/224], Step [100/225], Loss: 0.0\n","Epoch [105/224], Step [200/225], Loss: 1.4901157641133977e-08\n","Epoch [106/224], Step [100/225], Loss: 0.0\n","Epoch [106/224], Step [200/225], Loss: 0.0\n","Epoch [107/224], Step [100/225], Loss: 0.0\n","Epoch [107/224], Step [200/225], Loss: 0.0\n","Epoch [108/224], Step [100/225], Loss: 0.0\n","Epoch [108/224], Step [200/225], Loss: 0.0\n","Epoch [109/224], Step [100/225], Loss: 0.0\n","Epoch [109/224], Step [200/225], Loss: 0.0\n","Epoch [110/224], Step [100/225], Loss: 0.0\n","Epoch [110/224], Step [200/225], Loss: 0.0\n","Epoch [111/224], Step [100/225], Loss: 0.0\n","Epoch [111/224], Step [200/225], Loss: 0.0\n","Epoch [112/224], Step [100/225], Loss: 0.0\n","Epoch [112/224], Step [200/225], Loss: 0.0\n","Epoch [113/224], Step [100/225], Loss: 0.0\n","Epoch [113/224], Step [200/225], Loss: 0.0\n","Epoch [114/224], Step [100/225], Loss: 0.0\n","Epoch [114/224], Step [200/225], Loss: 3.725290076417309e-09\n","Epoch [115/224], Step [100/225], Loss: 0.0\n","Epoch [115/224], Step [200/225], Loss: 0.0\n","Epoch [116/224], Step [100/225], Loss: 0.0\n","Epoch [116/224], Step [200/225], Loss: 0.0\n","Epoch [117/224], Step [100/225], Loss: 0.0\n","Epoch [117/224], Step [200/225], Loss: 0.0\n","Epoch [118/224], Step [100/225], Loss: 0.0\n","Epoch [118/224], Step [200/225], Loss: 0.0\n","Epoch [119/224], Step [100/225], Loss: 0.0\n","Epoch [119/224], Step [200/225], Loss: 0.0\n","Epoch [120/224], Step [100/225], Loss: 0.0\n","Epoch [120/224], Step [200/225], Loss: 0.0\n","Epoch [121/224], Step [100/225], Loss: 0.0\n","Epoch [121/224], Step [200/225], Loss: 0.0\n","Epoch [122/224], Step [100/225], Loss: 0.0\n","Epoch [122/224], Step [200/225], Loss: 0.0\n","Epoch [123/224], Step [100/225], Loss: 3.725290076417309e-09\n","Epoch [123/224], Step [200/225], Loss: 0.0\n","Epoch [124/224], Step [100/225], Loss: 0.0\n","Epoch [124/224], Step [200/225], Loss: 0.0\n","Epoch [125/224], Step [100/225], Loss: 0.0\n","Epoch [125/224], Step [200/225], Loss: 0.0\n","Epoch [126/224], Step [100/225], Loss: 3.725290076417309e-09\n","Epoch [126/224], Step [200/225], Loss: 0.0\n","Epoch [127/224], Step [100/225], Loss: 0.0\n","Epoch [127/224], Step [200/225], Loss: 0.0\n","Epoch [128/224], Step [100/225], Loss: 0.0\n","Epoch [128/224], Step [200/225], Loss: 0.0\n","Epoch [129/224], Step [100/225], Loss: 0.0\n","Epoch [129/224], Step [200/225], Loss: 0.0\n","Epoch [130/224], Step [100/225], Loss: 0.0\n","Epoch [130/224], Step [200/225], Loss: 0.0\n","Epoch [131/224], Step [100/225], Loss: 0.0\n","Epoch [131/224], Step [200/225], Loss: 0.0\n","Epoch [132/224], Step [100/225], Loss: 0.0\n","Epoch [132/224], Step [200/225], Loss: 0.0\n","Epoch [133/224], Step [100/225], Loss: 0.0\n","Epoch [133/224], Step [200/225], Loss: 0.0\n","Epoch [134/224], Step [100/225], Loss: 0.0\n","Epoch [134/224], Step [200/225], Loss: 0.0\n","Epoch [135/224], Step [100/225], Loss: 0.0\n","Epoch [135/224], Step [200/225], Loss: 0.0\n","Epoch [136/224], Step [100/225], Loss: 0.0\n","Epoch [136/224], Step [200/225], Loss: 0.0\n","Epoch [137/224], Step [100/225], Loss: 0.0\n","Epoch [137/224], Step [200/225], Loss: 0.0\n","Epoch [138/224], Step [100/225], Loss: 0.0\n","Epoch [138/224], Step [200/225], Loss: 0.0\n","Epoch [139/224], Step [100/225], Loss: 0.0\n","Epoch [139/224], Step [200/225], Loss: 0.0\n","Epoch [140/224], Step [100/225], Loss: 0.0\n","Epoch [140/224], Step [200/225], Loss: 0.0\n","Epoch [141/224], Step [100/225], Loss: 0.0\n","Epoch [141/224], Step [200/225], Loss: 0.0\n","Epoch [142/224], Step [100/225], Loss: 0.0\n","Epoch [142/224], Step [200/225], Loss: 0.0\n","Epoch [143/224], Step [100/225], Loss: 0.0\n","Epoch [143/224], Step [200/225], Loss: 0.0\n","Epoch [144/224], Step [100/225], Loss: 0.0\n","Epoch [144/224], Step [200/225], Loss: 0.0\n","Epoch [145/224], Step [100/225], Loss: 0.0\n","Epoch [145/224], Step [200/225], Loss: 0.0\n","Epoch [146/224], Step [100/225], Loss: 0.0\n","Epoch [146/224], Step [200/225], Loss: 0.0\n","Epoch [147/224], Step [100/225], Loss: 0.0\n","Epoch [147/224], Step [200/225], Loss: 0.0\n","Epoch [148/224], Step [100/225], Loss: 0.0\n","Epoch [148/224], Step [200/225], Loss: 0.0\n","Epoch [149/224], Step [100/225], Loss: 0.0\n","Epoch [149/224], Step [200/225], Loss: 0.0\n","Epoch [150/224], Step [100/225], Loss: 0.0\n","Epoch [150/224], Step [200/225], Loss: 0.0\n","Epoch [151/224], Step [100/225], Loss: 0.0\n","Epoch [151/224], Step [200/225], Loss: 0.0\n","Epoch [152/224], Step [100/225], Loss: 0.0\n","Epoch [152/224], Step [200/225], Loss: 0.0\n","Epoch [153/224], Step [100/225], Loss: 0.0\n","Epoch [153/224], Step [200/225], Loss: 0.0\n","Epoch [154/224], Step [100/225], Loss: 0.0\n","Epoch [154/224], Step [200/225], Loss: 0.0\n","Epoch [155/224], Step [100/225], Loss: 0.0\n","Epoch [155/224], Step [200/225], Loss: 0.0\n","Epoch [156/224], Step [100/225], Loss: 0.0\n","Epoch [156/224], Step [200/225], Loss: 0.0\n","Epoch [157/224], Step [100/225], Loss: 0.0\n","Epoch [157/224], Step [200/225], Loss: 0.0\n","Epoch [158/224], Step [100/225], Loss: 0.0\n","Epoch [158/224], Step [200/225], Loss: 0.0\n","Epoch [159/224], Step [100/225], Loss: 0.0\n","Epoch [159/224], Step [200/225], Loss: 0.0\n","Epoch [160/224], Step [100/225], Loss: 0.0\n","Epoch [160/224], Step [200/225], Loss: 0.0\n","Epoch [161/224], Step [100/225], Loss: 0.0\n","Epoch [161/224], Step [200/225], Loss: 0.0\n","Epoch [162/224], Step [100/225], Loss: 0.0\n","Epoch [162/224], Step [200/225], Loss: 0.0\n","Epoch [163/224], Step [100/225], Loss: 0.0\n","Epoch [163/224], Step [200/225], Loss: 0.0\n","Epoch [164/224], Step [100/225], Loss: 0.0\n","Epoch [164/224], Step [200/225], Loss: 0.0\n","Epoch [165/224], Step [100/225], Loss: 0.0\n","Epoch [165/224], Step [200/225], Loss: 0.0\n","Epoch [166/224], Step [100/225], Loss: 0.0\n","Epoch [166/224], Step [200/225], Loss: 0.0\n","Epoch [167/224], Step [100/225], Loss: 0.0\n","Epoch [167/224], Step [200/225], Loss: 0.0\n","Epoch [168/224], Step [100/225], Loss: 0.0\n","Epoch [168/224], Step [200/225], Loss: 0.0\n","Epoch [169/224], Step [100/225], Loss: 0.0\n","Epoch [169/224], Step [200/225], Loss: 0.0\n","Epoch [170/224], Step [100/225], Loss: 0.0\n","Epoch [170/224], Step [200/225], Loss: 0.0\n","Epoch [171/224], Step [100/225], Loss: 0.0\n","Epoch [171/224], Step [200/225], Loss: 0.0\n","Epoch [172/224], Step [100/225], Loss: 0.0\n","Epoch [172/224], Step [200/225], Loss: 0.0\n","Epoch [173/224], Step [100/225], Loss: 0.0\n","Epoch [173/224], Step [200/225], Loss: 0.0\n","Epoch [174/224], Step [100/225], Loss: 0.0\n","Epoch [174/224], Step [200/225], Loss: 0.0\n","Epoch [175/224], Step [100/225], Loss: 0.0\n","Epoch [175/224], Step [200/225], Loss: 0.0\n","Epoch [176/224], Step [100/225], Loss: 0.0\n","Epoch [176/224], Step [200/225], Loss: 0.0\n","Epoch [177/224], Step [100/225], Loss: 0.0\n","Epoch [177/224], Step [200/225], Loss: 0.0\n","Epoch [178/224], Step [100/225], Loss: 0.0\n","Epoch [178/224], Step [200/225], Loss: 0.0\n","Epoch [179/224], Step [100/225], Loss: 0.0\n","Epoch [179/224], Step [200/225], Loss: 0.0\n","Epoch [180/224], Step [100/225], Loss: 0.0\n","Epoch [180/224], Step [200/225], Loss: 0.0\n","Epoch [181/224], Step [100/225], Loss: 0.0\n","Epoch [181/224], Step [200/225], Loss: 0.0\n","Epoch [182/224], Step [100/225], Loss: 0.0\n","Epoch [182/224], Step [200/225], Loss: 0.0\n","Epoch [183/224], Step [100/225], Loss: 0.0\n","Epoch [183/224], Step [200/225], Loss: 0.0\n","Epoch [184/224], Step [100/225], Loss: 0.0\n","Epoch [184/224], Step [200/225], Loss: 0.0\n","Epoch [185/224], Step [100/225], Loss: 0.0\n","Epoch [185/224], Step [200/225], Loss: 0.0\n","Epoch [186/224], Step [100/225], Loss: 0.0\n","Epoch [186/224], Step [200/225], Loss: 0.0\n","Epoch [187/224], Step [100/225], Loss: 0.0\n","Epoch [187/224], Step [200/225], Loss: 0.0\n","Epoch [188/224], Step [100/225], Loss: 0.0\n","Epoch [188/224], Step [200/225], Loss: 0.0\n","Epoch [189/224], Step [100/225], Loss: 0.0\n","Epoch [189/224], Step [200/225], Loss: 0.0\n","Epoch [190/224], Step [100/225], Loss: 0.0\n","Epoch [190/224], Step [200/225], Loss: 0.0\n","Epoch [191/224], Step [100/225], Loss: 0.0\n","Epoch [191/224], Step [200/225], Loss: 0.0\n","Epoch [192/224], Step [100/225], Loss: 0.0\n","Epoch [192/224], Step [200/225], Loss: 0.0\n","Epoch [193/224], Step [100/225], Loss: 0.0\n","Epoch [193/224], Step [200/225], Loss: 0.0\n","Epoch [194/224], Step [100/225], Loss: 0.0\n","Epoch [194/224], Step [200/225], Loss: 0.0\n","Epoch [195/224], Step [100/225], Loss: 0.0\n","Epoch [195/224], Step [200/225], Loss: 0.0\n","Epoch [196/224], Step [100/225], Loss: 0.0\n","Epoch [196/224], Step [200/225], Loss: 0.0\n","Epoch [197/224], Step [100/225], Loss: 0.0\n","Epoch [197/224], Step [200/225], Loss: 0.0\n","Epoch [198/224], Step [100/225], Loss: 0.0\n","Epoch [198/224], Step [200/225], Loss: 0.0\n","Epoch [199/224], Step [100/225], Loss: 0.0\n","Epoch [199/224], Step [200/225], Loss: 0.0\n","Epoch [200/224], Step [100/225], Loss: 0.0\n","Epoch [200/224], Step [200/225], Loss: 0.0\n","Epoch [201/224], Step [100/225], Loss: 0.0\n","Epoch [201/224], Step [200/225], Loss: 0.0\n","Epoch [202/224], Step [100/225], Loss: 0.0\n","Epoch [202/224], Step [200/225], Loss: 0.0\n","Epoch [203/224], Step [100/225], Loss: 0.0\n","Epoch [203/224], Step [200/225], Loss: 0.0\n","Epoch [204/224], Step [100/225], Loss: 0.0\n","Epoch [204/224], Step [200/225], Loss: 0.0\n","Epoch [205/224], Step [100/225], Loss: 0.0\n","Epoch [205/224], Step [200/225], Loss: 0.0\n","Epoch [206/224], Step [100/225], Loss: 0.0\n","Epoch [206/224], Step [200/225], Loss: 0.0\n","Epoch [207/224], Step [100/225], Loss: 0.0\n","Epoch [207/224], Step [200/225], Loss: 0.0\n","Epoch [208/224], Step [100/225], Loss: 0.0\n","Epoch [208/224], Step [200/225], Loss: 0.0\n","Epoch [209/224], Step [100/225], Loss: 0.0\n","Epoch [209/224], Step [200/225], Loss: 0.0\n","Epoch [210/224], Step [100/225], Loss: 0.0\n","Epoch [210/224], Step [200/225], Loss: 0.0\n","Epoch [211/224], Step [100/225], Loss: 0.0\n","Epoch [211/224], Step [200/225], Loss: 0.0\n","Epoch [212/224], Step [100/225], Loss: 0.0\n","Epoch [212/224], Step [200/225], Loss: 0.0\n","Epoch [213/224], Step [100/225], Loss: 0.0\n","Epoch [213/224], Step [200/225], Loss: 0.0\n","Epoch [214/224], Step [100/225], Loss: 0.0\n","Epoch [214/224], Step [200/225], Loss: 0.0\n","Epoch [215/224], Step [100/225], Loss: 0.0\n","Epoch [215/224], Step [200/225], Loss: 0.0\n","Epoch [216/224], Step [100/225], Loss: 0.0\n","Epoch [216/224], Step [200/225], Loss: 0.0\n","Epoch [217/224], Step [100/225], Loss: 0.0\n","Epoch [217/224], Step [200/225], Loss: 0.0\n","Epoch [218/224], Step [100/225], Loss: 0.0\n","Epoch [218/224], Step [200/225], Loss: 0.0\n","Epoch [219/224], Step [100/225], Loss: 0.0\n","Epoch [219/224], Step [200/225], Loss: 0.0\n","Epoch [220/224], Step [100/225], Loss: 0.0\n","Epoch [220/224], Step [200/225], Loss: 0.0\n","Epoch [221/224], Step [100/225], Loss: 0.0\n","Epoch [221/224], Step [200/225], Loss: 0.0\n","Epoch [222/224], Step [100/225], Loss: 0.0\n","Epoch [222/224], Step [200/225], Loss: 0.0\n","Epoch [223/224], Step [100/225], Loss: 0.0\n","Epoch [223/224], Step [200/225], Loss: 0.0\n","Epoch [224/224], Step [100/225], Loss: 0.0\n","Epoch [224/224], Step [200/225], Loss: 0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLSY0e74IL1A","executionInfo":{"elapsed":847,"status":"ok","timestamp":1623301797560,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"50b2546a-3f21-4fa7-c6a0-ad28c13d1fe3"},"source":["# Test the model\n","test_dataset = CustomDataset(X_test_0712, y_test_0712)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=20, shuffle=False)\n","\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (images, labels) in enumerate(test_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        if i==0:\n","          y_pre_0712 = predicted.tolist()\n","        else:\n","          y_pre_0712.extend(predicted.tolist())\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","\n","torch.save(model.state_dict(), 'model.ckpt')\n","\n","target_names =  ['Nominal', 'Faulty']\n","print(classification_report(y_test_0712.tolist(), y_pre_0712, target_names=target_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 100.0\n","Iteration:1,  Accuracy: 100.0\n","Iteration:2,  Accuracy: 100.0\n","Iteration:3,  Accuracy: 100.0\n","Iteration:4,  Accuracy: 100.0\n","Iteration:5,  Accuracy: 100.0\n","Iteration:6,  Accuracy: 100.0\n","Iteration:7,  Accuracy: 100.0\n","Iteration:8,  Accuracy: 100.0\n","Iteration:9,  Accuracy: 100.0\n","Iteration:10,  Accuracy: 100.0\n","Iteration:11,  Accuracy: 100.0\n","Iteration:12,  Accuracy: 100.0\n","Iteration:13,  Accuracy: 100.0\n","Iteration:14,  Accuracy: 100.0\n","Iteration:15,  Accuracy: 100.0\n","Iteration:16,  Accuracy: 100.0\n","Iteration:17,  Accuracy: 100.0\n","Iteration:18,  Accuracy: 100.0\n","Iteration:19,  Accuracy: 100.0\n","Iteration:20,  Accuracy: 100.0\n","Iteration:21,  Accuracy: 100.0\n","Iteration:22,  Accuracy: 100.0\n","Iteration:23,  Accuracy: 100.0\n","Iteration:24,  Accuracy: 100.0\n","Iteration:25,  Accuracy: 100.0\n","Iteration:26,  Accuracy: 100.0\n","Iteration:27,  Accuracy: 100.0\n","Iteration:28,  Accuracy: 100.0\n","Iteration:29,  Accuracy: 100.0\n","Iteration:30,  Accuracy: 100.0\n","Iteration:31,  Accuracy: 100.0\n","Iteration:32,  Accuracy: 100.0\n","Iteration:33,  Accuracy: 100.0\n","Iteration:34,  Accuracy: 99.85714285714286\n","Iteration:35,  Accuracy: 99.86111111111111\n","Iteration:36,  Accuracy: 99.72972972972973\n","Iteration:37,  Accuracy: 99.73684210526316\n","Iteration:38,  Accuracy: 99.74358974358974\n","Iteration:39,  Accuracy: 99.75\n","Iteration:40,  Accuracy: 99.7560975609756\n","Iteration:41,  Accuracy: 99.76190476190476\n","Iteration:42,  Accuracy: 99.76744186046511\n","Iteration:43,  Accuracy: 99.77272727272727\n","Iteration:44,  Accuracy: 99.77777777777777\n","Iteration:45,  Accuracy: 99.78260869565217\n","Iteration:46,  Accuracy: 99.7872340425532\n","Iteration:47,  Accuracy: 99.79166666666667\n","Iteration:48,  Accuracy: 99.79591836734694\n","Iteration:49,  Accuracy: 99.8\n","Iteration:50,  Accuracy: 99.80392156862744\n","Iteration:51,  Accuracy: 99.8076923076923\n","Iteration:52,  Accuracy: 99.81132075471699\n","Iteration:53,  Accuracy: 99.81481481481481\n","Iteration:54,  Accuracy: 99.81818181818181\n","Iteration:55,  Accuracy: 99.82142857142857\n","Iteration:56,  Accuracy: 99.82456140350877\n","Iteration:57,  Accuracy: 99.82758620689656\n","Iteration:58,  Accuracy: 99.83050847457628\n","Iteration:59,  Accuracy: 99.83333333333333\n","Iteration:60,  Accuracy: 99.8360655737705\n","Iteration:61,  Accuracy: 99.83870967741936\n","Iteration:62,  Accuracy: 99.84126984126983\n","Iteration:63,  Accuracy: 99.84375\n","Iteration:64,  Accuracy: 99.84615384615384\n","Iteration:65,  Accuracy: 99.84848484848484\n","Iteration:66,  Accuracy: 99.85074626865672\n","Iteration:67,  Accuracy: 99.8529411764706\n","Iteration:68,  Accuracy: 99.85507246376811\n","Iteration:69,  Accuracy: 99.85714285714286\n","Iteration:70,  Accuracy: 99.85915492957747\n","Iteration:71,  Accuracy: 99.86111111111111\n","Iteration:72,  Accuracy: 99.86301369863014\n","Iteration:73,  Accuracy: 99.86486486486487\n","Iteration:74,  Accuracy: 99.86666666666666\n","Iteration:75,  Accuracy: 99.86842105263158\n","Iteration:76,  Accuracy: 99.87012987012987\n","Iteration:77,  Accuracy: 99.87179487179488\n","Iteration:78,  Accuracy: 99.87341772151899\n","Iteration:79,  Accuracy: 99.875\n","Iteration:80,  Accuracy: 99.87654320987654\n","Iteration:81,  Accuracy: 99.8780487804878\n","Iteration:82,  Accuracy: 99.87951807228916\n","Iteration:83,  Accuracy: 99.88095238095238\n","Iteration:84,  Accuracy: 99.88235294117646\n","Iteration:85,  Accuracy: 99.88372093023256\n","Iteration:86,  Accuracy: 99.88505747126437\n","Iteration:87,  Accuracy: 99.88636363636364\n","Iteration:88,  Accuracy: 99.88764044943821\n","Iteration:89,  Accuracy: 99.83324068927182\n","Test Accuracy of the model on the 10000 test images: 99.83324068927182 %\n","              precision    recall  f1-score   support\n","\n","     Nominal       1.00      1.00      1.00       843\n","      Faulty       1.00      1.00      1.00       956\n","\n","    accuracy                           1.00      1799\n","   macro avg       1.00      1.00      1.00      1799\n","weighted avg       1.00      1.00      1.00      1799\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lymVGyGDQsUU"},"source":["### 12일 데이터를 학습한 모델로 13일 데이터 분류하기\n","##### 논문 결과) acc 61%"]},{"cell_type":"code","metadata":{"id":"d0MpeYUjPf2n"},"source":["X_0713_transformed = np.load(\"X_0713_transformed.npy\")\n","y_0713_transformed = np.load(\"y_0713_transformed.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"21fKq5Pbe-HS"},"source":["X_train = np.concatenate(( X_train_0712, X_test_0712), axis=0)\n","y_train = np.concatenate(( y_train_0712, y_test_0712), axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29JL-p7HZ3kL","executionInfo":{"elapsed":8,"status":"ok","timestamp":1623301799445,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"827bc78d-e503-4de4-c1db-722de4419387"},"source":["print(X_train.shape)\n","print(np.bincount(y_train))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(8992, 160)\n","[4214 4778]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxPVjSr8ZZtH","executionInfo":{"elapsed":1196483,"status":"ok","timestamp":1623302995922,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"bd38467b-e754-4ef1-bed6-eff15132bbf9"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 2\n","batch_size = 35\n","num_epochs = len(y_train) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train, y_train)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=35, shuffle=True)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/256], Step [100/257], Loss: 0.16657941043376923\n","Epoch [1/256], Step [200/257], Loss: 0.06347063928842545\n","Epoch [2/256], Step [100/257], Loss: 0.021714773029088974\n","Epoch [2/256], Step [200/257], Loss: 0.006323862820863724\n","Epoch [3/256], Step [100/257], Loss: 0.10770280659198761\n","Epoch [3/256], Step [200/257], Loss: 0.026077792048454285\n","Epoch [4/256], Step [100/257], Loss: 0.029044324532151222\n","Epoch [4/256], Step [200/257], Loss: 0.04301884025335312\n","Epoch [5/256], Step [100/257], Loss: 0.12814775109291077\n","Epoch [5/256], Step [200/257], Loss: 0.0026919811498373747\n","Epoch [6/256], Step [100/257], Loss: 0.010787892155349255\n","Epoch [6/256], Step [200/257], Loss: 0.004405262414366007\n","Epoch [7/256], Step [100/257], Loss: 0.06666428595781326\n","Epoch [7/256], Step [200/257], Loss: 0.0036222219932824373\n","Epoch [8/256], Step [100/257], Loss: 0.005862861406058073\n","Epoch [8/256], Step [200/257], Loss: 0.12053440511226654\n","Epoch [9/256], Step [100/257], Loss: 0.035588521510362625\n","Epoch [9/256], Step [200/257], Loss: 0.0026775600854307413\n","Epoch [10/256], Step [100/257], Loss: 0.09877374768257141\n","Epoch [10/256], Step [200/257], Loss: 0.004921954125165939\n","Epoch [11/256], Step [100/257], Loss: 0.004773123189806938\n","Epoch [11/256], Step [200/257], Loss: 0.23649317026138306\n","Epoch [12/256], Step [100/257], Loss: 0.012468956410884857\n","Epoch [12/256], Step [200/257], Loss: 0.02278994210064411\n","Epoch [13/256], Step [100/257], Loss: 0.004819631110876799\n","Epoch [13/256], Step [200/257], Loss: 0.0027970443479716778\n","Epoch [14/256], Step [100/257], Loss: 0.0003193884331267327\n","Epoch [14/256], Step [200/257], Loss: 0.001858679112046957\n","Epoch [15/256], Step [100/257], Loss: 0.014637540094554424\n","Epoch [15/256], Step [200/257], Loss: 0.002112890360876918\n","Epoch [16/256], Step [100/257], Loss: 0.0001511884038336575\n","Epoch [16/256], Step [200/257], Loss: 0.0010558723006397486\n","Epoch [17/256], Step [100/257], Loss: 0.0014463155530393124\n","Epoch [17/256], Step [200/257], Loss: 0.0003163593646604568\n","Epoch [18/256], Step [100/257], Loss: 0.001805617823265493\n","Epoch [18/256], Step [200/257], Loss: 0.002989665139466524\n","Epoch [19/256], Step [100/257], Loss: 0.00023882764799054712\n","Epoch [19/256], Step [200/257], Loss: 0.0001276613911613822\n","Epoch [20/256], Step [100/257], Loss: 0.034524645656347275\n","Epoch [20/256], Step [200/257], Loss: 0.004022055771201849\n","Epoch [21/256], Step [100/257], Loss: 0.0018293570028617978\n","Epoch [21/256], Step [200/257], Loss: 0.0003588262770790607\n","Epoch [22/256], Step [100/257], Loss: 0.001383021823130548\n","Epoch [22/256], Step [200/257], Loss: 0.0003363776777405292\n","Epoch [23/256], Step [100/257], Loss: 2.370140100538265e-05\n","Epoch [23/256], Step [200/257], Loss: 3.7123965739738196e-05\n","Epoch [24/256], Step [100/257], Loss: 0.001642564544454217\n","Epoch [24/256], Step [200/257], Loss: 0.004313600715249777\n","Epoch [25/256], Step [100/257], Loss: 0.0007936424226500094\n","Epoch [25/256], Step [200/257], Loss: 0.016490763053297997\n","Epoch [26/256], Step [100/257], Loss: 0.0003584836085792631\n","Epoch [26/256], Step [200/257], Loss: 0.0005108713521622121\n","Epoch [27/256], Step [100/257], Loss: 0.00030436707311309874\n","Epoch [27/256], Step [200/257], Loss: 0.0004280547145754099\n","Epoch [28/256], Step [100/257], Loss: 0.00032749390811659396\n","Epoch [28/256], Step [200/257], Loss: 0.0006454375106841326\n","Epoch [29/256], Step [100/257], Loss: 4.440963675733656e-05\n","Epoch [29/256], Step [200/257], Loss: 2.7750369554269128e-05\n","Epoch [30/256], Step [100/257], Loss: 1.1181318768649362e-05\n","Epoch [30/256], Step [200/257], Loss: 6.402787403203547e-05\n","Epoch [31/256], Step [100/257], Loss: 0.00013554573524743319\n","Epoch [31/256], Step [200/257], Loss: 3.138597821816802e-05\n","Epoch [32/256], Step [100/257], Loss: 8.264321513706818e-05\n","Epoch [32/256], Step [200/257], Loss: 0.0024523495230823755\n","Epoch [33/256], Step [100/257], Loss: 0.003413743106648326\n","Epoch [33/256], Step [200/257], Loss: 0.0014246113132685423\n","Epoch [34/256], Step [100/257], Loss: 0.020003430545330048\n","Epoch [34/256], Step [200/257], Loss: 0.0022596283815801144\n","Epoch [35/256], Step [100/257], Loss: 0.0006995064322836697\n","Epoch [35/256], Step [200/257], Loss: 0.028094932436943054\n","Epoch [36/256], Step [100/257], Loss: 0.0017469655722379684\n","Epoch [36/256], Step [200/257], Loss: 0.002200348535552621\n","Epoch [37/256], Step [100/257], Loss: 4.625863948604092e-05\n","Epoch [37/256], Step [200/257], Loss: 0.0045289453119039536\n","Epoch [38/256], Step [100/257], Loss: 0.10289063304662704\n","Epoch [38/256], Step [200/257], Loss: 0.0005300938501022756\n","Epoch [39/256], Step [100/257], Loss: 0.0003102955233771354\n","Epoch [39/256], Step [200/257], Loss: 0.020325958728790283\n","Epoch [40/256], Step [100/257], Loss: 9.709987352835014e-05\n","Epoch [40/256], Step [200/257], Loss: 0.0004814651911146939\n","Epoch [41/256], Step [100/257], Loss: 0.005938390269875526\n","Epoch [41/256], Step [200/257], Loss: 0.10065694898366928\n","Epoch [42/256], Step [100/257], Loss: 0.01680729165673256\n","Epoch [42/256], Step [200/257], Loss: 0.004179146606475115\n","Epoch [43/256], Step [100/257], Loss: 0.02107749879360199\n","Epoch [43/256], Step [200/257], Loss: 0.010925560258328915\n","Epoch [44/256], Step [100/257], Loss: 0.0017515730578452349\n","Epoch [44/256], Step [200/257], Loss: 0.00040736718801781535\n","Epoch [45/256], Step [100/257], Loss: 0.0025247756857424974\n","Epoch [45/256], Step [200/257], Loss: 9.824068547459319e-05\n","Epoch [46/256], Step [100/257], Loss: 0.010894249193370342\n","Epoch [46/256], Step [200/257], Loss: 3.879575524479151e-05\n","Epoch [47/256], Step [100/257], Loss: 0.002194730332121253\n","Epoch [47/256], Step [200/257], Loss: 0.0005289741675369442\n","Epoch [48/256], Step [100/257], Loss: 0.006868906319141388\n","Epoch [48/256], Step [200/257], Loss: 7.543602987425402e-05\n","Epoch [49/256], Step [100/257], Loss: 0.005586420185863972\n","Epoch [49/256], Step [200/257], Loss: 0.010534066706895828\n","Epoch [50/256], Step [100/257], Loss: 0.005982452072203159\n","Epoch [50/256], Step [200/257], Loss: 0.013234841637313366\n","Epoch [51/256], Step [100/257], Loss: 0.0001902865042211488\n","Epoch [51/256], Step [200/257], Loss: 8.343818626599386e-05\n","Epoch [52/256], Step [100/257], Loss: 0.045287083834409714\n","Epoch [52/256], Step [200/257], Loss: 0.001768150250427425\n","Epoch [53/256], Step [100/257], Loss: 4.7703637392260134e-05\n","Epoch [53/256], Step [200/257], Loss: 0.02204541116952896\n","Epoch [54/256], Step [100/257], Loss: 0.007882658392190933\n","Epoch [54/256], Step [200/257], Loss: 0.0009202679502777755\n","Epoch [55/256], Step [100/257], Loss: 0.00039774985634721816\n","Epoch [55/256], Step [200/257], Loss: 0.0014473055489361286\n","Epoch [56/256], Step [100/257], Loss: 0.0002859141677618027\n","Epoch [56/256], Step [200/257], Loss: 0.00032412196742370725\n","Epoch [57/256], Step [100/257], Loss: 0.0011033958289772272\n","Epoch [57/256], Step [200/257], Loss: 5.709628021577373e-05\n","Epoch [58/256], Step [100/257], Loss: 0.0012687179259955883\n","Epoch [58/256], Step [200/257], Loss: 0.012659335508942604\n","Epoch [59/256], Step [100/257], Loss: 0.004440347198396921\n","Epoch [59/256], Step [200/257], Loss: 0.0038240947760641575\n","Epoch [60/256], Step [100/257], Loss: 0.00024108732759486884\n","Epoch [60/256], Step [200/257], Loss: 0.0038826584350317717\n","Epoch [61/256], Step [100/257], Loss: 0.0008816540939733386\n","Epoch [61/256], Step [200/257], Loss: 0.00042503056465648115\n","Epoch [62/256], Step [100/257], Loss: 0.0006592922145500779\n","Epoch [62/256], Step [200/257], Loss: 0.00040710868779569864\n","Epoch [63/256], Step [100/257], Loss: 0.0006628430564887822\n","Epoch [63/256], Step [200/257], Loss: 0.0008596164989285171\n","Epoch [64/256], Step [100/257], Loss: 0.00024373883206862956\n","Epoch [64/256], Step [200/257], Loss: 0.00016952329315245152\n","Epoch [65/256], Step [100/257], Loss: 0.0013903896324336529\n","Epoch [65/256], Step [200/257], Loss: 0.00035604852018877864\n","Epoch [66/256], Step [100/257], Loss: 0.0014226711355149746\n","Epoch [66/256], Step [200/257], Loss: 5.3079897043062374e-05\n","Epoch [67/256], Step [100/257], Loss: 0.03961106389760971\n","Epoch [67/256], Step [200/257], Loss: 0.0014367146650329232\n","Epoch [68/256], Step [100/257], Loss: 0.001640691189095378\n","Epoch [68/256], Step [200/257], Loss: 0.0017305177170783281\n","Epoch [69/256], Step [100/257], Loss: 0.0019020208856090903\n","Epoch [69/256], Step [200/257], Loss: 0.0003586698148865253\n","Epoch [70/256], Step [100/257], Loss: 0.005180142819881439\n","Epoch [70/256], Step [200/257], Loss: 0.0015616093296557665\n","Epoch [71/256], Step [100/257], Loss: 0.0002511167258489877\n","Epoch [71/256], Step [200/257], Loss: 0.00010452402057126164\n","Epoch [72/256], Step [100/257], Loss: 0.0003886873309966177\n","Epoch [72/256], Step [200/257], Loss: 0.0005782474763691425\n","Epoch [73/256], Step [100/257], Loss: 0.00011202003952348605\n","Epoch [73/256], Step [200/257], Loss: 0.10161935538053513\n","Epoch [74/256], Step [100/257], Loss: 0.012482176534831524\n","Epoch [74/256], Step [200/257], Loss: 0.010673734359443188\n","Epoch [75/256], Step [100/257], Loss: 0.0015943478792905807\n","Epoch [75/256], Step [200/257], Loss: 0.0013107924023643136\n","Epoch [76/256], Step [100/257], Loss: 0.03549080714583397\n","Epoch [76/256], Step [200/257], Loss: 0.005421431269496679\n","Epoch [77/256], Step [100/257], Loss: 0.09153816848993301\n","Epoch [77/256], Step [200/257], Loss: 0.004688168875873089\n","Epoch [78/256], Step [100/257], Loss: 0.0016708477633073926\n","Epoch [78/256], Step [200/257], Loss: 0.0041035558097064495\n","Epoch [79/256], Step [100/257], Loss: 0.006294095888733864\n","Epoch [79/256], Step [200/257], Loss: 0.10554051399230957\n","Epoch [80/256], Step [100/257], Loss: 0.0009924143087118864\n","Epoch [80/256], Step [200/257], Loss: 0.2606792449951172\n","Epoch [81/256], Step [100/257], Loss: 0.21509259939193726\n","Epoch [81/256], Step [200/257], Loss: 0.0035277451388537884\n","Epoch [82/256], Step [100/257], Loss: 0.023124996572732925\n","Epoch [82/256], Step [200/257], Loss: 0.0407397635281086\n","Epoch [83/256], Step [100/257], Loss: 0.18291744589805603\n","Epoch [83/256], Step [200/257], Loss: 0.0015823724679648876\n","Epoch [84/256], Step [100/257], Loss: 0.012684402987360954\n","Epoch [84/256], Step [200/257], Loss: 0.013059388846158981\n","Epoch [85/256], Step [100/257], Loss: 0.0042661940678954124\n","Epoch [85/256], Step [200/257], Loss: 0.0053327674977481365\n","Epoch [86/256], Step [100/257], Loss: 0.003609528299421072\n","Epoch [86/256], Step [200/257], Loss: 0.022647997364401817\n","Epoch [87/256], Step [100/257], Loss: 0.0009614809532649815\n","Epoch [87/256], Step [200/257], Loss: 0.007025711704045534\n","Epoch [88/256], Step [100/257], Loss: 0.03185036778450012\n","Epoch [88/256], Step [200/257], Loss: 0.015420634299516678\n","Epoch [89/256], Step [100/257], Loss: 0.0012014216044917703\n","Epoch [89/256], Step [200/257], Loss: 0.0029653224628418684\n","Epoch [90/256], Step [100/257], Loss: 0.006532592233270407\n","Epoch [90/256], Step [200/257], Loss: 0.02317899651825428\n","Epoch [91/256], Step [100/257], Loss: 0.0027653430588543415\n","Epoch [91/256], Step [200/257], Loss: 0.0008673957781866193\n","Epoch [92/256], Step [100/257], Loss: 0.07013043761253357\n","Epoch [92/256], Step [200/257], Loss: 0.005410319194197655\n","Epoch [93/256], Step [100/257], Loss: 0.0032806165982037783\n","Epoch [93/256], Step [200/257], Loss: 0.0038594582583755255\n","Epoch [94/256], Step [100/257], Loss: 0.0025733993388712406\n","Epoch [94/256], Step [200/257], Loss: 0.0017337774625048041\n","Epoch [95/256], Step [100/257], Loss: 0.04081515967845917\n","Epoch [95/256], Step [200/257], Loss: 0.0364559106528759\n","Epoch [96/256], Step [100/257], Loss: 0.3180603086948395\n","Epoch [96/256], Step [200/257], Loss: 0.013152890838682652\n","Epoch [97/256], Step [100/257], Loss: 0.002258728723973036\n","Epoch [97/256], Step [200/257], Loss: 0.15899494290351868\n","Epoch [98/256], Step [100/257], Loss: 0.02745748683810234\n","Epoch [98/256], Step [200/257], Loss: 0.00797448679804802\n","Epoch [99/256], Step [100/257], Loss: 0.01502535305917263\n","Epoch [99/256], Step [200/257], Loss: 0.030486324802041054\n","Epoch [100/256], Step [100/257], Loss: 0.023224124684929848\n","Epoch [100/256], Step [200/257], Loss: 0.008199542760848999\n","Epoch [101/256], Step [100/257], Loss: 0.017064038664102554\n","Epoch [101/256], Step [200/257], Loss: 0.00991656444966793\n","Epoch [102/256], Step [100/257], Loss: 0.02087641879916191\n","Epoch [102/256], Step [200/257], Loss: 0.09931782633066177\n","Epoch [103/256], Step [100/257], Loss: 0.10304399579763412\n","Epoch [103/256], Step [200/257], Loss: 0.042059797793626785\n","Epoch [104/256], Step [100/257], Loss: 0.034947749227285385\n","Epoch [104/256], Step [200/257], Loss: 0.056445326656103134\n","Epoch [105/256], Step [100/257], Loss: 0.0022879340685904026\n","Epoch [105/256], Step [200/257], Loss: 0.008985225111246109\n","Epoch [106/256], Step [100/257], Loss: 0.1331678181886673\n","Epoch [106/256], Step [200/257], Loss: 0.010152619332075119\n","Epoch [107/256], Step [100/257], Loss: 0.14900429546833038\n","Epoch [107/256], Step [200/257], Loss: 0.1177305206656456\n","Epoch [108/256], Step [100/257], Loss: 0.008879868313670158\n","Epoch [108/256], Step [200/257], Loss: 0.027519267052412033\n","Epoch [109/256], Step [100/257], Loss: 0.006029624491930008\n","Epoch [109/256], Step [200/257], Loss: 0.0009011043002828956\n","Epoch [110/256], Step [100/257], Loss: 0.04170840606093407\n","Epoch [110/256], Step [200/257], Loss: 0.00733313150703907\n","Epoch [111/256], Step [100/257], Loss: 0.14154188334941864\n","Epoch [111/256], Step [200/257], Loss: 0.011639686301350594\n","Epoch [112/256], Step [100/257], Loss: 0.012665792368352413\n","Epoch [112/256], Step [200/257], Loss: 0.022600794211030006\n","Epoch [113/256], Step [100/257], Loss: 0.043342091143131256\n","Epoch [113/256], Step [200/257], Loss: 0.022391654551029205\n","Epoch [114/256], Step [100/257], Loss: 0.133161261677742\n","Epoch [114/256], Step [200/257], Loss: 0.05558139458298683\n","Epoch [115/256], Step [100/257], Loss: 0.0055603706277906895\n","Epoch [115/256], Step [200/257], Loss: 0.01433485932648182\n","Epoch [116/256], Step [100/257], Loss: 0.13412147760391235\n","Epoch [116/256], Step [200/257], Loss: 0.0025958481710404158\n","Epoch [117/256], Step [100/257], Loss: 0.004997509066015482\n","Epoch [117/256], Step [200/257], Loss: 0.002788582118228078\n","Epoch [118/256], Step [100/257], Loss: 0.0023246805649250746\n","Epoch [118/256], Step [200/257], Loss: 0.10537385940551758\n","Epoch [119/256], Step [100/257], Loss: 0.0057044196873903275\n","Epoch [119/256], Step [200/257], Loss: 0.007510197814553976\n","Epoch [120/256], Step [100/257], Loss: 0.004417410586029291\n","Epoch [120/256], Step [200/257], Loss: 0.01291240006685257\n","Epoch [121/256], Step [100/257], Loss: 0.022918472066521645\n","Epoch [121/256], Step [200/257], Loss: 0.005718528293073177\n","Epoch [122/256], Step [100/257], Loss: 0.1224612221121788\n","Epoch [122/256], Step [200/257], Loss: 0.013028147630393505\n","Epoch [123/256], Step [100/257], Loss: 0.005742610897868872\n","Epoch [123/256], Step [200/257], Loss: 0.007930277846753597\n","Epoch [124/256], Step [100/257], Loss: 0.13203571736812592\n","Epoch [124/256], Step [200/257], Loss: 0.14411108195781708\n","Epoch [125/256], Step [100/257], Loss: 0.09666401147842407\n","Epoch [125/256], Step [200/257], Loss: 0.07380334287881851\n","Epoch [126/256], Step [100/257], Loss: 0.16466587781906128\n","Epoch [126/256], Step [200/257], Loss: 0.0035311244428157806\n","Epoch [127/256], Step [100/257], Loss: 0.008186494931578636\n","Epoch [127/256], Step [200/257], Loss: 0.12734563648700714\n","Epoch [128/256], Step [100/257], Loss: 0.007731591816991568\n","Epoch [128/256], Step [200/257], Loss: 0.008413528092205524\n","Epoch [129/256], Step [100/257], Loss: 0.015381399542093277\n","Epoch [129/256], Step [200/257], Loss: 0.1293114870786667\n","Epoch [130/256], Step [100/257], Loss: 0.04489344730973244\n","Epoch [130/256], Step [200/257], Loss: 0.1519438624382019\n","Epoch [131/256], Step [100/257], Loss: 0.006122926250100136\n","Epoch [131/256], Step [200/257], Loss: 0.024551937356591225\n","Epoch [132/256], Step [100/257], Loss: 0.011642876081168652\n","Epoch [132/256], Step [200/257], Loss: 0.12178276479244232\n","Epoch [133/256], Step [100/257], Loss: 0.005168803967535496\n","Epoch [133/256], Step [200/257], Loss: 0.09167350083589554\n","Epoch [134/256], Step [100/257], Loss: 0.05261452868580818\n","Epoch [134/256], Step [200/257], Loss: 0.024419957771897316\n","Epoch [135/256], Step [100/257], Loss: 0.030499614775180817\n","Epoch [135/256], Step [200/257], Loss: 0.010830176994204521\n","Epoch [136/256], Step [100/257], Loss: 0.008009485900402069\n","Epoch [136/256], Step [200/257], Loss: 0.0951528325676918\n","Epoch [137/256], Step [100/257], Loss: 0.19004976749420166\n","Epoch [137/256], Step [200/257], Loss: 0.04258877784013748\n","Epoch [138/256], Step [100/257], Loss: 0.10548436641693115\n","Epoch [138/256], Step [200/257], Loss: 0.06676815450191498\n","Epoch [139/256], Step [100/257], Loss: 0.06134320795536041\n","Epoch [139/256], Step [200/257], Loss: 0.023486189544200897\n","Epoch [140/256], Step [100/257], Loss: 0.01850680634379387\n","Epoch [140/256], Step [200/257], Loss: 0.037928372621536255\n","Epoch [141/256], Step [100/257], Loss: 0.01030498556792736\n","Epoch [141/256], Step [200/257], Loss: 0.06999615579843521\n","Epoch [142/256], Step [100/257], Loss: 0.056819848716259\n","Epoch [142/256], Step [200/257], Loss: 0.0647662878036499\n","Epoch [143/256], Step [100/257], Loss: 0.02712080627679825\n","Epoch [143/256], Step [200/257], Loss: 0.02747507579624653\n","Epoch [144/256], Step [100/257], Loss: 0.04751897603273392\n","Epoch [144/256], Step [200/257], Loss: 0.08359915763139725\n","Epoch [145/256], Step [100/257], Loss: 0.06673156470060349\n","Epoch [145/256], Step [200/257], Loss: 0.14070011675357819\n","Epoch [146/256], Step [100/257], Loss: 0.3726782500743866\n","Epoch [146/256], Step [200/257], Loss: 0.10674171149730682\n","Epoch [147/256], Step [100/257], Loss: 0.040960099548101425\n","Epoch [147/256], Step [200/257], Loss: 0.03798474371433258\n","Epoch [148/256], Step [100/257], Loss: 0.2168761044740677\n","Epoch [148/256], Step [200/257], Loss: 0.19406452775001526\n","Epoch [149/256], Step [100/257], Loss: 0.23686283826828003\n","Epoch [149/256], Step [200/257], Loss: 0.1621442288160324\n","Epoch [150/256], Step [100/257], Loss: 0.2971620559692383\n","Epoch [150/256], Step [200/257], Loss: 0.3011005222797394\n","Epoch [151/256], Step [100/257], Loss: 0.06405843794345856\n","Epoch [151/256], Step [200/257], Loss: 0.20495809614658356\n","Epoch [152/256], Step [100/257], Loss: 0.07471653819084167\n","Epoch [152/256], Step [200/257], Loss: 0.17438296973705292\n","Epoch [153/256], Step [100/257], Loss: 0.0959792360663414\n","Epoch [153/256], Step [200/257], Loss: 0.08974937349557877\n","Epoch [154/256], Step [100/257], Loss: 0.19494248926639557\n","Epoch [154/256], Step [200/257], Loss: 0.030934082344174385\n","Epoch [155/256], Step [100/257], Loss: 0.1889086365699768\n","Epoch [155/256], Step [200/257], Loss: 0.08693689107894897\n","Epoch [156/256], Step [100/257], Loss: 0.07322544604539871\n","Epoch [156/256], Step [200/257], Loss: 0.054064564406871796\n","Epoch [157/256], Step [100/257], Loss: 0.09999804943799973\n","Epoch [157/256], Step [200/257], Loss: 0.14919725060462952\n","Epoch [158/256], Step [100/257], Loss: 0.06183020770549774\n","Epoch [158/256], Step [200/257], Loss: 0.044119562953710556\n","Epoch [159/256], Step [100/257], Loss: 0.026219815015792847\n","Epoch [159/256], Step [200/257], Loss: 0.13049568235874176\n","Epoch [160/256], Step [100/257], Loss: 0.13636144995689392\n","Epoch [160/256], Step [200/257], Loss: 0.23604150116443634\n","Epoch [161/256], Step [100/257], Loss: 0.04551580175757408\n","Epoch [161/256], Step [200/257], Loss: 0.18773365020751953\n","Epoch [162/256], Step [100/257], Loss: 0.1697261482477188\n","Epoch [162/256], Step [200/257], Loss: 0.34963247179985046\n","Epoch [163/256], Step [100/257], Loss: 0.13061928749084473\n","Epoch [163/256], Step [200/257], Loss: 0.02869115211069584\n","Epoch [164/256], Step [100/257], Loss: 0.09421344101428986\n","Epoch [164/256], Step [200/257], Loss: 0.2179606705904007\n","Epoch [165/256], Step [100/257], Loss: 0.269768625497818\n","Epoch [165/256], Step [200/257], Loss: 0.05087190493941307\n","Epoch [166/256], Step [100/257], Loss: 0.0748230516910553\n","Epoch [166/256], Step [200/257], Loss: 0.19032782316207886\n","Epoch [167/256], Step [100/257], Loss: 0.26317116618156433\n","Epoch [167/256], Step [200/257], Loss: 0.06478674709796906\n","Epoch [168/256], Step [100/257], Loss: 0.07832502573728561\n","Epoch [168/256], Step [200/257], Loss: 0.08675937354564667\n","Epoch [169/256], Step [100/257], Loss: 0.19399231672286987\n","Epoch [169/256], Step [200/257], Loss: 0.04859456419944763\n","Epoch [170/256], Step [100/257], Loss: 0.06110590696334839\n","Epoch [170/256], Step [200/257], Loss: 0.03883080184459686\n","Epoch [171/256], Step [100/257], Loss: 0.2446332424879074\n","Epoch [171/256], Step [200/257], Loss: 0.09082099050283432\n","Epoch [172/256], Step [100/257], Loss: 0.08145047724246979\n","Epoch [172/256], Step [200/257], Loss: 0.11153107136487961\n","Epoch [173/256], Step [100/257], Loss: 0.039556678384542465\n","Epoch [173/256], Step [200/257], Loss: 0.37804871797561646\n","Epoch [174/256], Step [100/257], Loss: 0.23043498396873474\n","Epoch [174/256], Step [200/257], Loss: 0.11602738499641418\n","Epoch [175/256], Step [100/257], Loss: 0.1622440069913864\n","Epoch [175/256], Step [200/257], Loss: 0.03504619747400284\n","Epoch [176/256], Step [100/257], Loss: 0.10501017421483994\n","Epoch [176/256], Step [200/257], Loss: 0.2646116018295288\n","Epoch [177/256], Step [100/257], Loss: 0.19512125849723816\n","Epoch [177/256], Step [200/257], Loss: 0.09410901367664337\n","Epoch [178/256], Step [100/257], Loss: 0.07879120856523514\n","Epoch [178/256], Step [200/257], Loss: 0.15166111290454865\n","Epoch [179/256], Step [100/257], Loss: 0.07527481764554977\n","Epoch [179/256], Step [200/257], Loss: 0.026158956810832024\n","Epoch [180/256], Step [100/257], Loss: 0.3231463134288788\n","Epoch [180/256], Step [200/257], Loss: 0.2569812536239624\n","Epoch [181/256], Step [100/257], Loss: 0.04351334646344185\n","Epoch [181/256], Step [200/257], Loss: 0.12023240327835083\n","Epoch [182/256], Step [100/257], Loss: 0.26782628893852234\n","Epoch [182/256], Step [200/257], Loss: 0.20750275254249573\n","Epoch [183/256], Step [100/257], Loss: 0.1336432695388794\n","Epoch [183/256], Step [200/257], Loss: 0.008719201199710369\n","Epoch [184/256], Step [100/257], Loss: 0.07968907058238983\n","Epoch [184/256], Step [200/257], Loss: 0.14775411784648895\n","Epoch [185/256], Step [100/257], Loss: 0.06313112378120422\n","Epoch [185/256], Step [200/257], Loss: 0.18853192031383514\n","Epoch [186/256], Step [100/257], Loss: 0.2697157859802246\n","Epoch [186/256], Step [200/257], Loss: 0.12244515866041183\n","Epoch [187/256], Step [100/257], Loss: 0.0271739661693573\n","Epoch [187/256], Step [200/257], Loss: 0.2827189564704895\n","Epoch [188/256], Step [100/257], Loss: 0.11132604628801346\n","Epoch [188/256], Step [200/257], Loss: 0.1429780125617981\n","Epoch [189/256], Step [100/257], Loss: 0.03426051512360573\n","Epoch [189/256], Step [200/257], Loss: 0.03054613061249256\n","Epoch [190/256], Step [100/257], Loss: 0.13807158172130585\n","Epoch [190/256], Step [200/257], Loss: 0.12348733842372894\n","Epoch [191/256], Step [100/257], Loss: 0.05452847480773926\n","Epoch [191/256], Step [200/257], Loss: 0.0955633595585823\n","Epoch [192/256], Step [100/257], Loss: 0.16119742393493652\n","Epoch [192/256], Step [200/257], Loss: 0.08601064234972\n","Epoch [193/256], Step [100/257], Loss: 0.15275926887989044\n","Epoch [193/256], Step [200/257], Loss: 0.17456045746803284\n","Epoch [194/256], Step [100/257], Loss: 0.19901345670223236\n","Epoch [194/256], Step [200/257], Loss: 0.05240822210907936\n","Epoch [195/256], Step [100/257], Loss: 0.06849312037229538\n","Epoch [195/256], Step [200/257], Loss: 0.1279865801334381\n","Epoch [196/256], Step [100/257], Loss: 0.12626217305660248\n","Epoch [196/256], Step [200/257], Loss: 0.04714248329401016\n","Epoch [197/256], Step [100/257], Loss: 0.05625968798995018\n","Epoch [197/256], Step [200/257], Loss: 0.025500144809484482\n","Epoch [198/256], Step [100/257], Loss: 0.2712711691856384\n","Epoch [198/256], Step [200/257], Loss: 0.05910741165280342\n","Epoch [199/256], Step [100/257], Loss: 0.24610400199890137\n","Epoch [199/256], Step [200/257], Loss: 0.02653542347252369\n","Epoch [200/256], Step [100/257], Loss: 0.03336276486515999\n","Epoch [200/256], Step [200/257], Loss: 0.010541820898652077\n","Epoch [201/256], Step [100/257], Loss: 0.05976192653179169\n","Epoch [201/256], Step [200/257], Loss: 0.248580664396286\n","Epoch [202/256], Step [100/257], Loss: 0.12935498356819153\n","Epoch [202/256], Step [200/257], Loss: 0.1878696233034134\n","Epoch [203/256], Step [100/257], Loss: 0.048321906477212906\n","Epoch [203/256], Step [200/257], Loss: 0.12441907823085785\n","Epoch [204/256], Step [100/257], Loss: 0.08434164524078369\n","Epoch [204/256], Step [200/257], Loss: 0.05475906655192375\n","Epoch [205/256], Step [100/257], Loss: 0.07392686605453491\n","Epoch [205/256], Step [200/257], Loss: 0.07750316709280014\n","Epoch [206/256], Step [100/257], Loss: 0.391020804643631\n","Epoch [206/256], Step [200/257], Loss: 0.21794384717941284\n","Epoch [207/256], Step [100/257], Loss: 0.29703623056411743\n","Epoch [207/256], Step [200/257], Loss: 0.12264309078454971\n","Epoch [208/256], Step [100/257], Loss: 0.1569705605506897\n","Epoch [208/256], Step [200/257], Loss: 0.0544244647026062\n","Epoch [209/256], Step [100/257], Loss: 0.024021591991186142\n","Epoch [209/256], Step [200/257], Loss: 0.11407361924648285\n","Epoch [210/256], Step [100/257], Loss: 0.01790526509284973\n","Epoch [210/256], Step [200/257], Loss: 0.19996921718120575\n","Epoch [211/256], Step [100/257], Loss: 0.026896199211478233\n","Epoch [211/256], Step [200/257], Loss: 0.08303049951791763\n","Epoch [212/256], Step [100/257], Loss: 0.009185375645756721\n","Epoch [212/256], Step [200/257], Loss: 0.14864476025104523\n","Epoch [213/256], Step [100/257], Loss: 0.035782456398010254\n","Epoch [213/256], Step [200/257], Loss: 0.09826850891113281\n","Epoch [214/256], Step [100/257], Loss: 0.17443357408046722\n","Epoch [214/256], Step [200/257], Loss: 0.13356590270996094\n","Epoch [215/256], Step [100/257], Loss: 0.19519570469856262\n","Epoch [215/256], Step [200/257], Loss: 0.048587869852781296\n","Epoch [216/256], Step [100/257], Loss: 0.20511487126350403\n","Epoch [216/256], Step [200/257], Loss: 0.10152735561132431\n","Epoch [217/256], Step [100/257], Loss: 0.059034157544374466\n","Epoch [217/256], Step [200/257], Loss: 0.23428453505039215\n","Epoch [218/256], Step [100/257], Loss: 0.11397497355937958\n","Epoch [218/256], Step [200/257], Loss: 0.11448457092046738\n","Epoch [219/256], Step [100/257], Loss: 0.19930417835712433\n","Epoch [219/256], Step [200/257], Loss: 0.0701754093170166\n","Epoch [220/256], Step [100/257], Loss: 0.14168180525302887\n","Epoch [220/256], Step [200/257], Loss: 0.2728935778141022\n","Epoch [221/256], Step [100/257], Loss: 0.028241991996765137\n","Epoch [221/256], Step [200/257], Loss: 0.15191827714443207\n","Epoch [222/256], Step [100/257], Loss: 0.039767343550920486\n","Epoch [222/256], Step [200/257], Loss: 0.3865117132663727\n","Epoch [223/256], Step [100/257], Loss: 0.07459872961044312\n","Epoch [223/256], Step [200/257], Loss: 0.05309498310089111\n","Epoch [224/256], Step [100/257], Loss: 0.036338675767183304\n","Epoch [224/256], Step [200/257], Loss: 0.06380470097064972\n","Epoch [225/256], Step [100/257], Loss: 0.15448610484600067\n","Epoch [225/256], Step [200/257], Loss: 0.19596368074417114\n","Epoch [226/256], Step [100/257], Loss: 0.11465781182050705\n","Epoch [226/256], Step [200/257], Loss: 0.1157442256808281\n","Epoch [227/256], Step [100/257], Loss: 0.08795515447854996\n","Epoch [227/256], Step [200/257], Loss: 0.10601236671209335\n","Epoch [228/256], Step [100/257], Loss: 0.07586578279733658\n","Epoch [228/256], Step [200/257], Loss: 0.07323065400123596\n","Epoch [229/256], Step [100/257], Loss: 0.1252867877483368\n","Epoch [229/256], Step [200/257], Loss: 0.10242021828889847\n","Epoch [230/256], Step [100/257], Loss: 0.10522471368312836\n","Epoch [230/256], Step [200/257], Loss: 0.16249726712703705\n","Epoch [231/256], Step [100/257], Loss: 0.1307038515806198\n","Epoch [231/256], Step [200/257], Loss: 0.13201352953910828\n","Epoch [232/256], Step [100/257], Loss: 0.1734573245048523\n","Epoch [232/256], Step [200/257], Loss: 0.06435520201921463\n","Epoch [233/256], Step [100/257], Loss: 0.1353798657655716\n","Epoch [233/256], Step [200/257], Loss: 0.08956515043973923\n","Epoch [234/256], Step [100/257], Loss: 0.33366161584854126\n","Epoch [234/256], Step [200/257], Loss: 0.12219825387001038\n","Epoch [235/256], Step [100/257], Loss: 0.12791340053081512\n","Epoch [235/256], Step [200/257], Loss: 0.09248829632997513\n","Epoch [236/256], Step [100/257], Loss: 0.1185026541352272\n","Epoch [236/256], Step [200/257], Loss: 0.040078841149806976\n","Epoch [237/256], Step [100/257], Loss: 0.04449255391955376\n","Epoch [237/256], Step [200/257], Loss: 0.3199269771575928\n","Epoch [238/256], Step [100/257], Loss: 0.1115560457110405\n","Epoch [238/256], Step [200/257], Loss: 0.038389358669519424\n","Epoch [239/256], Step [100/257], Loss: 0.10517507791519165\n","Epoch [239/256], Step [200/257], Loss: 0.06294147670269012\n","Epoch [240/256], Step [100/257], Loss: 0.23270297050476074\n","Epoch [240/256], Step [200/257], Loss: 0.13291536271572113\n","Epoch [241/256], Step [100/257], Loss: 0.05388103425502777\n","Epoch [241/256], Step [200/257], Loss: 0.22135931253433228\n","Epoch [242/256], Step [100/257], Loss: 0.06785207241773605\n","Epoch [242/256], Step [200/257], Loss: 0.10072598606348038\n","Epoch [243/256], Step [100/257], Loss: 0.06279557943344116\n","Epoch [243/256], Step [200/257], Loss: 0.08680646121501923\n","Epoch [244/256], Step [100/257], Loss: 0.32711464166641235\n","Epoch [244/256], Step [200/257], Loss: 0.03563844785094261\n","Epoch [245/256], Step [100/257], Loss: 0.1537400186061859\n","Epoch [245/256], Step [200/257], Loss: 0.1373070925474167\n","Epoch [246/256], Step [100/257], Loss: 0.10873322933912277\n","Epoch [246/256], Step [200/257], Loss: 0.22475995123386383\n","Epoch [247/256], Step [100/257], Loss: 0.08182923495769501\n","Epoch [247/256], Step [200/257], Loss: 0.12735021114349365\n","Epoch [248/256], Step [100/257], Loss: 0.16653798520565033\n","Epoch [248/256], Step [200/257], Loss: 0.04995671287178993\n","Epoch [249/256], Step [100/257], Loss: 0.29239505529403687\n","Epoch [249/256], Step [200/257], Loss: 0.06665211915969849\n","Epoch [250/256], Step [100/257], Loss: 0.04606785252690315\n","Epoch [250/256], Step [200/257], Loss: 0.29217004776000977\n","Epoch [251/256], Step [100/257], Loss: 0.057877276092767715\n","Epoch [251/256], Step [200/257], Loss: 0.05098040774464607\n","Epoch [252/256], Step [100/257], Loss: 0.12369903922080994\n","Epoch [252/256], Step [200/257], Loss: 0.016297072172164917\n","Epoch [253/256], Step [100/257], Loss: 0.08068957924842834\n","Epoch [253/256], Step [200/257], Loss: 0.06590184569358826\n","Epoch [254/256], Step [100/257], Loss: 0.17707419395446777\n","Epoch [254/256], Step [200/257], Loss: 0.4841039478778839\n","Epoch [255/256], Step [100/257], Loss: 0.04334639757871628\n","Epoch [255/256], Step [200/257], Loss: 0.07150579988956451\n","Epoch [256/256], Step [100/257], Loss: 0.17246250808238983\n","Epoch [256/256], Step [200/257], Loss: 0.13937166333198547\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bx78coqZaUQB","executionInfo":{"elapsed":15,"status":"ok","timestamp":1623302995924,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"64055c74-cc68-4263-c0e9-fe9645b52bec"},"source":["# Test the model\n","test_dataset = CustomDataset(X_0713_transformed, y_0713_transformed)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=5, shuffle=False)\n","\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (images, labels) in enumerate(test_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        if i==0:\n","          y_pre = predicted.tolist()\n","        else:\n","          y_pre.extend(predicted.tolist())\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","\n","torch.save(model.state_dict(), 'model.ckpt')\n","\n","target_names =  ['Nominal', 'Faulty']\n","print(classification_report(y_0713_transformed.tolist(), y_pre, target_names=target_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 80.0\n","Iteration:1,  Accuracy: 90.0\n","Iteration:2,  Accuracy: 93.33333333333333\n","Iteration:3,  Accuracy: 95.0\n","Iteration:4,  Accuracy: 92.0\n","Iteration:5,  Accuracy: 86.66666666666667\n","Iteration:6,  Accuracy: 88.57142857142857\n","Iteration:7,  Accuracy: 90.0\n","Iteration:8,  Accuracy: 91.11111111111111\n","Iteration:9,  Accuracy: 92.0\n","Iteration:10,  Accuracy: 92.72727272727273\n","Iteration:11,  Accuracy: 91.66666666666667\n","Iteration:12,  Accuracy: 92.3076923076923\n","Iteration:13,  Accuracy: 91.42857142857143\n","Iteration:14,  Accuracy: 90.66666666666667\n","Iteration:15,  Accuracy: 91.25\n","Iteration:16,  Accuracy: 91.76470588235294\n","Iteration:17,  Accuracy: 90.0\n","Iteration:18,  Accuracy: 88.42105263157895\n","Iteration:19,  Accuracy: 89.0\n","Iteration:20,  Accuracy: 88.57142857142857\n","Iteration:21,  Accuracy: 88.18181818181819\n","Iteration:22,  Accuracy: 88.69565217391305\n","Iteration:23,  Accuracy: 89.16666666666667\n","Iteration:24,  Accuracy: 88.8\n","Iteration:25,  Accuracy: 89.23076923076923\n","Iteration:26,  Accuracy: 88.88888888888889\n","Iteration:27,  Accuracy: 88.57142857142857\n","Iteration:28,  Accuracy: 88.27586206896552\n","Iteration:29,  Accuracy: 88.0\n","Iteration:30,  Accuracy: 88.38709677419355\n","Iteration:31,  Accuracy: 88.75\n","Iteration:32,  Accuracy: 88.48484848484848\n","Iteration:33,  Accuracy: 88.23529411764706\n","Iteration:34,  Accuracy: 87.42857142857143\n","Iteration:35,  Accuracy: 87.22222222222223\n","Iteration:36,  Accuracy: 87.02702702702703\n","Iteration:37,  Accuracy: 87.36842105263158\n","Iteration:38,  Accuracy: 86.15384615384616\n","Iteration:39,  Accuracy: 86.0\n","Iteration:40,  Accuracy: 85.36585365853658\n","Iteration:41,  Accuracy: 84.76190476190476\n","Iteration:42,  Accuracy: 84.18604651162791\n","Iteration:43,  Accuracy: 84.0909090909091\n","Iteration:44,  Accuracy: 84.44444444444444\n","Iteration:45,  Accuracy: 83.91304347826087\n","Iteration:46,  Accuracy: 83.40425531914893\n","Iteration:47,  Accuracy: 83.75\n","Iteration:48,  Accuracy: 84.08163265306122\n","Iteration:49,  Accuracy: 83.6\n","Iteration:50,  Accuracy: 83.92156862745098\n","Iteration:51,  Accuracy: 84.23076923076923\n","Iteration:52,  Accuracy: 84.52830188679245\n","Iteration:53,  Accuracy: 84.81481481481481\n","Iteration:54,  Accuracy: 84.72727272727273\n","Iteration:55,  Accuracy: 84.64285714285714\n","Iteration:56,  Accuracy: 84.91228070175438\n","Iteration:57,  Accuracy: 85.17241379310344\n","Iteration:58,  Accuracy: 84.7457627118644\n","Iteration:59,  Accuracy: 84.33333333333333\n","Iteration:60,  Accuracy: 84.26229508196721\n","Iteration:61,  Accuracy: 83.54838709677419\n","Iteration:62,  Accuracy: 83.4920634920635\n","Iteration:63,  Accuracy: 83.75\n","Iteration:64,  Accuracy: 83.38461538461539\n","Iteration:65,  Accuracy: 83.63636363636364\n","Iteration:66,  Accuracy: 82.98507462686567\n","Iteration:67,  Accuracy: 82.6470588235294\n","Iteration:68,  Accuracy: 82.02898550724638\n","Iteration:69,  Accuracy: 82.0\n","Iteration:70,  Accuracy: 82.25352112676056\n","Iteration:71,  Accuracy: 82.5\n","Iteration:72,  Accuracy: 82.73972602739725\n","Iteration:73,  Accuracy: 82.97297297297297\n","Iteration:74,  Accuracy: 82.66666666666667\n","Iteration:75,  Accuracy: 82.89473684210526\n","Iteration:76,  Accuracy: 83.11688311688312\n","Iteration:77,  Accuracy: 83.33333333333333\n","Iteration:78,  Accuracy: 83.0379746835443\n","Iteration:79,  Accuracy: 82.75\n","Iteration:80,  Accuracy: 82.71604938271605\n","Iteration:81,  Accuracy: 82.92682926829268\n","Iteration:82,  Accuracy: 81.92771084337349\n","Iteration:83,  Accuracy: 80.95238095238095\n","Iteration:84,  Accuracy: 80.94117647058823\n","Iteration:85,  Accuracy: 81.16279069767442\n","Iteration:86,  Accuracy: 81.14942528735632\n","Iteration:87,  Accuracy: 81.36363636363636\n","Iteration:88,  Accuracy: 80.67415730337079\n","Iteration:89,  Accuracy: 80.22222222222223\n","Test Accuracy of the model: 80.22222222222223 %\n","              precision    recall  f1-score   support\n","\n","     Nominal       0.94      0.75      0.83       293\n","      Faulty       0.66      0.90      0.76       157\n","\n","    accuracy                           0.80       450\n","   macro avg       0.80      0.83      0.80       450\n","weighted avg       0.84      0.80      0.81       450\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dkvl6-mCQ3hG"},"source":["### 3. 2020-07-13 데이터로만 학습 및 테스트<br>\n","##### 논문 결과) Acc 97%"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I6GtieAY5eZk","executionInfo":{"status":"ok","timestamp":1623814619002,"user_tz":-540,"elapsed":674,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"86d36654-89c4-4458-d924-fc8b4a0c1e86"},"source":["print(y_train_0713.shape)\n","print(y_test_0713.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(7189,)\n","(1798,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hocJvxmT4qFj","executionInfo":{"status":"ok","timestamp":1623815193656,"user_tz":-540,"elapsed":565925,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"8a5bb4bf-11a2-4055-e018-656dccefef7b"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 2\n","batch_size = 16\n","num_epochs = len(y_test_0713) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train_0713, y_train_0713)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/112], Step [100/450], Loss: 0.419612318277359\n","Epoch [1/112], Step [200/450], Loss: 0.02647334896028042\n","Epoch [1/112], Step [300/450], Loss: 0.06020445004105568\n","Epoch [1/112], Step [400/450], Loss: 0.26374879479408264\n","Epoch [2/112], Step [100/450], Loss: 0.12959034740924835\n","Epoch [2/112], Step [200/450], Loss: 0.015097755007445812\n","Epoch [2/112], Step [300/450], Loss: 0.1302727609872818\n","Epoch [2/112], Step [400/450], Loss: 0.18510979413986206\n","Epoch [3/112], Step [100/450], Loss: 0.3548528850078583\n","Epoch [3/112], Step [200/450], Loss: 0.23519019782543182\n","Epoch [3/112], Step [300/450], Loss: 0.2643558084964752\n","Epoch [3/112], Step [400/450], Loss: 0.08168578147888184\n","Epoch [4/112], Step [100/450], Loss: 0.16218599677085876\n","Epoch [4/112], Step [200/450], Loss: 0.018866531550884247\n","Epoch [4/112], Step [300/450], Loss: 0.007489549461752176\n","Epoch [4/112], Step [400/450], Loss: 0.09179055690765381\n","Epoch [5/112], Step [100/450], Loss: 0.02490401640534401\n","Epoch [5/112], Step [200/450], Loss: 0.0355212539434433\n","Epoch [5/112], Step [300/450], Loss: 0.008588537573814392\n","Epoch [5/112], Step [400/450], Loss: 0.10875479131937027\n","Epoch [6/112], Step [100/450], Loss: 0.04769865795969963\n","Epoch [6/112], Step [200/450], Loss: 0.005196929443627596\n","Epoch [6/112], Step [300/450], Loss: 0.00927738007158041\n","Epoch [6/112], Step [400/450], Loss: 0.04273693636059761\n","Epoch [7/112], Step [100/450], Loss: 0.027533411979675293\n","Epoch [7/112], Step [200/450], Loss: 0.03345721215009689\n","Epoch [7/112], Step [300/450], Loss: 0.15151238441467285\n","Epoch [7/112], Step [400/450], Loss: 0.012569495476782322\n","Epoch [8/112], Step [100/450], Loss: 0.09376226365566254\n","Epoch [8/112], Step [200/450], Loss: 0.0017458037473261356\n","Epoch [8/112], Step [300/450], Loss: 0.00637183990329504\n","Epoch [8/112], Step [400/450], Loss: 0.143539160490036\n","Epoch [9/112], Step [100/450], Loss: 0.32412800192832947\n","Epoch [9/112], Step [200/450], Loss: 0.004509942140430212\n","Epoch [9/112], Step [300/450], Loss: 0.04034605994820595\n","Epoch [9/112], Step [400/450], Loss: 0.4728078544139862\n","Epoch [10/112], Step [100/450], Loss: 0.014895576052367687\n","Epoch [10/112], Step [200/450], Loss: 0.003254943061619997\n","Epoch [10/112], Step [300/450], Loss: 0.0004586910654325038\n","Epoch [10/112], Step [400/450], Loss: 0.013645092025399208\n","Epoch [11/112], Step [100/450], Loss: 0.0024890366476029158\n","Epoch [11/112], Step [200/450], Loss: 0.0009153431747108698\n","Epoch [11/112], Step [300/450], Loss: 0.00032692335662432015\n","Epoch [11/112], Step [400/450], Loss: 0.004460284486413002\n","Epoch [12/112], Step [100/450], Loss: 0.0011044723214581609\n","Epoch [12/112], Step [200/450], Loss: 0.0012246632250025868\n","Epoch [12/112], Step [300/450], Loss: 0.0007079753559082747\n","Epoch [12/112], Step [400/450], Loss: 0.00019140324729960412\n","Epoch [13/112], Step [100/450], Loss: 0.001320352777838707\n","Epoch [13/112], Step [200/450], Loss: 0.0006735606584697962\n","Epoch [13/112], Step [300/450], Loss: 0.0058869486674666405\n","Epoch [13/112], Step [400/450], Loss: 0.014788180589675903\n","Epoch [14/112], Step [100/450], Loss: 0.11342272162437439\n","Epoch [14/112], Step [200/450], Loss: 0.008368199691176414\n","Epoch [14/112], Step [300/450], Loss: 0.28131332993507385\n","Epoch [14/112], Step [400/450], Loss: 0.001290329615585506\n","Epoch [15/112], Step [100/450], Loss: 0.0218135267496109\n","Epoch [15/112], Step [200/450], Loss: 0.0013358891010284424\n","Epoch [15/112], Step [300/450], Loss: 0.21379093825817108\n","Epoch [15/112], Step [400/450], Loss: 0.006395591422915459\n","Epoch [16/112], Step [100/450], Loss: 0.007488096132874489\n","Epoch [16/112], Step [200/450], Loss: 0.23291637003421783\n","Epoch [16/112], Step [300/450], Loss: 0.008210213854908943\n","Epoch [16/112], Step [400/450], Loss: 0.047821588814258575\n","Epoch [17/112], Step [100/450], Loss: 0.010459868237376213\n","Epoch [17/112], Step [200/450], Loss: 0.001070612226612866\n","Epoch [17/112], Step [300/450], Loss: 0.002262667752802372\n","Epoch [17/112], Step [400/450], Loss: 0.00019424101628828794\n","Epoch [18/112], Step [100/450], Loss: 0.006443391554057598\n","Epoch [18/112], Step [200/450], Loss: 0.1011510118842125\n","Epoch [18/112], Step [300/450], Loss: 0.0006790628540329635\n","Epoch [18/112], Step [400/450], Loss: 0.005264442414045334\n","Epoch [19/112], Step [100/450], Loss: 0.0009392418432980776\n","Epoch [19/112], Step [200/450], Loss: 0.004554208368062973\n","Epoch [19/112], Step [300/450], Loss: 0.003026662627235055\n","Epoch [19/112], Step [400/450], Loss: 0.0004130782908760011\n","Epoch [20/112], Step [100/450], Loss: 0.030032185837626457\n","Epoch [20/112], Step [200/450], Loss: 0.007011588662862778\n","Epoch [20/112], Step [300/450], Loss: 0.0020050592720508575\n","Epoch [20/112], Step [400/450], Loss: 0.001264540129341185\n","Epoch [21/112], Step [100/450], Loss: 0.0003460956795606762\n","Epoch [21/112], Step [200/450], Loss: 0.0025635198689997196\n","Epoch [21/112], Step [300/450], Loss: 0.0037916949950158596\n","Epoch [21/112], Step [400/450], Loss: 0.0011324308579787612\n","Epoch [22/112], Step [100/450], Loss: 0.0003493386029731482\n","Epoch [22/112], Step [200/450], Loss: 0.001958895241841674\n","Epoch [22/112], Step [300/450], Loss: 0.0016801822930574417\n","Epoch [22/112], Step [400/450], Loss: 0.0036818746011704206\n","Epoch [23/112], Step [100/450], Loss: 0.00201354525052011\n","Epoch [23/112], Step [200/450], Loss: 0.0009884017053991556\n","Epoch [23/112], Step [300/450], Loss: 0.0034358978737145662\n","Epoch [23/112], Step [400/450], Loss: 0.0166903268545866\n","Epoch [24/112], Step [100/450], Loss: 0.002424676204100251\n","Epoch [24/112], Step [200/450], Loss: 0.2703075110912323\n","Epoch [24/112], Step [300/450], Loss: 0.0023054659832268953\n","Epoch [24/112], Step [400/450], Loss: 0.0010959906503558159\n","Epoch [25/112], Step [100/450], Loss: 0.0014214186230674386\n","Epoch [25/112], Step [200/450], Loss: 0.032829418778419495\n","Epoch [25/112], Step [300/450], Loss: 0.00025233914493583143\n","Epoch [25/112], Step [400/450], Loss: 0.04490378499031067\n","Epoch [26/112], Step [100/450], Loss: 0.0032706549391150475\n","Epoch [26/112], Step [200/450], Loss: 0.0005841055535711348\n","Epoch [26/112], Step [300/450], Loss: 0.0022164282854646444\n","Epoch [26/112], Step [400/450], Loss: 0.0001124874543165788\n","Epoch [27/112], Step [100/450], Loss: 8.716370939509943e-05\n","Epoch [27/112], Step [200/450], Loss: 0.003460470587015152\n","Epoch [27/112], Step [300/450], Loss: 0.013678716495633125\n","Epoch [27/112], Step [400/450], Loss: 0.00041421689093112946\n","Epoch [28/112], Step [100/450], Loss: 0.2620968818664551\n","Epoch [28/112], Step [200/450], Loss: 0.0008804560056887567\n","Epoch [28/112], Step [300/450], Loss: 0.0022554548922926188\n","Epoch [28/112], Step [400/450], Loss: 0.020436177030205727\n","Epoch [29/112], Step [100/450], Loss: 0.0021223316434770823\n","Epoch [29/112], Step [200/450], Loss: 0.0015547998482361436\n","Epoch [29/112], Step [300/450], Loss: 0.026321709156036377\n","Epoch [29/112], Step [400/450], Loss: 0.00011625338811427355\n","Epoch [30/112], Step [100/450], Loss: 0.0008865788113325834\n","Epoch [30/112], Step [200/450], Loss: 0.0013347358908504248\n","Epoch [30/112], Step [300/450], Loss: 0.00013076557661406696\n","Epoch [30/112], Step [400/450], Loss: 0.017533008009195328\n","Epoch [31/112], Step [100/450], Loss: 0.00021627976093441248\n","Epoch [31/112], Step [200/450], Loss: 0.028060557320713997\n","Epoch [31/112], Step [300/450], Loss: 4.691932917921804e-05\n","Epoch [31/112], Step [400/450], Loss: 0.00011968992475885898\n","Epoch [32/112], Step [100/450], Loss: 0.01585863158106804\n","Epoch [32/112], Step [200/450], Loss: 0.0021238878834992647\n","Epoch [32/112], Step [300/450], Loss: 0.00023015531769488007\n","Epoch [32/112], Step [400/450], Loss: 0.007710201665759087\n","Epoch [33/112], Step [100/450], Loss: 0.00047072654706425965\n","Epoch [33/112], Step [200/450], Loss: 0.004417013842612505\n","Epoch [33/112], Step [300/450], Loss: 0.0013027575332671404\n","Epoch [33/112], Step [400/450], Loss: 0.0012443504529073834\n","Epoch [34/112], Step [100/450], Loss: 0.0011799971107393503\n","Epoch [34/112], Step [200/450], Loss: 0.0009107335936278105\n","Epoch [34/112], Step [300/450], Loss: 0.0016239609103649855\n","Epoch [34/112], Step [400/450], Loss: 0.0010233365464955568\n","Epoch [35/112], Step [100/450], Loss: 0.00016113597666844726\n","Epoch [35/112], Step [200/450], Loss: 0.004042420070618391\n","Epoch [35/112], Step [300/450], Loss: 0.0001315202098339796\n","Epoch [35/112], Step [400/450], Loss: 0.0015201837522909045\n","Epoch [36/112], Step [100/450], Loss: 0.001011289656162262\n","Epoch [36/112], Step [200/450], Loss: 0.003372718347236514\n","Epoch [36/112], Step [300/450], Loss: 0.00024587326333858073\n","Epoch [36/112], Step [400/450], Loss: 0.0011665716301649809\n","Epoch [37/112], Step [100/450], Loss: 0.00010098446364281699\n","Epoch [37/112], Step [200/450], Loss: 0.0018395768711343408\n","Epoch [37/112], Step [300/450], Loss: 0.00031780905555933714\n","Epoch [37/112], Step [400/450], Loss: 0.0005866214632987976\n","Epoch [38/112], Step [100/450], Loss: 0.00038827763637527823\n","Epoch [38/112], Step [200/450], Loss: 7.68598256399855e-05\n","Epoch [38/112], Step [300/450], Loss: 8.206637721741572e-05\n","Epoch [38/112], Step [400/450], Loss: 5.7230270613217726e-05\n","Epoch [39/112], Step [100/450], Loss: 0.00012018950656056404\n","Epoch [39/112], Step [200/450], Loss: 0.0003063080657739192\n","Epoch [39/112], Step [300/450], Loss: 0.0005139696295373142\n","Epoch [39/112], Step [400/450], Loss: 0.0030707514379173517\n","Epoch [40/112], Step [100/450], Loss: 0.022831283509731293\n","Epoch [40/112], Step [200/450], Loss: 7.500404899474233e-05\n","Epoch [40/112], Step [300/450], Loss: 0.0002504958538338542\n","Epoch [40/112], Step [400/450], Loss: 0.0002944362349808216\n","Epoch [41/112], Step [100/450], Loss: 0.0028916189912706614\n","Epoch [41/112], Step [200/450], Loss: 0.07499075680971146\n","Epoch [41/112], Step [300/450], Loss: 0.00033589653321541846\n","Epoch [41/112], Step [400/450], Loss: 0.0015541267348453403\n","Epoch [42/112], Step [100/450], Loss: 0.011483416892588139\n","Epoch [42/112], Step [200/450], Loss: 0.0018011912470683455\n","Epoch [42/112], Step [300/450], Loss: 0.0031836661510169506\n","Epoch [42/112], Step [400/450], Loss: 0.00026574230287224054\n","Epoch [43/112], Step [100/450], Loss: 0.0005549858324229717\n","Epoch [43/112], Step [200/450], Loss: 0.0008260324830189347\n","Epoch [43/112], Step [300/450], Loss: 0.0005530319176614285\n","Epoch [43/112], Step [400/450], Loss: 0.00016262056306004524\n","Epoch [44/112], Step [100/450], Loss: 0.001224111532792449\n","Epoch [44/112], Step [200/450], Loss: 9.450090874452144e-05\n","Epoch [44/112], Step [300/450], Loss: 0.006457777693867683\n","Epoch [44/112], Step [400/450], Loss: 0.00046723015839233994\n","Epoch [45/112], Step [100/450], Loss: 0.0002507189637981355\n","Epoch [45/112], Step [200/450], Loss: 0.0005273241549730301\n","Epoch [45/112], Step [300/450], Loss: 0.00014656304847449064\n","Epoch [45/112], Step [400/450], Loss: 0.000197875066078268\n","Epoch [46/112], Step [100/450], Loss: 0.003144408343359828\n","Epoch [46/112], Step [200/450], Loss: 0.00013383608893491328\n","Epoch [46/112], Step [300/450], Loss: 0.00024640565970912576\n","Epoch [46/112], Step [400/450], Loss: 0.00012479773431550711\n","Epoch [47/112], Step [100/450], Loss: 0.0007832383271306753\n","Epoch [47/112], Step [200/450], Loss: 0.00023457598581444472\n","Epoch [47/112], Step [300/450], Loss: 0.0001610892213648185\n","Epoch [47/112], Step [400/450], Loss: 0.0003498326404951513\n","Epoch [48/112], Step [100/450], Loss: 0.00036478746915236115\n","Epoch [48/112], Step [200/450], Loss: 0.00011165750038344413\n","Epoch [48/112], Step [300/450], Loss: 1.7709524399833754e-05\n","Epoch [48/112], Step [400/450], Loss: 0.0008447326254099607\n","Epoch [49/112], Step [100/450], Loss: 0.2981870472431183\n","Epoch [49/112], Step [200/450], Loss: 0.000382877275114879\n","Epoch [49/112], Step [300/450], Loss: 0.008252390660345554\n","Epoch [49/112], Step [400/450], Loss: 0.0536087304353714\n","Epoch [50/112], Step [100/450], Loss: 0.0010076634353026748\n","Epoch [50/112], Step [200/450], Loss: 0.5740517973899841\n","Epoch [50/112], Step [300/450], Loss: 0.0003208329144399613\n","Epoch [50/112], Step [400/450], Loss: 0.0011062436969950795\n","Epoch [51/112], Step [100/450], Loss: 0.00021038307750131935\n","Epoch [51/112], Step [200/450], Loss: 0.00018061559239868075\n","Epoch [51/112], Step [300/450], Loss: 0.0025550315622240305\n","Epoch [51/112], Step [400/450], Loss: 0.0008295479346998036\n","Epoch [52/112], Step [100/450], Loss: 0.10899368673563004\n","Epoch [52/112], Step [200/450], Loss: 0.0003343620046507567\n","Epoch [52/112], Step [300/450], Loss: 0.00018919545982498676\n","Epoch [52/112], Step [400/450], Loss: 0.00015597738092765212\n","Epoch [53/112], Step [100/450], Loss: 0.00044406973756849766\n","Epoch [53/112], Step [200/450], Loss: 0.00026492762845009565\n","Epoch [53/112], Step [300/450], Loss: 0.00015537509170826524\n","Epoch [53/112], Step [400/450], Loss: 6.568033859366551e-05\n","Epoch [54/112], Step [100/450], Loss: 0.0002718636824283749\n","Epoch [54/112], Step [200/450], Loss: 7.830119284335524e-05\n","Epoch [54/112], Step [300/450], Loss: 3.0186642106855288e-05\n","Epoch [54/112], Step [400/450], Loss: 0.0002338820049772039\n","Epoch [55/112], Step [100/450], Loss: 2.50465891440399e-05\n","Epoch [55/112], Step [200/450], Loss: 0.1139412671327591\n","Epoch [55/112], Step [300/450], Loss: 3.0195926228770986e-05\n","Epoch [55/112], Step [400/450], Loss: 0.0007197721861302853\n","Epoch [56/112], Step [100/450], Loss: 0.00019589388102758676\n","Epoch [56/112], Step [200/450], Loss: 8.046496077440679e-06\n","Epoch [56/112], Step [300/450], Loss: 0.00010761847079265863\n","Epoch [56/112], Step [400/450], Loss: 0.06592085212469101\n","Epoch [57/112], Step [100/450], Loss: 0.0001816676085582003\n","Epoch [57/112], Step [200/450], Loss: 0.0006435897666960955\n","Epoch [57/112], Step [300/450], Loss: 0.0340241976082325\n","Epoch [57/112], Step [400/450], Loss: 0.00020132909412495792\n","Epoch [58/112], Step [100/450], Loss: 0.000836738501675427\n","Epoch [58/112], Step [200/450], Loss: 0.00011313437425997108\n","Epoch [58/112], Step [300/450], Loss: 0.000740690273232758\n","Epoch [58/112], Step [400/450], Loss: 0.0008937491802498698\n","Epoch [59/112], Step [100/450], Loss: 0.0022371152881532907\n","Epoch [59/112], Step [200/450], Loss: 0.0002493494830559939\n","Epoch [59/112], Step [300/450], Loss: 0.0010199848329648376\n","Epoch [59/112], Step [400/450], Loss: 0.0009366804151795805\n","Epoch [60/112], Step [100/450], Loss: 0.0005684680072590709\n","Epoch [60/112], Step [200/450], Loss: 0.0007213365752249956\n","Epoch [60/112], Step [300/450], Loss: 0.0010093911550939083\n","Epoch [60/112], Step [400/450], Loss: 0.0035528684966266155\n","Epoch [61/112], Step [100/450], Loss: 0.00015051754598971456\n","Epoch [61/112], Step [200/450], Loss: 0.0005667791701853275\n","Epoch [61/112], Step [300/450], Loss: 7.393192936433479e-05\n","Epoch [61/112], Step [400/450], Loss: 0.0014000998344272375\n","Epoch [62/112], Step [100/450], Loss: 0.00031216838397085667\n","Epoch [62/112], Step [200/450], Loss: 0.001272021559998393\n","Epoch [62/112], Step [300/450], Loss: 6.419262354029343e-05\n","Epoch [62/112], Step [400/450], Loss: 0.00019983471429441124\n","Epoch [63/112], Step [100/450], Loss: 0.000397940311813727\n","Epoch [63/112], Step [200/450], Loss: 0.0007122189272195101\n","Epoch [63/112], Step [300/450], Loss: 0.0008239507442340255\n","Epoch [63/112], Step [400/450], Loss: 0.0005077829118818045\n","Epoch [64/112], Step [100/450], Loss: 0.00010832925181603059\n","Epoch [64/112], Step [200/450], Loss: 9.988213423639536e-05\n","Epoch [64/112], Step [300/450], Loss: 0.004693166818469763\n","Epoch [64/112], Step [400/450], Loss: 9.414786472916603e-05\n","Epoch [65/112], Step [100/450], Loss: 0.00013104101526550949\n","Epoch [65/112], Step [200/450], Loss: 0.00040616828482598066\n","Epoch [65/112], Step [300/450], Loss: 3.813257353613153e-05\n","Epoch [65/112], Step [400/450], Loss: 4.910368807031773e-05\n","Epoch [66/112], Step [100/450], Loss: 0.00017531360208522528\n","Epoch [66/112], Step [200/450], Loss: 0.00023701332975178957\n","Epoch [66/112], Step [300/450], Loss: 0.001049575163051486\n","Epoch [66/112], Step [400/450], Loss: 0.00019632965268101543\n","Epoch [67/112], Step [100/450], Loss: 0.00011583123705349863\n","Epoch [67/112], Step [200/450], Loss: 0.00038086206768639386\n","Epoch [67/112], Step [300/450], Loss: 1.7269661839236505e-05\n","Epoch [67/112], Step [400/450], Loss: 0.002591921715065837\n","Epoch [68/112], Step [100/450], Loss: 0.0015442847507074475\n","Epoch [68/112], Step [200/450], Loss: 0.0008174687973223627\n","Epoch [68/112], Step [300/450], Loss: 4.519286812865175e-05\n","Epoch [68/112], Step [400/450], Loss: 1.6957083062152378e-05\n","Epoch [69/112], Step [100/450], Loss: 0.03343946114182472\n","Epoch [69/112], Step [200/450], Loss: 0.0002258561726193875\n","Epoch [69/112], Step [300/450], Loss: 0.00010518930503167212\n","Epoch [69/112], Step [400/450], Loss: 0.004159778822213411\n","Epoch [70/112], Step [100/450], Loss: 0.0024066816549748182\n","Epoch [70/112], Step [200/450], Loss: 0.0012826481834053993\n","Epoch [70/112], Step [300/450], Loss: 0.00033299881033599377\n","Epoch [70/112], Step [400/450], Loss: 0.0866326242685318\n","Epoch [71/112], Step [100/450], Loss: 0.004386805929243565\n","Epoch [71/112], Step [200/450], Loss: 0.0005453354679048061\n","Epoch [71/112], Step [300/450], Loss: 0.00030982185853645205\n","Epoch [71/112], Step [400/450], Loss: 0.002359055681154132\n","Epoch [72/112], Step [100/450], Loss: 0.0009741568937897682\n","Epoch [72/112], Step [200/450], Loss: 0.0002662041806615889\n","Epoch [72/112], Step [300/450], Loss: 0.0010911927092820406\n","Epoch [72/112], Step [400/450], Loss: 0.0010970600415021181\n","Epoch [73/112], Step [100/450], Loss: 0.01869864948093891\n","Epoch [73/112], Step [200/450], Loss: 0.006983715575188398\n","Epoch [73/112], Step [300/450], Loss: 0.005806064233183861\n","Epoch [73/112], Step [400/450], Loss: 0.00018948421347886324\n","Epoch [74/112], Step [100/450], Loss: 0.002202139236032963\n","Epoch [74/112], Step [200/450], Loss: 0.0005757134058512747\n","Epoch [74/112], Step [300/450], Loss: 0.011372322216629982\n","Epoch [74/112], Step [400/450], Loss: 0.0010120857041329145\n","Epoch [75/112], Step [100/450], Loss: 0.018298527225852013\n","Epoch [75/112], Step [200/450], Loss: 0.00026935519417747855\n","Epoch [75/112], Step [300/450], Loss: 0.0010475959861651063\n","Epoch [75/112], Step [400/450], Loss: 0.0012383759021759033\n","Epoch [76/112], Step [100/450], Loss: 0.011548276990652084\n","Epoch [76/112], Step [200/450], Loss: 0.0022394072730094194\n","Epoch [76/112], Step [300/450], Loss: 0.0023049400188028812\n","Epoch [76/112], Step [400/450], Loss: 0.004752463195472956\n","Epoch [77/112], Step [100/450], Loss: 0.00013920440687797964\n","Epoch [77/112], Step [200/450], Loss: 0.2675463855266571\n","Epoch [77/112], Step [300/450], Loss: 0.0003420308348722756\n","Epoch [77/112], Step [400/450], Loss: 0.00010774576367111877\n","Epoch [78/112], Step [100/450], Loss: 0.004016293212771416\n","Epoch [78/112], Step [200/450], Loss: 0.0003948261437471956\n","Epoch [78/112], Step [300/450], Loss: 0.014469983987510204\n","Epoch [78/112], Step [400/450], Loss: 0.014173688367009163\n","Epoch [79/112], Step [100/450], Loss: 5.9124198742210865e-05\n","Epoch [79/112], Step [200/450], Loss: 0.0004974048351868987\n","Epoch [79/112], Step [300/450], Loss: 2.887704613385722e-05\n","Epoch [79/112], Step [400/450], Loss: 0.005346676800400019\n","Epoch [80/112], Step [100/450], Loss: 0.0065506272949278355\n","Epoch [80/112], Step [200/450], Loss: 0.001137205515988171\n","Epoch [80/112], Step [300/450], Loss: 7.078589260345325e-05\n","Epoch [80/112], Step [400/450], Loss: 3.175504753016867e-05\n","Epoch [81/112], Step [100/450], Loss: 4.111630914849229e-05\n","Epoch [81/112], Step [200/450], Loss: 0.00020874016627203673\n","Epoch [81/112], Step [300/450], Loss: 9.385478188050911e-05\n","Epoch [81/112], Step [400/450], Loss: 5.570321445702575e-05\n","Epoch [82/112], Step [100/450], Loss: 0.008030922152101994\n","Epoch [82/112], Step [200/450], Loss: 0.0033113856334239244\n","Epoch [82/112], Step [300/450], Loss: 0.0035320944152772427\n","Epoch [82/112], Step [400/450], Loss: 0.0005138103733770549\n","Epoch [83/112], Step [100/450], Loss: 0.0034894829150289297\n","Epoch [83/112], Step [200/450], Loss: 0.0019692089408636093\n","Epoch [83/112], Step [300/450], Loss: 0.03185977414250374\n","Epoch [83/112], Step [400/450], Loss: 0.0002460008836351335\n","Epoch [84/112], Step [100/450], Loss: 0.00178387644700706\n","Epoch [84/112], Step [200/450], Loss: 0.0012537384172901511\n","Epoch [84/112], Step [300/450], Loss: 0.00016999630315694958\n","Epoch [84/112], Step [400/450], Loss: 0.0011377553455531597\n","Epoch [85/112], Step [100/450], Loss: 0.00041058045462705195\n","Epoch [85/112], Step [200/450], Loss: 0.02935701422393322\n","Epoch [85/112], Step [300/450], Loss: 0.016682161018252373\n","Epoch [85/112], Step [400/450], Loss: 0.007895751856267452\n","Epoch [86/112], Step [100/450], Loss: 0.0020939428359270096\n","Epoch [86/112], Step [200/450], Loss: 0.00019828809308819473\n","Epoch [86/112], Step [300/450], Loss: 0.00011636905401246622\n","Epoch [86/112], Step [400/450], Loss: 2.1263229427859187e-05\n","Epoch [87/112], Step [100/450], Loss: 0.00021684054809156805\n","Epoch [87/112], Step [200/450], Loss: 0.0001747543428791687\n","Epoch [87/112], Step [300/450], Loss: 1.162288640443876e-06\n","Epoch [87/112], Step [400/450], Loss: 4.21911317971535e-05\n","Epoch [88/112], Step [100/450], Loss: 0.0003925757482647896\n","Epoch [88/112], Step [200/450], Loss: 0.0028039307799190283\n","Epoch [88/112], Step [300/450], Loss: 0.15982820093631744\n","Epoch [88/112], Step [400/450], Loss: 4.069532951689325e-05\n","Epoch [89/112], Step [100/450], Loss: 7.952637679409236e-05\n","Epoch [89/112], Step [200/450], Loss: 4.52956446679309e-05\n","Epoch [89/112], Step [300/450], Loss: 1.7336269593215548e-05\n","Epoch [89/112], Step [400/450], Loss: 0.00042968636262230575\n","Epoch [90/112], Step [100/450], Loss: 5.167894414626062e-05\n","Epoch [90/112], Step [200/450], Loss: 0.0021339724771678448\n","Epoch [90/112], Step [300/450], Loss: 0.0017602317966520786\n","Epoch [90/112], Step [400/450], Loss: 0.007087142672389746\n","Epoch [91/112], Step [100/450], Loss: 0.17675146460533142\n","Epoch [91/112], Step [200/450], Loss: 0.2633577585220337\n","Epoch [91/112], Step [300/450], Loss: 0.04328633099794388\n","Epoch [91/112], Step [400/450], Loss: 0.01530243456363678\n","Epoch [92/112], Step [100/450], Loss: 0.1051764115691185\n","Epoch [92/112], Step [200/450], Loss: 0.0037071399856358767\n","Epoch [92/112], Step [300/450], Loss: 0.03497322276234627\n","Epoch [92/112], Step [400/450], Loss: 0.0005270414985716343\n","Epoch [93/112], Step [100/450], Loss: 0.0028658374212682247\n","Epoch [93/112], Step [200/450], Loss: 0.011052249930799007\n","Epoch [93/112], Step [300/450], Loss: 0.041039180010557175\n","Epoch [93/112], Step [400/450], Loss: 0.014123107306659222\n","Epoch [94/112], Step [100/450], Loss: 0.1337003856897354\n","Epoch [94/112], Step [200/450], Loss: 0.008449705317616463\n","Epoch [94/112], Step [300/450], Loss: 0.0037696650251746178\n","Epoch [94/112], Step [400/450], Loss: 0.0045019229874014854\n","Epoch [95/112], Step [100/450], Loss: 0.003730447730049491\n","Epoch [95/112], Step [200/450], Loss: 0.0008147123735398054\n","Epoch [95/112], Step [300/450], Loss: 0.04944925010204315\n","Epoch [95/112], Step [400/450], Loss: 0.0015256393235176802\n","Epoch [96/112], Step [100/450], Loss: 4.121190795558505e-05\n","Epoch [96/112], Step [200/450], Loss: 0.0025311855133622885\n","Epoch [96/112], Step [300/450], Loss: 0.11841602623462677\n","Epoch [96/112], Step [400/450], Loss: 0.000981215387582779\n","Epoch [97/112], Step [100/450], Loss: 0.0007746305782347918\n","Epoch [97/112], Step [200/450], Loss: 0.0012580471811816096\n","Epoch [97/112], Step [300/450], Loss: 0.005154219456017017\n","Epoch [97/112], Step [400/450], Loss: 0.0006200639763846993\n","Epoch [98/112], Step [100/450], Loss: 0.001023476361297071\n","Epoch [98/112], Step [200/450], Loss: 0.15363717079162598\n","Epoch [98/112], Step [300/450], Loss: 0.021257907152175903\n","Epoch [98/112], Step [400/450], Loss: 0.0016991073498502374\n","Epoch [99/112], Step [100/450], Loss: 0.0006193529698066413\n","Epoch [99/112], Step [200/450], Loss: 0.0005468672607094049\n","Epoch [99/112], Step [300/450], Loss: 0.01293137390166521\n","Epoch [99/112], Step [400/450], Loss: 0.00866107176989317\n","Epoch [100/112], Step [100/450], Loss: 0.007977801375091076\n","Epoch [100/112], Step [200/450], Loss: 0.00013805588241666555\n","Epoch [100/112], Step [300/450], Loss: 3.532661503413692e-05\n","Epoch [100/112], Step [400/450], Loss: 0.0014125369489192963\n","Epoch [101/112], Step [100/450], Loss: 0.010623385198414326\n","Epoch [101/112], Step [200/450], Loss: 0.0009588253451511264\n","Epoch [101/112], Step [300/450], Loss: 0.05726377293467522\n","Epoch [101/112], Step [400/450], Loss: 0.001060526818037033\n","Epoch [102/112], Step [100/450], Loss: 0.025477439165115356\n","Epoch [102/112], Step [200/450], Loss: 0.0495653934776783\n","Epoch [102/112], Step [300/450], Loss: 0.0005005826824344695\n","Epoch [102/112], Step [400/450], Loss: 0.15417613089084625\n","Epoch [103/112], Step [100/450], Loss: 0.00014570014900527894\n","Epoch [103/112], Step [200/450], Loss: 0.0013096394250169396\n","Epoch [103/112], Step [300/450], Loss: 0.00016392709221690893\n","Epoch [103/112], Step [400/450], Loss: 0.00021647100220434368\n","Epoch [104/112], Step [100/450], Loss: 0.001663927803747356\n","Epoch [104/112], Step [200/450], Loss: 0.00022172507306095213\n","Epoch [104/112], Step [300/450], Loss: 0.000331936520524323\n","Epoch [104/112], Step [400/450], Loss: 0.008534958586096764\n","Epoch [105/112], Step [100/450], Loss: 0.001077552093192935\n","Epoch [105/112], Step [200/450], Loss: 0.004724489059299231\n","Epoch [105/112], Step [300/450], Loss: 0.0001849598775152117\n","Epoch [105/112], Step [400/450], Loss: 0.0010330684017390013\n","Epoch [106/112], Step [100/450], Loss: 0.1585233211517334\n","Epoch [106/112], Step [200/450], Loss: 0.0030967022757977247\n","Epoch [106/112], Step [300/450], Loss: 0.014942099340260029\n","Epoch [106/112], Step [400/450], Loss: 0.0010510352440178394\n","Epoch [107/112], Step [100/450], Loss: 0.4344968795776367\n","Epoch [107/112], Step [200/450], Loss: 0.001250730361789465\n","Epoch [107/112], Step [300/450], Loss: 0.19963528215885162\n","Epoch [107/112], Step [400/450], Loss: 0.41378018260002136\n","Epoch [108/112], Step [100/450], Loss: 0.0016325063770636916\n","Epoch [108/112], Step [200/450], Loss: 0.013142669573426247\n","Epoch [108/112], Step [300/450], Loss: 0.0006302394322119653\n","Epoch [108/112], Step [400/450], Loss: 0.0025961673818528652\n","Epoch [109/112], Step [100/450], Loss: 0.43301042914390564\n","Epoch [109/112], Step [200/450], Loss: 0.0116495992988348\n","Epoch [109/112], Step [300/450], Loss: 0.0031361461151391268\n","Epoch [109/112], Step [400/450], Loss: 0.0003678547509480268\n","Epoch [110/112], Step [100/450], Loss: 0.0004450312990229577\n","Epoch [110/112], Step [200/450], Loss: 0.0015006939647719264\n","Epoch [110/112], Step [300/450], Loss: 0.0013175818603485823\n","Epoch [110/112], Step [400/450], Loss: 0.0006803977303206921\n","Epoch [111/112], Step [100/450], Loss: 0.0017259403830394149\n","Epoch [111/112], Step [200/450], Loss: 0.002519508358091116\n","Epoch [111/112], Step [300/450], Loss: 0.0015811125049367547\n","Epoch [111/112], Step [400/450], Loss: 0.0033146790228784084\n","Epoch [112/112], Step [100/450], Loss: 0.0008555392269045115\n","Epoch [112/112], Step [200/450], Loss: 0.001509369700215757\n","Epoch [112/112], Step [300/450], Loss: 0.2601998448371887\n","Epoch [112/112], Step [400/450], Loss: 0.0002954901137854904\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GRbS2vwdrRAG"},"source":["## Output 연산 소요 시간 측정까지 겸함"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5Lcjufq4qdL","executionInfo":{"status":"ok","timestamp":1623816655950,"user_tz":-540,"elapsed":1201,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"1940e6f0-6a28-4f5a-b37d-12251ef67181"},"source":["# Test the model\n","test_dataset = CustomDataset(X_test_0713, y_test_0713)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False) #batch size 원래 10이었음\n","\n","gpu_time_list=[]\n","gpu_time_list = np.array(gpu_time_list)\n","output_time_list=[]\n","output_time_list = np.array(output_time_list)\n","all_time_list=[]\n","all_time_list = np.array(all_time_list)\n","\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (data, labels) in enumerate(test_loader):\n","        time1 = time.time()\n","        data = data.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","        time2 = time.time()\n","        outputs = model(images)\n","        time3 = time.time()\n","        _, predicted = torch.max(outputs.data, 1)\n","        if i==0:\n","          y_pre_0713 = predicted.tolist()\n","        else:\n","          y_pre_0713.extend(predicted.tolist())\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","        gpu_time_list = np.append(gpu_time_list, time2-time1)\n","        output_time_list = np.append(output_time_list, time3-time2)\n","        all_time_list = np.append(all_time_list, time3-time1)\n","\n","    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n","    print(\"GPU에 올리는데 걸린 평균 시간:{}, model에 넣고 output 계산하는데 걸린 평균 시간:{}, count 제외 output 나오기까지 총 소요 시간 평균:{}\".format(gpu_time_list.mean(), output_time_list.mean(), all_time_list.mean()))\n","    print(\"GPU에 올리는데 걸린 max 시간:{}, model에 넣고 output 계산하는데 걸린 max 시간:{}, count 제외 output 나오기까지 총 소요 시간 max:{}\".format(gpu_time_list.max(), output_time_list.max(), all_time_list.max()))\n","    print(\"GPU에 올리는데 걸린 min 시간:{}, model에 넣고 output 계산하는데 걸린 min 시간:{}, count 제외 output 나오기까지 총 소요 시간 min:{}\".format(gpu_time_list.min(), output_time_list.min(), all_time_list.min()))\n","\n","# Save the model checkpoint\n","\n","torch.save(model.state_dict(), 'model.ckpt')\n","\n","target_names =  ['Nominal', 'Faulty']\n","print(classification_report(y_test_0713.tolist(), y_pre_0713, target_names=target_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 80.0\n","Iteration:1,  Accuracy: 90.0\n","Iteration:2,  Accuracy: 93.33333333333333\n","Iteration:3,  Accuracy: 95.0\n","Iteration:4,  Accuracy: 96.0\n","Iteration:5,  Accuracy: 96.66666666666667\n","Iteration:6,  Accuracy: 97.14285714285714\n","Iteration:7,  Accuracy: 97.5\n","Iteration:8,  Accuracy: 96.66666666666667\n","Iteration:9,  Accuracy: 97.0\n","Iteration:10,  Accuracy: 97.27272727272727\n","Iteration:11,  Accuracy: 97.5\n","Iteration:12,  Accuracy: 97.6923076923077\n","Iteration:13,  Accuracy: 97.85714285714286\n","Iteration:14,  Accuracy: 98.0\n","Iteration:15,  Accuracy: 98.125\n","Iteration:16,  Accuracy: 97.6470588235294\n","Iteration:17,  Accuracy: 97.77777777777777\n","Iteration:18,  Accuracy: 97.89473684210526\n","Iteration:19,  Accuracy: 98.0\n","Iteration:20,  Accuracy: 98.0952380952381\n","Iteration:21,  Accuracy: 98.18181818181819\n","Iteration:22,  Accuracy: 98.26086956521739\n","Iteration:23,  Accuracy: 98.33333333333333\n","Iteration:24,  Accuracy: 98.4\n","Iteration:25,  Accuracy: 98.46153846153847\n","Iteration:26,  Accuracy: 98.51851851851852\n","Iteration:27,  Accuracy: 98.57142857142857\n","Iteration:28,  Accuracy: 98.62068965517241\n","Iteration:29,  Accuracy: 98.66666666666667\n","Iteration:30,  Accuracy: 98.70967741935483\n","Iteration:31,  Accuracy: 98.75\n","Iteration:32,  Accuracy: 98.78787878787878\n","Iteration:33,  Accuracy: 98.82352941176471\n","Iteration:34,  Accuracy: 98.85714285714286\n","Iteration:35,  Accuracy: 98.88888888888889\n","Iteration:36,  Accuracy: 98.91891891891892\n","Iteration:37,  Accuracy: 98.6842105263158\n","Iteration:38,  Accuracy: 98.71794871794872\n","Iteration:39,  Accuracy: 98.75\n","Iteration:40,  Accuracy: 98.78048780487805\n","Iteration:41,  Accuracy: 98.80952380952381\n","Iteration:42,  Accuracy: 98.83720930232558\n","Iteration:43,  Accuracy: 98.86363636363636\n","Iteration:44,  Accuracy: 98.88888888888889\n","Iteration:45,  Accuracy: 98.91304347826087\n","Iteration:46,  Accuracy: 98.72340425531915\n","Iteration:47,  Accuracy: 98.75\n","Iteration:48,  Accuracy: 98.77551020408163\n","Iteration:49,  Accuracy: 98.8\n","Iteration:50,  Accuracy: 98.82352941176471\n","Iteration:51,  Accuracy: 98.84615384615384\n","Iteration:52,  Accuracy: 98.86792452830188\n","Iteration:53,  Accuracy: 98.88888888888889\n","Iteration:54,  Accuracy: 98.9090909090909\n","Iteration:55,  Accuracy: 98.92857142857143\n","Iteration:56,  Accuracy: 98.7719298245614\n","Iteration:57,  Accuracy: 98.79310344827586\n","Iteration:58,  Accuracy: 98.8135593220339\n","Iteration:59,  Accuracy: 98.83333333333333\n","Iteration:60,  Accuracy: 98.85245901639344\n","Iteration:61,  Accuracy: 98.87096774193549\n","Iteration:62,  Accuracy: 98.88888888888889\n","Iteration:63,  Accuracy: 98.90625\n","Iteration:64,  Accuracy: 98.92307692307692\n","Iteration:65,  Accuracy: 98.93939393939394\n","Iteration:66,  Accuracy: 98.95522388059702\n","Iteration:67,  Accuracy: 98.97058823529412\n","Iteration:68,  Accuracy: 98.98550724637681\n","Iteration:69,  Accuracy: 99.0\n","Iteration:70,  Accuracy: 99.01408450704226\n","Iteration:71,  Accuracy: 99.02777777777777\n","Iteration:72,  Accuracy: 99.04109589041096\n","Iteration:73,  Accuracy: 98.91891891891892\n","Iteration:74,  Accuracy: 98.8\n","Iteration:75,  Accuracy: 98.8157894736842\n","Iteration:76,  Accuracy: 98.83116883116882\n","Iteration:77,  Accuracy: 98.71794871794872\n","Iteration:78,  Accuracy: 98.73417721518987\n","Iteration:79,  Accuracy: 98.75\n","Iteration:80,  Accuracy: 98.76543209876543\n","Iteration:81,  Accuracy: 98.78048780487805\n","Iteration:82,  Accuracy: 98.79518072289157\n","Iteration:83,  Accuracy: 98.57142857142857\n","Iteration:84,  Accuracy: 98.58823529411765\n","Iteration:85,  Accuracy: 98.6046511627907\n","Iteration:86,  Accuracy: 98.62068965517241\n","Iteration:87,  Accuracy: 98.63636363636364\n","Iteration:88,  Accuracy: 98.65168539325843\n","Iteration:89,  Accuracy: 98.66666666666667\n","Iteration:90,  Accuracy: 98.68131868131869\n","Iteration:91,  Accuracy: 98.69565217391305\n","Iteration:92,  Accuracy: 98.60215053763442\n","Iteration:93,  Accuracy: 98.61702127659575\n","Iteration:94,  Accuracy: 98.63157894736842\n","Iteration:95,  Accuracy: 98.64583333333333\n","Iteration:96,  Accuracy: 98.65979381443299\n","Iteration:97,  Accuracy: 98.57142857142857\n","Iteration:98,  Accuracy: 98.58585858585859\n","Iteration:99,  Accuracy: 98.5\n","Iteration:100,  Accuracy: 98.51485148514851\n","Iteration:101,  Accuracy: 98.52941176470588\n","Iteration:102,  Accuracy: 98.54368932038835\n","Iteration:103,  Accuracy: 98.5576923076923\n","Iteration:104,  Accuracy: 98.57142857142857\n","Iteration:105,  Accuracy: 98.58490566037736\n","Iteration:106,  Accuracy: 98.59813084112149\n","Iteration:107,  Accuracy: 98.61111111111111\n","Iteration:108,  Accuracy: 98.62385321100918\n","Iteration:109,  Accuracy: 98.63636363636364\n","Iteration:110,  Accuracy: 98.64864864864865\n","Iteration:111,  Accuracy: 98.66071428571429\n","Iteration:112,  Accuracy: 98.67256637168141\n","Iteration:113,  Accuracy: 98.6842105263158\n","Iteration:114,  Accuracy: 98.69565217391305\n","Iteration:115,  Accuracy: 98.70689655172414\n","Iteration:116,  Accuracy: 98.71794871794872\n","Iteration:117,  Accuracy: 98.72881355932203\n","Iteration:118,  Accuracy: 98.73949579831933\n","Iteration:119,  Accuracy: 98.75\n","Iteration:120,  Accuracy: 98.7603305785124\n","Iteration:121,  Accuracy: 98.77049180327869\n","Iteration:122,  Accuracy: 98.78048780487805\n","Iteration:123,  Accuracy: 98.79032258064517\n","Iteration:124,  Accuracy: 98.8\n","Iteration:125,  Accuracy: 98.80952380952381\n","Iteration:126,  Accuracy: 98.81889763779527\n","Iteration:127,  Accuracy: 98.828125\n","Iteration:128,  Accuracy: 98.75968992248062\n","Iteration:129,  Accuracy: 98.76923076923077\n","Iteration:130,  Accuracy: 98.77862595419847\n","Iteration:131,  Accuracy: 98.78787878787878\n","Iteration:132,  Accuracy: 98.796992481203\n","Iteration:133,  Accuracy: 98.80597014925372\n","Iteration:134,  Accuracy: 98.81481481481481\n","Iteration:135,  Accuracy: 98.82352941176471\n","Iteration:136,  Accuracy: 98.83211678832117\n","Iteration:137,  Accuracy: 98.84057971014492\n","Iteration:138,  Accuracy: 98.84892086330935\n","Iteration:139,  Accuracy: 98.85714285714286\n","Iteration:140,  Accuracy: 98.86524822695036\n","Iteration:141,  Accuracy: 98.87323943661971\n","Iteration:142,  Accuracy: 98.88111888111888\n","Iteration:143,  Accuracy: 98.88888888888889\n","Iteration:144,  Accuracy: 98.89655172413794\n","Iteration:145,  Accuracy: 98.9041095890411\n","Iteration:146,  Accuracy: 98.91156462585035\n","Iteration:147,  Accuracy: 98.91891891891892\n","Iteration:148,  Accuracy: 98.85906040268456\n","Iteration:149,  Accuracy: 98.86666666666666\n","Iteration:150,  Accuracy: 98.87417218543047\n","Iteration:151,  Accuracy: 98.8157894736842\n","Iteration:152,  Accuracy: 98.82352941176471\n","Iteration:153,  Accuracy: 98.83116883116882\n","Iteration:154,  Accuracy: 98.83870967741936\n","Iteration:155,  Accuracy: 98.84615384615384\n","Iteration:156,  Accuracy: 98.85350318471338\n","Iteration:157,  Accuracy: 98.86075949367088\n","Iteration:158,  Accuracy: 98.86792452830188\n","Iteration:159,  Accuracy: 98.8125\n","Iteration:160,  Accuracy: 98.81987577639751\n","Iteration:161,  Accuracy: 98.82716049382717\n","Iteration:162,  Accuracy: 98.83435582822086\n","Iteration:163,  Accuracy: 98.84146341463415\n","Iteration:164,  Accuracy: 98.84848484848484\n","Iteration:165,  Accuracy: 98.855421686747\n","Iteration:166,  Accuracy: 98.8622754491018\n","Iteration:167,  Accuracy: 98.86904761904762\n","Iteration:168,  Accuracy: 98.87573964497041\n","Iteration:169,  Accuracy: 98.88235294117646\n","Iteration:170,  Accuracy: 98.88888888888889\n","Iteration:171,  Accuracy: 98.83720930232558\n","Iteration:172,  Accuracy: 98.78612716763006\n","Iteration:173,  Accuracy: 98.79310344827586\n","Iteration:174,  Accuracy: 98.8\n","Iteration:175,  Accuracy: 98.80681818181819\n","Iteration:176,  Accuracy: 98.8135593220339\n","Iteration:177,  Accuracy: 98.76404494382022\n","Iteration:178,  Accuracy: 98.77094972067039\n","Iteration:179,  Accuracy: 98.77641824249166\n","Test Accuracy of the model: 98.77641824249166 %\n","GPU에 올리는데 걸린 평균 시간:1.2482537163628472e-05, model에 넣고 output 계산하는데 걸린 평균 시간:0.0020899454752604167, count 제외 output 나오기까지 총 소요 시간 평균:0.0021024280124240452\n","GPU에 올리는데 걸린 max 시간:3.409385681152344e-05, model에 넣고 output 계산하는데 걸린 max 시간:0.0053558349609375, count 제외 output 나오기까지 총 소요 시간 max:0.005370616912841797\n","GPU에 올리는데 걸린 min 시간:1.0728836059570312e-05, model에 넣고 output 계산하는데 걸린 min 시간:0.0019466876983642578, count 제외 output 나오기까지 총 소요 시간 min:0.001959085464477539\n","              precision    recall  f1-score   support\n","\n","     Nominal       0.99      0.99      0.99      1169\n","      Faulty       0.98      0.98      0.98       629\n","\n","    accuracy                           0.99      1798\n","   macro avg       0.99      0.99      0.99      1798\n","weighted avg       0.99      0.99      0.99      1798\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sc7u7Cl9RDsF"},"source":["### 4. 13일 데이터를 학습한 모델로 12일 데이터 분류하기\n","##### 논문 결과) acc 87%"]},{"cell_type":"code","metadata":{"id":"wq7jhm89QH4r"},"source":["X_train_0713 = np.load(\"X_train_0713.npy\")\n","y_train_0713 = np.load(\"y_train_0713.npy\")\n","X_test_0713 = np.load(\"X_test_0713.npy\")\n","y_test_0713 = np.load(\"y_test_0713.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VpaM6OxOQga1"},"source":["X_train = np.concatenate(( X_train_0713, X_test_0713), axis=0)\n","y_train = np.concatenate((y_train_0713, y_test_0713), axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLXkN4xHQQ7L","executionInfo":{"elapsed":3,"status":"ok","timestamp":1623304504620,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"9860d391-5c0a-43e8-ee59-706b129c130d"},"source":["print(X_train.shape)\n","print(np.bincount(y_train))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(8987, 160)\n","[5842 3145]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hZxHfIen9ybM"},"source":["X_0712_transformed = np.load(\"X_0712_transformed.npy\")\n","y_0712_transformed = np.load(\"y_0712_transformed.npy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2N5kcJ99_RE","executionInfo":{"elapsed":871719,"status":"ok","timestamp":1623305420362,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"d9fdea69-56df-46f2-cf75-7132a2a4cb63"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 2\n","batch_size = 40\n","num_epochs = len(y_train) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train, y_train)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=40, shuffle=True)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/224], Step [100/225], Loss: 0.19490346312522888\n","Epoch [1/224], Step [200/225], Loss: 0.10253246128559113\n","Epoch [2/224], Step [100/225], Loss: 0.028521766886115074\n","Epoch [2/224], Step [200/225], Loss: 0.015614578500390053\n","Epoch [3/224], Step [100/225], Loss: 0.14765718579292297\n","Epoch [3/224], Step [200/225], Loss: 0.002417404670268297\n","Epoch [4/224], Step [100/225], Loss: 0.011181195266544819\n","Epoch [4/224], Step [200/225], Loss: 0.016713330522179604\n","Epoch [5/224], Step [100/225], Loss: 0.002504515927284956\n","Epoch [5/224], Step [200/225], Loss: 0.004899428226053715\n","Epoch [6/224], Step [100/225], Loss: 0.002659937832504511\n","Epoch [6/224], Step [200/225], Loss: 0.002493372419849038\n","Epoch [7/224], Step [100/225], Loss: 0.006979700177907944\n","Epoch [7/224], Step [200/225], Loss: 0.10469750314950943\n","Epoch [8/224], Step [100/225], Loss: 0.028265338391065598\n","Epoch [8/224], Step [200/225], Loss: 0.005444645881652832\n","Epoch [9/224], Step [100/225], Loss: 0.029413830488920212\n","Epoch [9/224], Step [200/225], Loss: 0.0025103995576500893\n","Epoch [10/224], Step [100/225], Loss: 0.0025701229460537434\n","Epoch [10/224], Step [200/225], Loss: 0.006409985013306141\n","Epoch [11/224], Step [100/225], Loss: 0.005833478178828955\n","Epoch [11/224], Step [200/225], Loss: 0.0017040751408785582\n","Epoch [12/224], Step [100/225], Loss: 0.0031128698028624058\n","Epoch [12/224], Step [200/225], Loss: 0.004200674593448639\n","Epoch [13/224], Step [100/225], Loss: 0.0032937624491751194\n","Epoch [13/224], Step [200/225], Loss: 0.12319739907979965\n","Epoch [14/224], Step [100/225], Loss: 0.0002744798839557916\n","Epoch [14/224], Step [200/225], Loss: 0.002097910037264228\n","Epoch [15/224], Step [100/225], Loss: 0.004182633478194475\n","Epoch [15/224], Step [200/225], Loss: 0.0029588951729238033\n","Epoch [16/224], Step [100/225], Loss: 0.0004130839370191097\n","Epoch [16/224], Step [200/225], Loss: 0.0007137854699976742\n","Epoch [17/224], Step [100/225], Loss: 0.00011737966269720346\n","Epoch [17/224], Step [200/225], Loss: 0.007244161330163479\n","Epoch [18/224], Step [100/225], Loss: 0.0001908610574901104\n","Epoch [18/224], Step [200/225], Loss: 0.00010326085612177849\n","Epoch [19/224], Step [100/225], Loss: 0.0002856434148270637\n","Epoch [19/224], Step [200/225], Loss: 0.00013099590432830155\n","Epoch [20/224], Step [100/225], Loss: 0.000412715453421697\n","Epoch [20/224], Step [200/225], Loss: 0.0011752647114917636\n","Epoch [21/224], Step [100/225], Loss: 0.0008737044408917427\n","Epoch [21/224], Step [200/225], Loss: 3.1676867365604267e-05\n","Epoch [22/224], Step [100/225], Loss: 0.00018223022925667465\n","Epoch [22/224], Step [200/225], Loss: 0.00017753693100530654\n","Epoch [23/224], Step [100/225], Loss: 2.655184653121978e-05\n","Epoch [23/224], Step [200/225], Loss: 2.0573414076352492e-05\n","Epoch [24/224], Step [100/225], Loss: 0.0016888302052393556\n","Epoch [24/224], Step [200/225], Loss: 0.0014289823593571782\n","Epoch [25/224], Step [100/225], Loss: 0.0011378341587260365\n","Epoch [25/224], Step [200/225], Loss: 0.0004383540654089302\n","Epoch [26/224], Step [100/225], Loss: 8.706825610715896e-05\n","Epoch [26/224], Step [200/225], Loss: 2.128990672645159e-05\n","Epoch [27/224], Step [100/225], Loss: 0.00041086686542257667\n","Epoch [27/224], Step [200/225], Loss: 0.00038386473897844553\n","Epoch [28/224], Step [100/225], Loss: 2.2584581529372372e-05\n","Epoch [28/224], Step [200/225], Loss: 1.9480070477584377e-05\n","Epoch [29/224], Step [100/225], Loss: 0.0002592144883237779\n","Epoch [29/224], Step [200/225], Loss: 3.5227840271545574e-05\n","Epoch [30/224], Step [100/225], Loss: 0.0001225742162205279\n","Epoch [30/224], Step [200/225], Loss: 2.2260159312281758e-05\n","Epoch [31/224], Step [100/225], Loss: 2.23257411562372e-05\n","Epoch [31/224], Step [200/225], Loss: 0.0285579115152359\n","Epoch [32/224], Step [100/225], Loss: 1.8892886146204546e-05\n","Epoch [32/224], Step [200/225], Loss: 1.766574860084802e-05\n","Epoch [33/224], Step [100/225], Loss: 2.209416743426118e-05\n","Epoch [33/224], Step [200/225], Loss: 1.5132473890844267e-05\n","Epoch [34/224], Step [100/225], Loss: 1.2346640687610488e-05\n","Epoch [34/224], Step [200/225], Loss: 1.4483247468888294e-05\n","Epoch [35/224], Step [100/225], Loss: 6.815650522185024e-06\n","Epoch [35/224], Step [200/225], Loss: 2.5772786102606915e-05\n","Epoch [36/224], Step [100/225], Loss: 4.437507868715329e-06\n","Epoch [36/224], Step [200/225], Loss: 3.439148031247896e-06\n","Epoch [37/224], Step [100/225], Loss: 2.1576804556389106e-06\n","Epoch [37/224], Step [200/225], Loss: 2.9593466024380177e-06\n","Epoch [38/224], Step [100/225], Loss: 3.975071740569547e-05\n","Epoch [38/224], Step [200/225], Loss: 2.0384691197250504e-06\n","Epoch [39/224], Step [100/225], Loss: 1.828059794206638e-05\n","Epoch [39/224], Step [200/225], Loss: 1.2218924894114025e-06\n","Epoch [40/224], Step [100/225], Loss: 1.3291776213009143e-06\n","Epoch [40/224], Step [200/225], Loss: 1.3172600574762328e-06\n","Epoch [41/224], Step [100/225], Loss: 1.999719188461313e-06\n","Epoch [41/224], Step [200/225], Loss: 1.4424285836867057e-06\n","Epoch [42/224], Step [100/225], Loss: 1.1712294281096547e-06\n","Epoch [42/224], Step [200/225], Loss: 1.2963985227543162e-06\n","Epoch [43/224], Step [100/225], Loss: 9.685742270448827e-07\n","Epoch [43/224], Step [200/225], Loss: 9.000289651339699e-07\n","Epoch [44/224], Step [100/225], Loss: 1.019237060972955e-06\n","Epoch [44/224], Step [200/225], Loss: 9.98375298877363e-07\n","Epoch [45/224], Step [100/225], Loss: 1.394736045767786e-06\n","Epoch [45/224], Step [200/225], Loss: 6.079665695324366e-07\n","Epoch [46/224], Step [100/225], Loss: 1.1503657333378214e-06\n","Epoch [46/224], Step [200/225], Loss: 4.8811953092808835e-06\n","Epoch [47/224], Step [100/225], Loss: 6.169074140416342e-07\n","Epoch [47/224], Step [200/225], Loss: 8.434037113147497e-07\n","Epoch [48/224], Step [100/225], Loss: 4.4405427956917265e-07\n","Epoch [48/224], Step [200/225], Loss: 5.871053190276143e-07\n","Epoch [49/224], Step [100/225], Loss: 4.1723222921064007e-07\n","Epoch [49/224], Step [200/225], Loss: 3.665680594622245e-07\n","Epoch [50/224], Step [100/225], Loss: 3.5166721090718056e-07\n","Epoch [50/224], Step [200/225], Loss: 3.6358807165015605e-07\n","Epoch [51/224], Step [100/225], Loss: 4.857774911215529e-07\n","Epoch [51/224], Step [200/225], Loss: 2.9206259455349937e-07\n","Epoch [52/224], Step [100/225], Loss: 3.2782537573439186e-07\n","Epoch [52/224], Step [200/225], Loss: 2.4139876586559694e-07\n","Epoch [53/224], Step [100/225], Loss: 3.1888464491203194e-07\n","Epoch [53/224], Step [200/225], Loss: 3.1888464491203194e-07\n","Epoch [54/224], Step [100/225], Loss: 2.3841853646899835e-07\n","Epoch [54/224], Step [200/225], Loss: 2.2649760467174929e-07\n","Epoch [55/224], Step [100/225], Loss: 1.2218950473652512e-07\n","Epoch [55/224], Step [200/225], Loss: 1.7285341868955584e-07\n","Epoch [56/224], Step [100/225], Loss: 1.3709065171951806e-07\n","Epoch [56/224], Step [200/225], Loss: 1.1324881654672936e-07\n","Epoch [57/224], Step [100/225], Loss: 1.51991798702511e-07\n","Epoch [57/224], Step [200/225], Loss: 1.6689297410721338e-07\n","Epoch [58/224], Step [100/225], Loss: 1.3709066593037278e-07\n","Epoch [58/224], Step [200/225], Loss: 9.83476553528817e-08\n","Epoch [59/224], Step [100/225], Loss: 1.0728833643725011e-07\n","Epoch [59/224], Step [200/225], Loss: 1.2516974834397843e-07\n","Epoch [60/224], Step [100/225], Loss: 1.0728833643725011e-07\n","Epoch [60/224], Step [200/225], Loss: 9.238719655968453e-08\n","Epoch [61/224], Step [100/225], Loss: 8.642673066105999e-08\n","Epoch [61/224], Step [200/225], Loss: 6.556509646316044e-08\n","Epoch [62/224], Step [100/225], Loss: 6.258487417198921e-08\n","Epoch [62/224], Step [200/225], Loss: 8.046626476243546e-08\n","Epoch [63/224], Step [100/225], Loss: 8.344532034243457e-07\n","Epoch [63/224], Step [200/225], Loss: 9.62606463872362e-07\n","Epoch [64/224], Step [100/225], Loss: 4.172324707951702e-08\n","Epoch [64/224], Step [200/225], Loss: 5.364417532405241e-08\n","Epoch [65/224], Step [100/225], Loss: 3.576278473360617e-08\n","Epoch [65/224], Step [200/225], Loss: 8.046626476243546e-08\n","Epoch [66/224], Step [100/225], Loss: 2.9802318834981634e-08\n","Epoch [66/224], Step [200/225], Loss: 6.258487417198921e-08\n","Epoch [67/224], Step [100/225], Loss: 3.576278473360617e-08\n","Epoch [67/224], Step [200/225], Loss: 6.258487417198921e-08\n","Epoch [68/224], Step [100/225], Loss: 2.9802318834981634e-08\n","Epoch [68/224], Step [200/225], Loss: 4.4703465817974575e-08\n","Epoch [69/224], Step [100/225], Loss: 8.642665250135906e-08\n","Epoch [69/224], Step [200/225], Loss: 2.384185648907078e-08\n","Epoch [70/224], Step [100/225], Loss: 2.086162353975851e-08\n","Epoch [70/224], Step [200/225], Loss: 2.6822087662026206e-08\n","Epoch [71/224], Step [100/225], Loss: 1.192092824453539e-08\n","Epoch [71/224], Step [200/225], Loss: 1.4901159417490817e-08\n","Epoch [72/224], Step [100/225], Loss: 9.83476056148902e-08\n","Epoch [72/224], Step [200/225], Loss: 8.940696183401542e-09\n","Epoch [73/224], Step [100/225], Loss: 1.192092735635697e-08\n","Epoch [73/224], Step [200/225], Loss: 9.834750613890719e-08\n","Epoch [74/224], Step [100/225], Loss: 1.192092824453539e-08\n","Epoch [74/224], Step [200/225], Loss: 1.4901159417490817e-08\n","Epoch [75/224], Step [100/225], Loss: 1.4901159417490817e-08\n","Epoch [75/224], Step [200/225], Loss: 1.4901159417490817e-08\n","Epoch [76/224], Step [100/225], Loss: 7.15254913075114e-08\n","Epoch [76/224], Step [200/225], Loss: 8.940696183401542e-09\n","Epoch [77/224], Step [100/225], Loss: 5.960464122267695e-09\n","Epoch [77/224], Step [200/225], Loss: 5.960464122267695e-09\n","Epoch [78/224], Step [100/225], Loss: 8.940696183401542e-09\n","Epoch [78/224], Step [200/225], Loss: 2.9802320611338473e-09\n","Epoch [79/224], Step [100/225], Loss: 5.960464122267695e-09\n","Epoch [79/224], Step [200/225], Loss: 1.400705542664582e-07\n","Epoch [80/224], Step [100/225], Loss: 2.9802320611338473e-09\n","Epoch [80/224], Step [200/225], Loss: 2.9802320611338473e-09\n","Epoch [81/224], Step [100/225], Loss: 0.0\n","Epoch [81/224], Step [200/225], Loss: 0.0\n","Epoch [82/224], Step [100/225], Loss: 2.9802320611338473e-09\n","Epoch [82/224], Step [200/225], Loss: 2.9802320611338473e-09\n","Epoch [83/224], Step [100/225], Loss: 8.940696183401542e-09\n","Epoch [83/224], Step [200/225], Loss: 2.9802320611338473e-09\n","Epoch [84/224], Step [100/225], Loss: 0.0\n","Epoch [84/224], Step [200/225], Loss: 1.4901156752955558e-08\n","Epoch [85/224], Step [100/225], Loss: 0.0\n","Epoch [85/224], Step [200/225], Loss: 0.0\n","Epoch [86/224], Step [100/225], Loss: 0.0\n","Epoch [86/224], Step [200/225], Loss: 0.0\n","Epoch [87/224], Step [100/225], Loss: 0.0\n","Epoch [87/224], Step [200/225], Loss: 0.0\n","Epoch [88/224], Step [100/225], Loss: 8.940695295223122e-09\n","Epoch [88/224], Step [200/225], Loss: 0.0\n","Epoch [89/224], Step [100/225], Loss: 4.47034445016925e-08\n","Epoch [89/224], Step [200/225], Loss: 0.0\n","Epoch [90/224], Step [100/225], Loss: 0.0\n","Epoch [90/224], Step [200/225], Loss: 0.0\n","Epoch [91/224], Step [100/225], Loss: 0.0\n","Epoch [91/224], Step [200/225], Loss: 0.0\n","Epoch [92/224], Step [100/225], Loss: 0.0\n","Epoch [92/224], Step [200/225], Loss: 0.0\n","Epoch [93/224], Step [100/225], Loss: 0.0\n","Epoch [93/224], Step [200/225], Loss: 0.0\n","Epoch [94/224], Step [100/225], Loss: 0.0\n","Epoch [94/224], Step [200/225], Loss: 0.0\n","Epoch [95/224], Step [100/225], Loss: 0.0\n","Epoch [95/224], Step [200/225], Loss: 0.0\n","Epoch [96/224], Step [100/225], Loss: 0.0\n","Epoch [96/224], Step [200/225], Loss: 0.0\n","Epoch [97/224], Step [100/225], Loss: 0.0\n","Epoch [97/224], Step [200/225], Loss: 0.0\n","Epoch [98/224], Step [100/225], Loss: 0.0\n","Epoch [98/224], Step [200/225], Loss: 0.0\n","Epoch [99/224], Step [100/225], Loss: 0.0\n","Epoch [99/224], Step [200/225], Loss: 0.0\n","Epoch [100/224], Step [100/225], Loss: 5.960463678178485e-09\n","Epoch [100/224], Step [200/225], Loss: 0.0\n","Epoch [101/224], Step [100/225], Loss: 0.0\n","Epoch [101/224], Step [200/225], Loss: 0.0\n","Epoch [102/224], Step [100/225], Loss: 0.0\n","Epoch [102/224], Step [200/225], Loss: 0.0\n","Epoch [103/224], Step [100/225], Loss: 0.0\n","Epoch [103/224], Step [200/225], Loss: 0.0\n","Epoch [104/224], Step [100/225], Loss: 0.0\n","Epoch [104/224], Step [200/225], Loss: 0.0\n","Epoch [105/224], Step [100/225], Loss: 0.0\n","Epoch [105/224], Step [200/225], Loss: 0.0\n","Epoch [106/224], Step [100/225], Loss: 2.9802320611338473e-09\n","Epoch [106/224], Step [200/225], Loss: 0.0\n","Epoch [107/224], Step [100/225], Loss: 0.0\n","Epoch [107/224], Step [200/225], Loss: 0.0\n","Epoch [108/224], Step [100/225], Loss: 5.960463678178485e-09\n","Epoch [108/224], Step [200/225], Loss: 0.0\n","Epoch [109/224], Step [100/225], Loss: 0.0\n","Epoch [109/224], Step [200/225], Loss: 0.0\n","Epoch [110/224], Step [100/225], Loss: 0.0\n","Epoch [110/224], Step [200/225], Loss: 0.0\n","Epoch [111/224], Step [100/225], Loss: 0.0\n","Epoch [111/224], Step [200/225], Loss: 0.0\n","Epoch [112/224], Step [100/225], Loss: 0.0\n","Epoch [112/224], Step [200/225], Loss: 0.0\n","Epoch [113/224], Step [100/225], Loss: 0.0\n","Epoch [113/224], Step [200/225], Loss: 0.0\n","Epoch [114/224], Step [100/225], Loss: 0.0\n","Epoch [114/224], Step [200/225], Loss: 0.0\n","Epoch [115/224], Step [100/225], Loss: 0.0\n","Epoch [115/224], Step [200/225], Loss: 0.0\n","Epoch [116/224], Step [100/225], Loss: 0.0\n","Epoch [116/224], Step [200/225], Loss: 0.0\n","Epoch [117/224], Step [100/225], Loss: 0.0\n","Epoch [117/224], Step [200/225], Loss: 0.0\n","Epoch [118/224], Step [100/225], Loss: 0.0\n","Epoch [118/224], Step [200/225], Loss: 0.0\n","Epoch [119/224], Step [100/225], Loss: 0.0\n","Epoch [119/224], Step [200/225], Loss: 0.0\n","Epoch [120/224], Step [100/225], Loss: 0.0\n","Epoch [120/224], Step [200/225], Loss: 0.0\n","Epoch [121/224], Step [100/225], Loss: 0.0\n","Epoch [121/224], Step [200/225], Loss: 0.0\n","Epoch [122/224], Step [100/225], Loss: 0.0\n","Epoch [122/224], Step [200/225], Loss: 0.0\n","Epoch [123/224], Step [100/225], Loss: 0.0\n","Epoch [123/224], Step [200/225], Loss: 0.0\n","Epoch [124/224], Step [100/225], Loss: 0.0\n","Epoch [124/224], Step [200/225], Loss: 0.0\n","Epoch [125/224], Step [100/225], Loss: 0.0\n","Epoch [125/224], Step [200/225], Loss: 0.0\n","Epoch [126/224], Step [100/225], Loss: 0.0\n","Epoch [126/224], Step [200/225], Loss: 0.0\n","Epoch [127/224], Step [100/225], Loss: 0.0\n","Epoch [127/224], Step [200/225], Loss: 0.0\n","Epoch [128/224], Step [100/225], Loss: 0.0\n","Epoch [128/224], Step [200/225], Loss: 0.0\n","Epoch [129/224], Step [100/225], Loss: 0.0\n","Epoch [129/224], Step [200/225], Loss: 0.0\n","Epoch [130/224], Step [100/225], Loss: 0.0\n","Epoch [130/224], Step [200/225], Loss: 0.0\n","Epoch [131/224], Step [100/225], Loss: 0.0\n","Epoch [131/224], Step [200/225], Loss: 0.0\n","Epoch [132/224], Step [100/225], Loss: 0.0\n","Epoch [132/224], Step [200/225], Loss: 0.0\n","Epoch [133/224], Step [100/225], Loss: 0.0\n","Epoch [133/224], Step [200/225], Loss: 0.0\n","Epoch [134/224], Step [100/225], Loss: 0.0\n","Epoch [134/224], Step [200/225], Loss: 0.0\n","Epoch [135/224], Step [100/225], Loss: 0.0\n","Epoch [135/224], Step [200/225], Loss: 0.0\n","Epoch [136/224], Step [100/225], Loss: 0.0\n","Epoch [136/224], Step [200/225], Loss: 0.0\n","Epoch [137/224], Step [100/225], Loss: 0.0\n","Epoch [137/224], Step [200/225], Loss: 0.0\n","Epoch [138/224], Step [100/225], Loss: 0.0\n","Epoch [138/224], Step [200/225], Loss: 0.0\n","Epoch [139/224], Step [100/225], Loss: 0.0\n","Epoch [139/224], Step [200/225], Loss: 0.0\n","Epoch [140/224], Step [100/225], Loss: 0.0\n","Epoch [140/224], Step [200/225], Loss: 0.0\n","Epoch [141/224], Step [100/225], Loss: 0.0\n","Epoch [141/224], Step [200/225], Loss: 0.0\n","Epoch [142/224], Step [100/225], Loss: 0.0\n","Epoch [142/224], Step [200/225], Loss: 0.0\n","Epoch [143/224], Step [100/225], Loss: 0.0\n","Epoch [143/224], Step [200/225], Loss: 0.0\n","Epoch [144/224], Step [100/225], Loss: 0.0\n","Epoch [144/224], Step [200/225], Loss: 0.0\n","Epoch [145/224], Step [100/225], Loss: 0.0\n","Epoch [145/224], Step [200/225], Loss: 0.0\n","Epoch [146/224], Step [100/225], Loss: 0.0\n","Epoch [146/224], Step [200/225], Loss: 0.0\n","Epoch [147/224], Step [100/225], Loss: 0.0\n","Epoch [147/224], Step [200/225], Loss: 0.0\n","Epoch [148/224], Step [100/225], Loss: 0.0\n","Epoch [148/224], Step [200/225], Loss: 0.0\n","Epoch [149/224], Step [100/225], Loss: 0.0\n","Epoch [149/224], Step [200/225], Loss: 0.0\n","Epoch [150/224], Step [100/225], Loss: 0.0\n","Epoch [150/224], Step [200/225], Loss: 0.0\n","Epoch [151/224], Step [100/225], Loss: 0.0\n","Epoch [151/224], Step [200/225], Loss: 0.0\n","Epoch [152/224], Step [100/225], Loss: 0.0\n","Epoch [152/224], Step [200/225], Loss: 0.0\n","Epoch [153/224], Step [100/225], Loss: 0.0\n","Epoch [153/224], Step [200/225], Loss: 0.0\n","Epoch [154/224], Step [100/225], Loss: 0.0\n","Epoch [154/224], Step [200/225], Loss: 0.0\n","Epoch [155/224], Step [100/225], Loss: 0.0\n","Epoch [155/224], Step [200/225], Loss: 0.0\n","Epoch [156/224], Step [100/225], Loss: 0.0\n","Epoch [156/224], Step [200/225], Loss: 0.0\n","Epoch [157/224], Step [100/225], Loss: 0.0\n","Epoch [157/224], Step [200/225], Loss: 0.0\n","Epoch [158/224], Step [100/225], Loss: 0.0\n","Epoch [158/224], Step [200/225], Loss: 0.0\n","Epoch [159/224], Step [100/225], Loss: 0.0\n","Epoch [159/224], Step [200/225], Loss: 0.0\n","Epoch [160/224], Step [100/225], Loss: 0.0\n","Epoch [160/224], Step [200/225], Loss: 0.0\n","Epoch [161/224], Step [100/225], Loss: 0.0\n","Epoch [161/224], Step [200/225], Loss: 0.0\n","Epoch [162/224], Step [100/225], Loss: 0.0\n","Epoch [162/224], Step [200/225], Loss: 0.0\n","Epoch [163/224], Step [100/225], Loss: 0.0\n","Epoch [163/224], Step [200/225], Loss: 0.0\n","Epoch [164/224], Step [100/225], Loss: 0.0\n","Epoch [164/224], Step [200/225], Loss: 0.0\n","Epoch [165/224], Step [100/225], Loss: 0.0\n","Epoch [165/224], Step [200/225], Loss: 0.0\n","Epoch [166/224], Step [100/225], Loss: 0.0\n","Epoch [166/224], Step [200/225], Loss: 0.0\n","Epoch [167/224], Step [100/225], Loss: 0.0\n","Epoch [167/224], Step [200/225], Loss: 0.0\n","Epoch [168/224], Step [100/225], Loss: 0.0\n","Epoch [168/224], Step [200/225], Loss: 0.0\n","Epoch [169/224], Step [100/225], Loss: 0.0\n","Epoch [169/224], Step [200/225], Loss: 0.0\n","Epoch [170/224], Step [100/225], Loss: 0.0\n","Epoch [170/224], Step [200/225], Loss: 0.0\n","Epoch [171/224], Step [100/225], Loss: 0.0\n","Epoch [171/224], Step [200/225], Loss: 0.0\n","Epoch [172/224], Step [100/225], Loss: 0.0\n","Epoch [172/224], Step [200/225], Loss: 0.0\n","Epoch [173/224], Step [100/225], Loss: 0.0\n","Epoch [173/224], Step [200/225], Loss: 0.0\n","Epoch [174/224], Step [100/225], Loss: 0.0\n","Epoch [174/224], Step [200/225], Loss: 0.0\n","Epoch [175/224], Step [100/225], Loss: 0.0\n","Epoch [175/224], Step [200/225], Loss: 0.0\n","Epoch [176/224], Step [100/225], Loss: 0.0\n","Epoch [176/224], Step [200/225], Loss: 0.0\n","Epoch [177/224], Step [100/225], Loss: 0.0\n","Epoch [177/224], Step [200/225], Loss: 0.0\n","Epoch [178/224], Step [100/225], Loss: 0.0\n","Epoch [178/224], Step [200/225], Loss: 0.0\n","Epoch [179/224], Step [100/225], Loss: 0.0\n","Epoch [179/224], Step [200/225], Loss: 0.0\n","Epoch [180/224], Step [100/225], Loss: 0.0\n","Epoch [180/224], Step [200/225], Loss: 0.0\n","Epoch [181/224], Step [100/225], Loss: 0.0\n","Epoch [181/224], Step [200/225], Loss: 0.0\n","Epoch [182/224], Step [100/225], Loss: 0.0\n","Epoch [182/224], Step [200/225], Loss: 0.0\n","Epoch [183/224], Step [100/225], Loss: 0.0\n","Epoch [183/224], Step [200/225], Loss: 0.0\n","Epoch [184/224], Step [100/225], Loss: 0.0\n","Epoch [184/224], Step [200/225], Loss: 0.0\n","Epoch [185/224], Step [100/225], Loss: 0.0\n","Epoch [185/224], Step [200/225], Loss: 0.0\n","Epoch [186/224], Step [100/225], Loss: 0.0\n","Epoch [186/224], Step [200/225], Loss: 0.0\n","Epoch [187/224], Step [100/225], Loss: 0.0\n","Epoch [187/224], Step [200/225], Loss: 0.0\n","Epoch [188/224], Step [100/225], Loss: 0.0\n","Epoch [188/224], Step [200/225], Loss: 0.0\n","Epoch [189/224], Step [100/225], Loss: 0.0\n","Epoch [189/224], Step [200/225], Loss: 0.0\n","Epoch [190/224], Step [100/225], Loss: 0.0\n","Epoch [190/224], Step [200/225], Loss: 0.0\n","Epoch [191/224], Step [100/225], Loss: 0.0\n","Epoch [191/224], Step [200/225], Loss: 0.0\n","Epoch [192/224], Step [100/225], Loss: 0.0\n","Epoch [192/224], Step [200/225], Loss: 0.0\n","Epoch [193/224], Step [100/225], Loss: 0.0\n","Epoch [193/224], Step [200/225], Loss: 0.0\n","Epoch [194/224], Step [100/225], Loss: 0.0\n","Epoch [194/224], Step [200/225], Loss: 0.0\n","Epoch [195/224], Step [100/225], Loss: 0.0\n","Epoch [195/224], Step [200/225], Loss: 0.0\n","Epoch [196/224], Step [100/225], Loss: 0.0\n","Epoch [196/224], Step [200/225], Loss: 0.0\n","Epoch [197/224], Step [100/225], Loss: 0.0\n","Epoch [197/224], Step [200/225], Loss: 0.0\n","Epoch [198/224], Step [100/225], Loss: 0.0\n","Epoch [198/224], Step [200/225], Loss: 0.0\n","Epoch [199/224], Step [100/225], Loss: 0.0\n","Epoch [199/224], Step [200/225], Loss: 0.0\n","Epoch [200/224], Step [100/225], Loss: 0.0\n","Epoch [200/224], Step [200/225], Loss: 0.0\n","Epoch [201/224], Step [100/225], Loss: 0.0\n","Epoch [201/224], Step [200/225], Loss: 0.0\n","Epoch [202/224], Step [100/225], Loss: 0.0\n","Epoch [202/224], Step [200/225], Loss: 0.0\n","Epoch [203/224], Step [100/225], Loss: 0.0\n","Epoch [203/224], Step [200/225], Loss: 0.0\n","Epoch [204/224], Step [100/225], Loss: 0.0\n","Epoch [204/224], Step [200/225], Loss: 0.0\n","Epoch [205/224], Step [100/225], Loss: 0.0\n","Epoch [205/224], Step [200/225], Loss: 0.0\n","Epoch [206/224], Step [100/225], Loss: 0.0\n","Epoch [206/224], Step [200/225], Loss: 0.0\n","Epoch [207/224], Step [100/225], Loss: 0.0\n","Epoch [207/224], Step [200/225], Loss: 0.0\n","Epoch [208/224], Step [100/225], Loss: 0.0\n","Epoch [208/224], Step [200/225], Loss: 0.0\n","Epoch [209/224], Step [100/225], Loss: 0.0\n","Epoch [209/224], Step [200/225], Loss: 0.0\n","Epoch [210/224], Step [100/225], Loss: 0.0\n","Epoch [210/224], Step [200/225], Loss: 0.0\n","Epoch [211/224], Step [100/225], Loss: 0.0\n","Epoch [211/224], Step [200/225], Loss: 0.0\n","Epoch [212/224], Step [100/225], Loss: 0.0\n","Epoch [212/224], Step [200/225], Loss: 0.0\n","Epoch [213/224], Step [100/225], Loss: 0.0\n","Epoch [213/224], Step [200/225], Loss: 0.0\n","Epoch [214/224], Step [100/225], Loss: 0.0\n","Epoch [214/224], Step [200/225], Loss: 0.0\n","Epoch [215/224], Step [100/225], Loss: 0.0\n","Epoch [215/224], Step [200/225], Loss: 0.0\n","Epoch [216/224], Step [100/225], Loss: 0.0\n","Epoch [216/224], Step [200/225], Loss: 0.0\n","Epoch [217/224], Step [100/225], Loss: 0.0\n","Epoch [217/224], Step [200/225], Loss: 0.0\n","Epoch [218/224], Step [100/225], Loss: 0.0\n","Epoch [218/224], Step [200/225], Loss: 0.0\n","Epoch [219/224], Step [100/225], Loss: 0.0\n","Epoch [219/224], Step [200/225], Loss: 0.0\n","Epoch [220/224], Step [100/225], Loss: 0.0\n","Epoch [220/224], Step [200/225], Loss: 0.0\n","Epoch [221/224], Step [100/225], Loss: 0.0\n","Epoch [221/224], Step [200/225], Loss: 0.0\n","Epoch [222/224], Step [100/225], Loss: 0.0\n","Epoch [222/224], Step [200/225], Loss: 0.0\n","Epoch [223/224], Step [100/225], Loss: 0.0\n","Epoch [223/224], Step [200/225], Loss: 0.0\n","Epoch [224/224], Step [100/225], Loss: 0.0\n","Epoch [224/224], Step [200/225], Loss: 0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNBHa8bkQ90f","executionInfo":{"elapsed":875,"status":"ok","timestamp":1623305421227,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"63565fcc-fa19-47e3-bcbd-bd77e8f0d076"},"source":["# Test the model\n","test_dataset = CustomDataset(X_0712_transformed, y_0712_transformed)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=5, shuffle=False)\n","\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (images, labels) in enumerate(test_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        if i==0:\n","          y_pre = predicted.tolist()\n","        else:\n","          y_pre.extend(predicted.tolist())\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","\n","torch.save(model.state_dict(), 'model.ckpt')\n","\n","target_names =  ['Nominal', 'Faulty']\n","print(classification_report(y_0712_transformed.tolist(), y_pre, target_names=target_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 100.0\n","Iteration:1,  Accuracy: 100.0\n","Iteration:2,  Accuracy: 100.0\n","Iteration:3,  Accuracy: 100.0\n","Iteration:4,  Accuracy: 92.0\n","Iteration:5,  Accuracy: 90.0\n","Iteration:6,  Accuracy: 88.57142857142857\n","Iteration:7,  Accuracy: 85.0\n","Iteration:8,  Accuracy: 84.44444444444444\n","Iteration:9,  Accuracy: 84.0\n","Iteration:10,  Accuracy: 85.45454545454545\n","Iteration:11,  Accuracy: 86.66666666666667\n","Iteration:12,  Accuracy: 87.6923076923077\n","Iteration:13,  Accuracy: 88.57142857142857\n","Iteration:14,  Accuracy: 89.33333333333333\n","Iteration:15,  Accuracy: 90.0\n","Iteration:16,  Accuracy: 90.58823529411765\n","Iteration:17,  Accuracy: 91.11111111111111\n","Iteration:18,  Accuracy: 91.57894736842105\n","Iteration:19,  Accuracy: 92.0\n","Iteration:20,  Accuracy: 92.38095238095238\n","Iteration:21,  Accuracy: 92.72727272727273\n","Iteration:22,  Accuracy: 93.04347826086956\n","Iteration:23,  Accuracy: 92.5\n","Iteration:24,  Accuracy: 91.2\n","Iteration:25,  Accuracy: 91.53846153846153\n","Iteration:26,  Accuracy: 91.85185185185185\n","Iteration:27,  Accuracy: 92.14285714285714\n","Iteration:28,  Accuracy: 92.41379310344827\n","Iteration:29,  Accuracy: 92.0\n","Iteration:30,  Accuracy: 92.25806451612904\n","Iteration:31,  Accuracy: 92.5\n","Iteration:32,  Accuracy: 92.72727272727273\n","Iteration:33,  Accuracy: 92.94117647058823\n","Iteration:34,  Accuracy: 93.14285714285714\n","Iteration:35,  Accuracy: 93.33333333333333\n","Iteration:36,  Accuracy: 92.97297297297297\n","Iteration:37,  Accuracy: 93.15789473684211\n","Iteration:38,  Accuracy: 92.82051282051282\n","Iteration:39,  Accuracy: 92.5\n","Iteration:40,  Accuracy: 92.6829268292683\n","Iteration:41,  Accuracy: 92.38095238095238\n","Iteration:42,  Accuracy: 92.55813953488372\n","Iteration:43,  Accuracy: 92.27272727272727\n","Iteration:44,  Accuracy: 92.44444444444444\n","Iteration:45,  Accuracy: 92.6086956521739\n","Iteration:46,  Accuracy: 92.76595744680851\n","Iteration:47,  Accuracy: 92.91666666666667\n","Iteration:48,  Accuracy: 93.06122448979592\n","Iteration:49,  Accuracy: 93.2\n","Iteration:50,  Accuracy: 93.33333333333333\n","Iteration:51,  Accuracy: 93.46153846153847\n","Iteration:52,  Accuracy: 92.83018867924528\n","Iteration:53,  Accuracy: 92.22222222222223\n","Iteration:54,  Accuracy: 91.63636363636364\n","Iteration:55,  Accuracy: 91.42857142857143\n","Iteration:56,  Accuracy: 91.57894736842105\n","Iteration:57,  Accuracy: 91.72413793103448\n","Iteration:58,  Accuracy: 91.86440677966101\n","Iteration:59,  Accuracy: 92.0\n","Iteration:60,  Accuracy: 92.1311475409836\n","Iteration:61,  Accuracy: 91.93548387096774\n","Iteration:62,  Accuracy: 92.06349206349206\n","Iteration:63,  Accuracy: 92.1875\n","Iteration:64,  Accuracy: 92.3076923076923\n","Iteration:65,  Accuracy: 92.42424242424242\n","Iteration:66,  Accuracy: 92.53731343283582\n","Iteration:67,  Accuracy: 92.6470588235294\n","Iteration:68,  Accuracy: 92.7536231884058\n","Iteration:69,  Accuracy: 92.85714285714286\n","Iteration:70,  Accuracy: 92.95774647887323\n","Iteration:71,  Accuracy: 93.05555555555556\n","Iteration:72,  Accuracy: 93.15068493150685\n","Iteration:73,  Accuracy: 93.24324324324324\n","Iteration:74,  Accuracy: 93.33333333333333\n","Iteration:75,  Accuracy: 93.42105263157895\n","Iteration:76,  Accuracy: 93.50649350649351\n","Iteration:77,  Accuracy: 93.58974358974359\n","Iteration:78,  Accuracy: 93.16455696202532\n","Iteration:79,  Accuracy: 93.0\n","Iteration:80,  Accuracy: 92.8395061728395\n","Iteration:81,  Accuracy: 92.92682926829268\n","Iteration:82,  Accuracy: 93.01204819277109\n","Iteration:83,  Accuracy: 93.0952380952381\n","Iteration:84,  Accuracy: 92.70588235294117\n","Iteration:85,  Accuracy: 92.55813953488372\n","Iteration:86,  Accuracy: 92.183908045977\n","Iteration:87,  Accuracy: 92.27272727272727\n","Iteration:88,  Accuracy: 92.35955056179775\n","Iteration:89,  Accuracy: 92.44444444444444\n","Test Accuracy of the model on the 10000 test images: 92.44444444444444 %\n","              precision    recall  f1-score   support\n","\n","     Nominal       0.88      0.98      0.92       211\n","      Faulty       0.98      0.88      0.93       239\n","\n","    accuracy                           0.92       450\n","   macro avg       0.93      0.93      0.92       450\n","weighted avg       0.93      0.92      0.92       450\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d8rO5J0Stk4d"},"source":["## Multi class classification\n","### 5. 21일 데이터 단독 학습\n","##### 논문 결과: 94%"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fckX_SqufEW","executionInfo":{"elapsed":340,"status":"ok","timestamp":1623740785237,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"faff9f53-0e62-4dc4-fece-8cc8c818fa88"},"source":["print(y_test_0721.shape)\n","#print(np.bincount(y_train_0721))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(5965,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J7K2ha0Js2br","executionInfo":{"elapsed":1479419,"status":"ok","timestamp":1623742267177,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"cdaa293d-79ec-4def-bf6c-13cd091f53cc"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 9\n","batch_size = 120\n","num_epochs = len(y_train_0721) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train_0721, y_train_0721)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=120, shuffle=True)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/198], Step [100/199], Loss: 0.5562757849693298\n","Epoch [2/198], Step [100/199], Loss: 0.3308072090148926\n","Epoch [3/198], Step [100/199], Loss: 0.32848304510116577\n","Epoch [4/198], Step [100/199], Loss: 0.09255971759557724\n","Epoch [5/198], Step [100/199], Loss: 0.18708154559135437\n","Epoch [6/198], Step [100/199], Loss: 0.1092614009976387\n","Epoch [7/198], Step [100/199], Loss: 0.04381103441119194\n","Epoch [8/198], Step [100/199], Loss: 0.020848872140049934\n","Epoch [9/198], Step [100/199], Loss: 0.05819854512810707\n","Epoch [10/198], Step [100/199], Loss: 0.041767846792936325\n","Epoch [11/198], Step [100/199], Loss: 0.005734832491725683\n","Epoch [12/198], Step [100/199], Loss: 0.042077790945768356\n","Epoch [13/198], Step [100/199], Loss: 0.0018070769729092717\n","Epoch [14/198], Step [100/199], Loss: 0.0005045937723480165\n","Epoch [15/198], Step [100/199], Loss: 0.0012840890558436513\n","Epoch [16/198], Step [100/199], Loss: 0.03329910710453987\n","Epoch [17/198], Step [100/199], Loss: 0.04496325924992561\n","Epoch [18/198], Step [100/199], Loss: 0.007541356608271599\n","Epoch [19/198], Step [100/199], Loss: 0.0029224350582808256\n","Epoch [20/198], Step [100/199], Loss: 0.0007605369319207966\n","Epoch [21/198], Step [100/199], Loss: 0.024635840207338333\n","Epoch [22/198], Step [100/199], Loss: 0.04233608767390251\n","Epoch [23/198], Step [100/199], Loss: 0.024493444710969925\n","Epoch [24/198], Step [100/199], Loss: 0.004779340233653784\n","Epoch [25/198], Step [100/199], Loss: 0.07818598300218582\n","Epoch [26/198], Step [100/199], Loss: 0.005559191107749939\n","Epoch [27/198], Step [100/199], Loss: 0.0034704566933214664\n","Epoch [28/198], Step [100/199], Loss: 0.0007953686290420592\n","Epoch [29/198], Step [100/199], Loss: 0.0009002176229842007\n","Epoch [30/198], Step [100/199], Loss: 0.010713552124798298\n","Epoch [31/198], Step [100/199], Loss: 0.00799183826893568\n","Epoch [32/198], Step [100/199], Loss: 0.0009253389434888959\n","Epoch [33/198], Step [100/199], Loss: 0.0010084665846079588\n","Epoch [34/198], Step [100/199], Loss: 0.0034673481713980436\n","Epoch [35/198], Step [100/199], Loss: 0.030947396531701088\n","Epoch [36/198], Step [100/199], Loss: 0.0013248667819425464\n","Epoch [37/198], Step [100/199], Loss: 0.0011799671920016408\n","Epoch [38/198], Step [100/199], Loss: 0.10943198204040527\n","Epoch [39/198], Step [100/199], Loss: 0.0007904852391220629\n","Epoch [40/198], Step [100/199], Loss: 0.000980472774244845\n","Epoch [41/198], Step [100/199], Loss: 0.0009781024418771267\n","Epoch [42/198], Step [100/199], Loss: 0.00027654788573272526\n","Epoch [43/198], Step [100/199], Loss: 0.00020613701781257987\n","Epoch [44/198], Step [100/199], Loss: 0.0001353854895569384\n","Epoch [45/198], Step [100/199], Loss: 0.00011412089224904776\n","Epoch [46/198], Step [100/199], Loss: 0.00021135184215381742\n","Epoch [47/198], Step [100/199], Loss: 5.147960109752603e-05\n","Epoch [48/198], Step [100/199], Loss: 0.020116271451115608\n","Epoch [49/198], Step [100/199], Loss: 0.0018072448438033462\n","Epoch [50/198], Step [100/199], Loss: 0.0010491535067558289\n","Epoch [51/198], Step [100/199], Loss: 0.001253661816008389\n","Epoch [52/198], Step [100/199], Loss: 0.004003054928034544\n","Epoch [53/198], Step [100/199], Loss: 0.0005450929165817797\n","Epoch [54/198], Step [100/199], Loss: 0.0005370709695853293\n","Epoch [55/198], Step [100/199], Loss: 0.0005463942652568221\n","Epoch [56/198], Step [100/199], Loss: 0.08223166316747665\n","Epoch [57/198], Step [100/199], Loss: 0.002322440268471837\n","Epoch [58/198], Step [100/199], Loss: 0.014079826883971691\n","Epoch [59/198], Step [100/199], Loss: 0.023044606670737267\n","Epoch [60/198], Step [100/199], Loss: 0.040219005197286606\n","Epoch [61/198], Step [100/199], Loss: 0.07828111946582794\n","Epoch [62/198], Step [100/199], Loss: 0.0008067695307545364\n","Epoch [63/198], Step [100/199], Loss: 0.0006115151918493211\n","Epoch [64/198], Step [100/199], Loss: 0.00031464904895983636\n","Epoch [65/198], Step [100/199], Loss: 5.900843825656921e-05\n","Epoch [66/198], Step [100/199], Loss: 0.057318348437547684\n","Epoch [67/198], Step [100/199], Loss: 0.0023065535351634026\n","Epoch [68/198], Step [100/199], Loss: 0.0020229644142091274\n","Epoch [69/198], Step [100/199], Loss: 0.004275861196219921\n","Epoch [70/198], Step [100/199], Loss: 0.0004679658741224557\n","Epoch [71/198], Step [100/199], Loss: 0.0027923271991312504\n","Epoch [72/198], Step [100/199], Loss: 0.00016845294157974422\n","Epoch [73/198], Step [100/199], Loss: 0.00035180713166482747\n","Epoch [74/198], Step [100/199], Loss: 0.00984198972582817\n","Epoch [75/198], Step [100/199], Loss: 0.0008860576199367642\n","Epoch [76/198], Step [100/199], Loss: 0.0025152515154331923\n","Epoch [77/198], Step [100/199], Loss: 0.0019820944871753454\n","Epoch [78/198], Step [100/199], Loss: 0.0003643881937023252\n","Epoch [79/198], Step [100/199], Loss: 0.008281562477350235\n","Epoch [80/198], Step [100/199], Loss: 0.10585904866456985\n","Epoch [81/198], Step [100/199], Loss: 0.0013946930412203074\n","Epoch [82/198], Step [100/199], Loss: 0.00034832951496355236\n","Epoch [83/198], Step [100/199], Loss: 0.00339161092415452\n","Epoch [84/198], Step [100/199], Loss: 0.0021198245231062174\n","Epoch [85/198], Step [100/199], Loss: 0.18335866928100586\n","Epoch [86/198], Step [100/199], Loss: 0.011910122819244862\n","Epoch [87/198], Step [100/199], Loss: 0.017445683479309082\n","Epoch [88/198], Step [100/199], Loss: 0.005378840956836939\n","Epoch [89/198], Step [100/199], Loss: 0.003299499861896038\n","Epoch [90/198], Step [100/199], Loss: 0.0007572535541839898\n","Epoch [91/198], Step [100/199], Loss: 0.00020135589875280857\n","Epoch [92/198], Step [100/199], Loss: 0.0107573876157403\n","Epoch [93/198], Step [100/199], Loss: 0.12454488128423691\n","Epoch [94/198], Step [100/199], Loss: 0.0002891196054406464\n","Epoch [95/198], Step [100/199], Loss: 0.04557066783308983\n","Epoch [96/198], Step [100/199], Loss: 0.009466481395065784\n","Epoch [97/198], Step [100/199], Loss: 0.00028487478266470134\n","Epoch [98/198], Step [100/199], Loss: 0.0014942088164389133\n","Epoch [99/198], Step [100/199], Loss: 0.012692051939666271\n","Epoch [100/198], Step [100/199], Loss: 0.08934811502695084\n","Epoch [101/198], Step [100/199], Loss: 0.0015238058986142278\n","Epoch [102/198], Step [100/199], Loss: 0.00044415926095098257\n","Epoch [103/198], Step [100/199], Loss: 0.0011534291552379727\n","Epoch [104/198], Step [100/199], Loss: 0.00033747669658623636\n","Epoch [105/198], Step [100/199], Loss: 0.0003336746303830296\n","Epoch [106/198], Step [100/199], Loss: 0.09870585054159164\n","Epoch [107/198], Step [100/199], Loss: 0.0005462226690724492\n","Epoch [108/198], Step [100/199], Loss: 0.007040230557322502\n","Epoch [109/198], Step [100/199], Loss: 0.013579226098954678\n","Epoch [110/198], Step [100/199], Loss: 0.07062820345163345\n","Epoch [111/198], Step [100/199], Loss: 0.008735398761928082\n","Epoch [112/198], Step [100/199], Loss: 0.0008575831889174879\n","Epoch [113/198], Step [100/199], Loss: 0.013985089026391506\n","Epoch [114/198], Step [100/199], Loss: 0.00032375360024161637\n","Epoch [115/198], Step [100/199], Loss: 0.0005281448247842491\n","Epoch [116/198], Step [100/199], Loss: 0.002851856639608741\n","Epoch [117/198], Step [100/199], Loss: 0.0008237608126364648\n","Epoch [118/198], Step [100/199], Loss: 0.049173880368471146\n","Epoch [119/198], Step [100/199], Loss: 0.0005013892077840865\n","Epoch [120/198], Step [100/199], Loss: 0.00817800872027874\n","Epoch [121/198], Step [100/199], Loss: 0.019476668909192085\n","Epoch [122/198], Step [100/199], Loss: 0.008394073694944382\n","Epoch [123/198], Step [100/199], Loss: 0.007157446350902319\n","Epoch [124/198], Step [100/199], Loss: 0.000619869155343622\n","Epoch [125/198], Step [100/199], Loss: 0.0022488157264888287\n","Epoch [126/198], Step [100/199], Loss: 0.0010811877436935902\n","Epoch [127/198], Step [100/199], Loss: 0.004517766181379557\n","Epoch [128/198], Step [100/199], Loss: 0.003611278720200062\n","Epoch [129/198], Step [100/199], Loss: 0.0042132181115448475\n","Epoch [130/198], Step [100/199], Loss: 0.000427908671554178\n","Epoch [131/198], Step [100/199], Loss: 0.0006472912500612438\n","Epoch [132/198], Step [100/199], Loss: 0.0007734971004538238\n","Epoch [133/198], Step [100/199], Loss: 0.007626272272318602\n","Epoch [134/198], Step [100/199], Loss: 0.0015965582570061088\n","Epoch [135/198], Step [100/199], Loss: 0.031154053285717964\n","Epoch [136/198], Step [100/199], Loss: 0.009317023679614067\n","Epoch [137/198], Step [100/199], Loss: 0.001276693888939917\n","Epoch [138/198], Step [100/199], Loss: 0.0016460075275972486\n","Epoch [139/198], Step [100/199], Loss: 0.00868937000632286\n","Epoch [140/198], Step [100/199], Loss: 0.03291267901659012\n","Epoch [141/198], Step [100/199], Loss: 0.006463626399636269\n","Epoch [142/198], Step [100/199], Loss: 0.022351006045937538\n","Epoch [143/198], Step [100/199], Loss: 0.005827571265399456\n","Epoch [144/198], Step [100/199], Loss: 0.005594116169959307\n","Epoch [145/198], Step [100/199], Loss: 0.007724030874669552\n","Epoch [146/198], Step [100/199], Loss: 0.004022884648293257\n","Epoch [147/198], Step [100/199], Loss: 0.0015991670079529285\n","Epoch [148/198], Step [100/199], Loss: 0.0007345945341512561\n","Epoch [149/198], Step [100/199], Loss: 0.00030166469514369965\n","Epoch [150/198], Step [100/199], Loss: 0.00035167665919288993\n","Epoch [151/198], Step [100/199], Loss: 0.00117214594502002\n","Epoch [152/198], Step [100/199], Loss: 0.0027217953465878963\n","Epoch [153/198], Step [100/199], Loss: 0.0012181948404759169\n","Epoch [154/198], Step [100/199], Loss: 0.0016511559952050447\n","Epoch [155/198], Step [100/199], Loss: 0.001252034679055214\n","Epoch [156/198], Step [100/199], Loss: 0.0011126423487439752\n","Epoch [157/198], Step [100/199], Loss: 0.000638791243545711\n","Epoch [158/198], Step [100/199], Loss: 0.017368219792842865\n","Epoch [159/198], Step [100/199], Loss: 0.000686395971570164\n","Epoch [160/198], Step [100/199], Loss: 0.008796723559498787\n","Epoch [161/198], Step [100/199], Loss: 0.0008479410316795111\n","Epoch [162/198], Step [100/199], Loss: 0.0003303022822365165\n","Epoch [163/198], Step [100/199], Loss: 0.0013758374843746424\n","Epoch [164/198], Step [100/199], Loss: 0.0038577690720558167\n","Epoch [165/198], Step [100/199], Loss: 0.01374015025794506\n","Epoch [166/198], Step [100/199], Loss: 0.00034138766932301223\n","Epoch [167/198], Step [100/199], Loss: 0.0012249201536178589\n","Epoch [168/198], Step [100/199], Loss: 0.0006834152736701071\n","Epoch [169/198], Step [100/199], Loss: 0.00027337303617969155\n","Epoch [170/198], Step [100/199], Loss: 0.020850755274295807\n","Epoch [171/198], Step [100/199], Loss: 0.004127857740968466\n","Epoch [172/198], Step [100/199], Loss: 0.06047246977686882\n","Epoch [173/198], Step [100/199], Loss: 0.002947803819552064\n","Epoch [174/198], Step [100/199], Loss: 0.001237318036146462\n","Epoch [175/198], Step [100/199], Loss: 0.0007437031599693\n","Epoch [176/198], Step [100/199], Loss: 0.00013572962780017406\n","Epoch [177/198], Step [100/199], Loss: 0.00010426861263113096\n","Epoch [178/198], Step [100/199], Loss: 0.00020274918642826378\n","Epoch [179/198], Step [100/199], Loss: 8.887345757102594e-05\n","Epoch [180/198], Step [100/199], Loss: 7.12822875357233e-05\n","Epoch [181/198], Step [100/199], Loss: 0.00012112783588236198\n","Epoch [182/198], Step [100/199], Loss: 7.374095730483532e-05\n","Epoch [183/198], Step [100/199], Loss: 9.007484186440706e-05\n","Epoch [184/198], Step [100/199], Loss: 5.0835380534408614e-05\n","Epoch [185/198], Step [100/199], Loss: 2.850923738151323e-05\n","Epoch [186/198], Step [100/199], Loss: 4.308187999413349e-05\n","Epoch [187/198], Step [100/199], Loss: 9.031149966176599e-05\n","Epoch [188/198], Step [100/199], Loss: 5.070974657428451e-05\n","Epoch [189/198], Step [100/199], Loss: 2.9436871045618318e-05\n","Epoch [190/198], Step [100/199], Loss: 2.25190869969083e-05\n","Epoch [191/198], Step [100/199], Loss: 1.6718346159905195e-05\n","Epoch [192/198], Step [100/199], Loss: 1.6338939531124197e-05\n","Epoch [193/198], Step [100/199], Loss: 7.98063410911709e-06\n","Epoch [194/198], Step [100/199], Loss: 2.080130434478633e-05\n","Epoch [195/198], Step [100/199], Loss: 8.638125109428074e-06\n","Epoch [196/198], Step [100/199], Loss: 2.2619011360802688e-05\n","Epoch [197/198], Step [100/199], Loss: 7.124157036741963e-06\n","Epoch [198/198], Step [100/199], Loss: 3.0527121452905703e-06\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HEPQmdxifVF-","executionInfo":{"elapsed":1038,"status":"ok","timestamp":1623742539569,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"6e64dfba-8de1-4154-a144-0ab0b8064b7e"},"source":["# Test the model\n","test_dataset = CustomDataset(X_test_0721, y_test_0721)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=50, shuffle=False)\n","\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (images, labels) in enumerate(test_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        if i==0:\n","          y_pre = predicted.tolist()\n","        else:\n","          y_pre.extend(predicted.tolist())\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","\n","torch.save(model.state_dict(), 'model.ckpt')\n","\n","target_names = ['nominal', 'R0.3', 'L0.9', 'L0.8','L0.7', 'L0.6', 'L0.5', 'L0.4', 'L0.3']\n","print(classification_report(y_test_0721.tolist(), y_pre, target_names=target_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 100.0\n","Iteration:1,  Accuracy: 100.0\n","Iteration:2,  Accuracy: 100.0\n","Iteration:3,  Accuracy: 100.0\n","Iteration:4,  Accuracy: 100.0\n","Iteration:5,  Accuracy: 100.0\n","Iteration:6,  Accuracy: 100.0\n","Iteration:7,  Accuracy: 100.0\n","Iteration:8,  Accuracy: 100.0\n","Iteration:9,  Accuracy: 100.0\n","Iteration:10,  Accuracy: 100.0\n","Iteration:11,  Accuracy: 100.0\n","Iteration:12,  Accuracy: 100.0\n","Iteration:13,  Accuracy: 100.0\n","Iteration:14,  Accuracy: 100.0\n","Iteration:15,  Accuracy: 100.0\n","Iteration:16,  Accuracy: 100.0\n","Iteration:17,  Accuracy: 100.0\n","Iteration:18,  Accuracy: 100.0\n","Iteration:19,  Accuracy: 99.9\n","Iteration:20,  Accuracy: 99.9047619047619\n","Iteration:21,  Accuracy: 99.9090909090909\n","Iteration:22,  Accuracy: 99.91304347826087\n","Iteration:23,  Accuracy: 99.91666666666667\n","Iteration:24,  Accuracy: 99.84\n","Iteration:25,  Accuracy: 99.84615384615384\n","Iteration:26,  Accuracy: 99.85185185185185\n","Iteration:27,  Accuracy: 99.85714285714286\n","Iteration:28,  Accuracy: 99.79310344827586\n","Iteration:29,  Accuracy: 99.8\n","Iteration:30,  Accuracy: 99.80645161290323\n","Iteration:31,  Accuracy: 99.8125\n","Iteration:32,  Accuracy: 99.81818181818181\n","Iteration:33,  Accuracy: 99.82352941176471\n","Iteration:34,  Accuracy: 99.82857142857142\n","Iteration:35,  Accuracy: 99.83333333333333\n","Iteration:36,  Accuracy: 99.83783783783784\n","Iteration:37,  Accuracy: 99.84210526315789\n","Iteration:38,  Accuracy: 99.84615384615384\n","Iteration:39,  Accuracy: 99.85\n","Iteration:40,  Accuracy: 99.85365853658537\n","Iteration:41,  Accuracy: 99.80952380952381\n","Iteration:42,  Accuracy: 99.81395348837209\n","Iteration:43,  Accuracy: 99.81818181818181\n","Iteration:44,  Accuracy: 99.82222222222222\n","Iteration:45,  Accuracy: 99.82608695652173\n","Iteration:46,  Accuracy: 99.82978723404256\n","Iteration:47,  Accuracy: 99.83333333333333\n","Iteration:48,  Accuracy: 99.83673469387755\n","Iteration:49,  Accuracy: 99.8\n","Iteration:50,  Accuracy: 99.80392156862744\n","Iteration:51,  Accuracy: 99.8076923076923\n","Iteration:52,  Accuracy: 99.81132075471699\n","Iteration:53,  Accuracy: 99.81481481481481\n","Iteration:54,  Accuracy: 99.81818181818181\n","Iteration:55,  Accuracy: 99.82142857142857\n","Iteration:56,  Accuracy: 99.82456140350877\n","Iteration:57,  Accuracy: 99.79310344827586\n","Iteration:58,  Accuracy: 99.79661016949153\n","Iteration:59,  Accuracy: 99.8\n","Iteration:60,  Accuracy: 99.80327868852459\n","Iteration:61,  Accuracy: 99.80645161290323\n","Iteration:62,  Accuracy: 99.80952380952381\n","Iteration:63,  Accuracy: 99.78125\n","Iteration:64,  Accuracy: 99.78461538461538\n","Iteration:65,  Accuracy: 99.78787878787878\n","Iteration:66,  Accuracy: 99.7910447761194\n","Iteration:67,  Accuracy: 99.79411764705883\n","Iteration:68,  Accuracy: 99.79710144927536\n","Iteration:69,  Accuracy: 99.8\n","Iteration:70,  Accuracy: 99.77464788732394\n","Iteration:71,  Accuracy: 99.77777777777777\n","Iteration:72,  Accuracy: 99.75342465753425\n","Iteration:73,  Accuracy: 99.75675675675676\n","Iteration:74,  Accuracy: 99.76\n","Iteration:75,  Accuracy: 99.76315789473684\n","Iteration:76,  Accuracy: 99.76623376623377\n","Iteration:77,  Accuracy: 99.76923076923077\n","Iteration:78,  Accuracy: 99.77215189873418\n","Iteration:79,  Accuracy: 99.775\n","Iteration:80,  Accuracy: 99.75308641975309\n","Iteration:81,  Accuracy: 99.7560975609756\n","Iteration:82,  Accuracy: 99.75903614457832\n","Iteration:83,  Accuracy: 99.73809523809524\n","Iteration:84,  Accuracy: 99.74117647058823\n","Iteration:85,  Accuracy: 99.74418604651163\n","Iteration:86,  Accuracy: 99.74712643678161\n","Iteration:87,  Accuracy: 99.75\n","Iteration:88,  Accuracy: 99.73033707865169\n","Iteration:89,  Accuracy: 99.73333333333333\n","Iteration:90,  Accuracy: 99.73626373626374\n","Iteration:91,  Accuracy: 99.71739130434783\n","Iteration:92,  Accuracy: 99.72043010752688\n","Iteration:93,  Accuracy: 99.72340425531915\n","Iteration:94,  Accuracy: 99.72631578947369\n","Iteration:95,  Accuracy: 99.72916666666667\n","Iteration:96,  Accuracy: 99.73195876288659\n","Iteration:97,  Accuracy: 99.73469387755102\n","Iteration:98,  Accuracy: 99.73737373737374\n","Iteration:99,  Accuracy: 99.74\n","Iteration:100,  Accuracy: 99.74257425742574\n","Iteration:101,  Accuracy: 99.74509803921569\n","Iteration:102,  Accuracy: 99.72815533980582\n","Iteration:103,  Accuracy: 99.6923076923077\n","Iteration:104,  Accuracy: 99.6952380952381\n","Iteration:105,  Accuracy: 99.69811320754717\n","Iteration:106,  Accuracy: 99.70093457943925\n","Iteration:107,  Accuracy: 99.70370370370371\n","Iteration:108,  Accuracy: 99.70642201834862\n","Iteration:109,  Accuracy: 99.7090909090909\n","Iteration:110,  Accuracy: 99.71171171171171\n","Iteration:111,  Accuracy: 99.71428571428571\n","Iteration:112,  Accuracy: 99.71681415929204\n","Iteration:113,  Accuracy: 99.71929824561404\n","Iteration:114,  Accuracy: 99.70434782608696\n","Iteration:115,  Accuracy: 99.6896551724138\n","Iteration:116,  Accuracy: 99.6923076923077\n","Iteration:117,  Accuracy: 99.69491525423729\n","Iteration:118,  Accuracy: 99.69747899159664\n","Iteration:119,  Accuracy: 99.69823973176865\n","Test Accuracy of the model: 99.69823973176865 %\n","              precision    recall  f1-score   support\n","\n","     nominal       1.00      1.00      1.00      2944\n","        R0.3       0.99      1.00      0.99       485\n","        L0.9       0.99      0.99      0.99       295\n","        L0.8       0.99      1.00      1.00       299\n","        L0.7       1.00      0.99      1.00       299\n","        L0.6       1.00      1.00      1.00       311\n","        L0.5       1.00      0.99      1.00       306\n","        L0.4       0.99      0.99      0.99       323\n","        L0.3       0.99      1.00      1.00       703\n","\n","    accuracy                           1.00      5965\n","   macro avg       1.00      1.00      1.00      5965\n","weighted avg       1.00      1.00      1.00      5965\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CZu70hjKt5Io"},"source":["### 6. 21일 학습 결과로 23일 테스트\n","##### 논문 결과: 48%"]},{"cell_type":"code","metadata":{"id":"eAthWGSAuAzX","executionInfo":{"status":"ok","timestamp":1624277207243,"user_tz":-540,"elapsed":899,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}}},"source":["X_train = np.concatenate(( X_train_0721, X_test_0721), axis=0)\n","y_train = np.concatenate(( y_train_0721, y_test_0721), axis=0)\n","X_test = np.concatenate(( X_train_0723, X_test_0723), axis=0)\n","y_test = np.concatenate((y_train_0723, y_test_0723), axis=0)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRqUADKhgoS5","executionInfo":{"status":"ok","timestamp":1624277211061,"user_tz":-540,"elapsed":5,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"6d06e20e-f54b-45bc-f60e-91cf6b5a78cf"},"source":["print(y_train.shape)\n","print(y_test.shape)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["(29821,)\n","(32267,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l5n9oA5XmXiY","executionInfo":{"status":"ok","timestamp":1624278918624,"user_tz":-540,"elapsed":1691635,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"94e9ee4e-1be3-4668-fc7e-4932e3555fc2"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 9\n","batch_size = 160\n","num_epochs = len(y_train) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train, y_train)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=160, shuffle=True)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Epoch [1/186], Step [100/187], Loss: 0.5315901041030884\n","Epoch [2/186], Step [100/187], Loss: 0.20726588368415833\n","Epoch [3/186], Step [100/187], Loss: 0.10642541944980621\n","Epoch [4/186], Step [100/187], Loss: 0.03837452828884125\n","Epoch [5/186], Step [100/187], Loss: 0.03140440583229065\n","Epoch [6/186], Step [100/187], Loss: 0.04022841900587082\n","Epoch [7/186], Step [100/187], Loss: 0.012049923650920391\n","Epoch [8/186], Step [100/187], Loss: 0.03550467640161514\n","Epoch [9/186], Step [100/187], Loss: 0.002751746214926243\n","Epoch [10/186], Step [100/187], Loss: 0.025552207604050636\n","Epoch [11/186], Step [100/187], Loss: 0.09670872986316681\n","Epoch [12/186], Step [100/187], Loss: 0.0039201779291033745\n","Epoch [13/186], Step [100/187], Loss: 0.0014876448549330235\n","Epoch [14/186], Step [100/187], Loss: 0.0016998080536723137\n","Epoch [15/186], Step [100/187], Loss: 0.061335571110248566\n","Epoch [16/186], Step [100/187], Loss: 0.044676922261714935\n","Epoch [17/186], Step [100/187], Loss: 0.00777738681063056\n","Epoch [18/186], Step [100/187], Loss: 0.004074714612215757\n","Epoch [19/186], Step [100/187], Loss: 0.004799161572009325\n","Epoch [20/186], Step [100/187], Loss: 0.0008065470610745251\n","Epoch [21/186], Step [100/187], Loss: 0.00039907367317937315\n","Epoch [22/186], Step [100/187], Loss: 0.06214182451367378\n","Epoch [23/186], Step [100/187], Loss: 0.0016297399997711182\n","Epoch [24/186], Step [100/187], Loss: 0.00782813411206007\n","Epoch [25/186], Step [100/187], Loss: 0.0069071040488779545\n","Epoch [26/186], Step [100/187], Loss: 0.015212884172797203\n","Epoch [27/186], Step [100/187], Loss: 0.009439658373594284\n","Epoch [28/186], Step [100/187], Loss: 0.015592996962368488\n","Epoch [29/186], Step [100/187], Loss: 0.0009231100557371974\n","Epoch [30/186], Step [100/187], Loss: 0.03840271383523941\n","Epoch [31/186], Step [100/187], Loss: 0.0015248627169057727\n","Epoch [32/186], Step [100/187], Loss: 0.017223652452230453\n","Epoch [33/186], Step [100/187], Loss: 0.00955157820135355\n","Epoch [34/186], Step [100/187], Loss: 0.0014962968416512012\n","Epoch [35/186], Step [100/187], Loss: 0.011995435692369938\n","Epoch [36/186], Step [100/187], Loss: 0.0006361448904499412\n","Epoch [37/186], Step [100/187], Loss: 0.003018645802512765\n","Epoch [38/186], Step [100/187], Loss: 0.0013692781794816256\n","Epoch [39/186], Step [100/187], Loss: 0.00023347891692537814\n","Epoch [40/186], Step [100/187], Loss: 8.924669964471832e-05\n","Epoch [41/186], Step [100/187], Loss: 4.053106385981664e-05\n","Epoch [42/186], Step [100/187], Loss: 0.0006166653474792838\n","Epoch [43/186], Step [100/187], Loss: 0.02785501815378666\n","Epoch [44/186], Step [100/187], Loss: 0.0018761660903692245\n","Epoch [45/186], Step [100/187], Loss: 0.007793080992996693\n","Epoch [46/186], Step [100/187], Loss: 0.0002808717545121908\n","Epoch [47/186], Step [100/187], Loss: 0.001579703064635396\n","Epoch [48/186], Step [100/187], Loss: 6.644318636972457e-05\n","Epoch [49/186], Step [100/187], Loss: 0.00029218540294095874\n","Epoch [50/186], Step [100/187], Loss: 6.570738332811743e-05\n","Epoch [51/186], Step [100/187], Loss: 2.201905954279937e-05\n","Epoch [52/186], Step [100/187], Loss: 2.087001303152647e-05\n","Epoch [53/186], Step [100/187], Loss: 0.00028776604449376464\n","Epoch [54/186], Step [100/187], Loss: 3.404262679396197e-05\n","Epoch [55/186], Step [100/187], Loss: 0.02945653535425663\n","Epoch [56/186], Step [100/187], Loss: 0.08593309670686722\n","Epoch [57/186], Step [100/187], Loss: 0.007183896843343973\n","Epoch [58/186], Step [100/187], Loss: 0.005234581418335438\n","Epoch [59/186], Step [100/187], Loss: 0.0008469355525448918\n","Epoch [60/186], Step [100/187], Loss: 0.00030868183239363134\n","Epoch [61/186], Step [100/187], Loss: 0.00018265508697368205\n","Epoch [62/186], Step [100/187], Loss: 5.611767846858129e-05\n","Epoch [63/186], Step [100/187], Loss: 5.619949661195278e-05\n","Epoch [64/186], Step [100/187], Loss: 0.0003534097340889275\n","Epoch [65/186], Step [100/187], Loss: 3.6765082768397406e-05\n","Epoch [66/186], Step [100/187], Loss: 0.00012355597573332489\n","Epoch [67/186], Step [100/187], Loss: 2.275711995025631e-05\n","Epoch [68/186], Step [100/187], Loss: 1.652137507335283e-05\n","Epoch [69/186], Step [100/187], Loss: 5.6539174693170935e-05\n","Epoch [70/186], Step [100/187], Loss: 1.1947902748943307e-05\n","Epoch [71/186], Step [100/187], Loss: 0.03567691892385483\n","Epoch [72/186], Step [100/187], Loss: 0.002819611458107829\n","Epoch [73/186], Step [100/187], Loss: 0.005570264998823404\n","Epoch [74/186], Step [100/187], Loss: 0.0009902006713673472\n","Epoch [75/186], Step [100/187], Loss: 0.008097928017377853\n","Epoch [76/186], Step [100/187], Loss: 0.017622318118810654\n","Epoch [77/186], Step [100/187], Loss: 0.0012865059543401003\n","Epoch [78/186], Step [100/187], Loss: 0.004173263441771269\n","Epoch [79/186], Step [100/187], Loss: 0.03411856293678284\n","Epoch [80/186], Step [100/187], Loss: 0.0003548190579749644\n","Epoch [81/186], Step [100/187], Loss: 0.00011379329953342676\n","Epoch [82/186], Step [100/187], Loss: 0.00021134103008080274\n","Epoch [83/186], Step [100/187], Loss: 0.04604116082191467\n","Epoch [84/186], Step [100/187], Loss: 0.005434271413832903\n","Epoch [85/186], Step [100/187], Loss: 0.011433047242462635\n","Epoch [86/186], Step [100/187], Loss: 0.0005323803052306175\n","Epoch [87/186], Step [100/187], Loss: 0.004423261154443026\n","Epoch [88/186], Step [100/187], Loss: 0.00092700234381482\n","Epoch [89/186], Step [100/187], Loss: 0.0006829715566709638\n","Epoch [90/186], Step [100/187], Loss: 0.059411682188510895\n","Epoch [91/186], Step [100/187], Loss: 0.0056800516322255135\n","Epoch [92/186], Step [100/187], Loss: 0.0005866097053512931\n","Epoch [93/186], Step [100/187], Loss: 0.0053766039200127125\n","Epoch [94/186], Step [100/187], Loss: 0.0026445058174431324\n","Epoch [95/186], Step [100/187], Loss: 0.00012134746793890372\n","Epoch [96/186], Step [100/187], Loss: 0.0009790734620764852\n","Epoch [97/186], Step [100/187], Loss: 4.827092925552279e-05\n","Epoch [98/186], Step [100/187], Loss: 0.00010009920515585691\n","Epoch [99/186], Step [100/187], Loss: 0.0004615162906702608\n","Epoch [100/186], Step [100/187], Loss: 0.03197908774018288\n","Epoch [101/186], Step [100/187], Loss: 0.003550733672454953\n","Epoch [102/186], Step [100/187], Loss: 0.06675839424133301\n","Epoch [103/186], Step [100/187], Loss: 0.014125265181064606\n","Epoch [104/186], Step [100/187], Loss: 0.000663402839563787\n","Epoch [105/186], Step [100/187], Loss: 0.0024231127463281155\n","Epoch [106/186], Step [100/187], Loss: 0.0003943530027754605\n","Epoch [107/186], Step [100/187], Loss: 0.0001326313940808177\n","Epoch [108/186], Step [100/187], Loss: 0.0024967051576822996\n","Epoch [109/186], Step [100/187], Loss: 0.008790270425379276\n","Epoch [110/186], Step [100/187], Loss: 0.01521213073283434\n","Epoch [111/186], Step [100/187], Loss: 0.005303824786096811\n","Epoch [112/186], Step [100/187], Loss: 0.003066068748012185\n","Epoch [113/186], Step [100/187], Loss: 0.04772736877202988\n","Epoch [114/186], Step [100/187], Loss: 0.007866738364100456\n","Epoch [115/186], Step [100/187], Loss: 0.0008169268257915974\n","Epoch [116/186], Step [100/187], Loss: 0.0023630186915397644\n","Epoch [117/186], Step [100/187], Loss: 0.0007539601065218449\n","Epoch [118/186], Step [100/187], Loss: 0.0003219840000383556\n","Epoch [119/186], Step [100/187], Loss: 0.0006238628411665559\n","Epoch [120/186], Step [100/187], Loss: 0.00011393478780519217\n","Epoch [121/186], Step [100/187], Loss: 0.0012211475986987352\n","Epoch [122/186], Step [100/187], Loss: 0.00995640642940998\n","Epoch [123/186], Step [100/187], Loss: 0.0003103869385086\n","Epoch [124/186], Step [100/187], Loss: 0.0015638504410162568\n","Epoch [125/186], Step [100/187], Loss: 0.0016778178978711367\n","Epoch [126/186], Step [100/187], Loss: 0.021152587607502937\n","Epoch [127/186], Step [100/187], Loss: 0.04701096564531326\n","Epoch [128/186], Step [100/187], Loss: 0.008903611451387405\n","Epoch [129/186], Step [100/187], Loss: 0.0008319313637912273\n","Epoch [130/186], Step [100/187], Loss: 0.0007568365545012057\n","Epoch [131/186], Step [100/187], Loss: 0.0010463518556207418\n","Epoch [132/186], Step [100/187], Loss: 0.00016656843945384026\n","Epoch [133/186], Step [100/187], Loss: 0.00910487025976181\n","Epoch [134/186], Step [100/187], Loss: 0.030475810170173645\n","Epoch [135/186], Step [100/187], Loss: 0.0010609463788568974\n","Epoch [136/186], Step [100/187], Loss: 0.0023043067194521427\n","Epoch [137/186], Step [100/187], Loss: 0.06887367367744446\n","Epoch [138/186], Step [100/187], Loss: 0.014948109164834023\n","Epoch [139/186], Step [100/187], Loss: 0.0018243225058540702\n","Epoch [140/186], Step [100/187], Loss: 0.00012226332910358906\n","Epoch [141/186], Step [100/187], Loss: 0.0003422435256652534\n","Epoch [142/186], Step [100/187], Loss: 0.00033728330163285136\n","Epoch [143/186], Step [100/187], Loss: 0.0015983961056917906\n","Epoch [144/186], Step [100/187], Loss: 0.00010720853606471792\n","Epoch [145/186], Step [100/187], Loss: 0.0011703850468620658\n","Epoch [146/186], Step [100/187], Loss: 0.02797604724764824\n","Epoch [147/186], Step [100/187], Loss: 0.0029696132987737656\n","Epoch [148/186], Step [100/187], Loss: 0.007575803902000189\n","Epoch [149/186], Step [100/187], Loss: 0.0002061006089206785\n","Epoch [150/186], Step [100/187], Loss: 0.000221868438529782\n","Epoch [151/186], Step [100/187], Loss: 0.00015143649943638593\n","Epoch [152/186], Step [100/187], Loss: 0.0002203235198976472\n","Epoch [153/186], Step [100/187], Loss: 0.00033969577634707093\n","Epoch [154/186], Step [100/187], Loss: 0.00033048022305592895\n","Epoch [155/186], Step [100/187], Loss: 2.802427661663387e-05\n","Epoch [156/186], Step [100/187], Loss: 4.561964306049049e-05\n","Epoch [157/186], Step [100/187], Loss: 2.1418003598228097e-05\n","Epoch [158/186], Step [100/187], Loss: 1.8205293599748984e-05\n","Epoch [159/186], Step [100/187], Loss: 4.502734373090789e-05\n","Epoch [160/186], Step [100/187], Loss: 1.635112494113855e-05\n","Epoch [161/186], Step [100/187], Loss: 8.840166628942825e-06\n","Epoch [162/186], Step [100/187], Loss: 9.947902071871795e-06\n","Epoch [163/186], Step [100/187], Loss: 3.121972986264154e-05\n","Epoch [164/186], Step [100/187], Loss: 1.3336064512259327e-05\n","Epoch [165/186], Step [100/187], Loss: 1.2582936506078113e-05\n","Epoch [166/186], Step [100/187], Loss: 1.9773690382862696e-06\n","Epoch [167/186], Step [100/187], Loss: 5.020768185204361e-06\n","Epoch [168/186], Step [100/187], Loss: 1.0800912605191115e-05\n","Epoch [169/186], Step [100/187], Loss: 7.337418992392486e-06\n","Epoch [170/186], Step [100/187], Loss: 1.978793807211332e-05\n","Epoch [171/186], Step [100/187], Loss: 9.290050002164207e-06\n","Epoch [172/186], Step [100/187], Loss: 8.878189873939846e-06\n","Epoch [173/186], Step [100/187], Loss: 2.9354482649068814e-06\n","Epoch [174/186], Step [100/187], Loss: 3.27587554238562e-06\n","Epoch [175/186], Step [100/187], Loss: 1.2061934285156894e-05\n","Epoch [176/186], Step [100/187], Loss: 2.453422439430142e-06\n","Epoch [177/186], Step [100/187], Loss: 1.4139523045741953e-05\n","Epoch [178/186], Step [100/187], Loss: 5.9012500059907325e-06\n","Epoch [179/186], Step [100/187], Loss: 2.5762433324416634e-06\n","Epoch [180/186], Step [100/187], Loss: 2.811015747283818e-06\n","Epoch [181/186], Step [100/187], Loss: 4.18740501118009e-06\n","Epoch [182/186], Step [100/187], Loss: 1.3165100654077833e-06\n","Epoch [183/186], Step [100/187], Loss: 2.545759116401314e-06\n","Epoch [184/186], Step [100/187], Loss: 2.696112460398581e-06\n","Epoch [185/186], Step [100/187], Loss: 1.3149960977898445e-06\n","Epoch [186/186], Step [100/187], Loss: 1.1861027360282606e-06\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NAy2Urc6mkv4","executionInfo":{"status":"ok","timestamp":1624278922922,"user_tz":-540,"elapsed":4305,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"d2df4428-ae3a-4ba1-b724-bfc6d160fec8"},"source":["# Test the model\n","test_dataset = CustomDataset(X_test, y_test)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=190, shuffle=False)\n","\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (images, labels) in enumerate(test_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        if i==0:\n","          y_pre = predicted.tolist()\n","        else:\n","          y_pre.extend(predicted.tolist())\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","\n","torch.save(model.state_dict(), 'model.ckpt')\n","\n","target_names = ['nominal', 'R0.3', 'L0.9', 'L0.8','L0.7', 'L0.6', 'L0.5', 'L0.4', 'L0.3']\n","print(classification_report(y_test.tolist(), y_pre, target_names=target_names))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 72.63157894736842\n","Iteration:1,  Accuracy: 74.47368421052632\n","Iteration:2,  Accuracy: 74.3859649122807\n","Iteration:3,  Accuracy: 74.73684210526316\n","Iteration:4,  Accuracy: 75.15789473684211\n","Iteration:5,  Accuracy: 75.35087719298245\n","Iteration:6,  Accuracy: 75.93984962406014\n","Iteration:7,  Accuracy: 76.05263157894737\n","Iteration:8,  Accuracy: 76.3157894736842\n","Iteration:9,  Accuracy: 76.3157894736842\n","Iteration:10,  Accuracy: 76.45933014354067\n","Iteration:11,  Accuracy: 76.00877192982456\n","Iteration:12,  Accuracy: 75.66801619433198\n","Iteration:13,  Accuracy: 74.81203007518798\n","Iteration:14,  Accuracy: 74.49122807017544\n","Iteration:15,  Accuracy: 74.83552631578948\n","Iteration:16,  Accuracy: 74.6749226006192\n","Iteration:17,  Accuracy: 74.50292397660819\n","Iteration:18,  Accuracy: 74.62603878116343\n","Iteration:19,  Accuracy: 74.65789473684211\n","Iteration:20,  Accuracy: 74.66165413533835\n","Iteration:21,  Accuracy: 74.5933014354067\n","Iteration:22,  Accuracy: 74.62242562929062\n","Iteration:23,  Accuracy: 74.75877192982456\n","Iteration:24,  Accuracy: 74.67368421052632\n","Iteration:25,  Accuracy: 74.7165991902834\n","Iteration:26,  Accuracy: 74.6588693957115\n","Iteration:27,  Accuracy: 74.62406015037594\n","Iteration:28,  Accuracy: 74.68239564428312\n","Iteration:29,  Accuracy: 74.47368421052632\n","Iteration:30,  Accuracy: 74.32937181663837\n","Iteration:31,  Accuracy: 74.29276315789474\n","Iteration:32,  Accuracy: 74.274322169059\n","Iteration:33,  Accuracy: 74.14860681114551\n","Iteration:34,  Accuracy: 74.06015037593986\n","Iteration:35,  Accuracy: 73.9766081871345\n","Iteration:36,  Accuracy: 73.84068278805121\n","Iteration:37,  Accuracy: 73.76731301939058\n","Iteration:38,  Accuracy: 73.7246963562753\n","Iteration:39,  Accuracy: 73.6842105263158\n","Iteration:40,  Accuracy: 73.77406931964056\n","Iteration:41,  Accuracy: 73.88471177944862\n","Iteration:42,  Accuracy: 73.85556915544676\n","Iteration:43,  Accuracy: 73.97129186602871\n","Iteration:44,  Accuracy: 74.0701754385965\n","Iteration:45,  Accuracy: 74.16475972540046\n","Iteration:46,  Accuracy: 74.12094064949608\n","Iteration:47,  Accuracy: 74.17763157894737\n","Iteration:48,  Accuracy: 74.16756176154672\n","Iteration:49,  Accuracy: 74.16842105263157\n","Iteration:50,  Accuracy: 74.08668730650155\n","Iteration:51,  Accuracy: 74.09919028340082\n","Iteration:52,  Accuracy: 73.94240317775571\n","Iteration:53,  Accuracy: 73.85964912280701\n","Iteration:54,  Accuracy: 73.90430622009569\n","Iteration:55,  Accuracy: 73.93796992481202\n","Iteration:56,  Accuracy: 74.02585410895661\n","Iteration:57,  Accuracy: 73.99274047186933\n","Iteration:58,  Accuracy: 73.98751115075825\n","Iteration:59,  Accuracy: 73.86842105263158\n","Iteration:60,  Accuracy: 73.87402933563416\n","Iteration:61,  Accuracy: 73.84550084889644\n","Iteration:62,  Accuracy: 73.85129490392649\n","Iteration:63,  Accuracy: 73.85690789473684\n","Iteration:64,  Accuracy: 73.77327935222672\n","Iteration:65,  Accuracy: 73.73205741626795\n","Iteration:66,  Accuracy: 73.74705420267085\n","Iteration:67,  Accuracy: 73.73839009287926\n","Iteration:68,  Accuracy: 73.79099923722349\n","Iteration:69,  Accuracy: 73.72932330827068\n","Iteration:70,  Accuracy: 73.76575240919199\n","Iteration:71,  Accuracy: 73.77923976608187\n","Iteration:72,  Accuracy: 73.78514780100937\n","Iteration:73,  Accuracy: 73.77667140825035\n","Iteration:74,  Accuracy: 73.69824561403509\n","Iteration:75,  Accuracy: 73.6842105263158\n","Iteration:76,  Accuracy: 73.6637047163363\n","Iteration:77,  Accuracy: 73.6842105263158\n","Iteration:78,  Accuracy: 73.75749500333112\n","Iteration:79,  Accuracy: 73.80263157894737\n","Iteration:80,  Accuracy: 73.78817413905134\n","Iteration:81,  Accuracy: 73.76765083440309\n","Iteration:82,  Accuracy: 73.70957514267597\n","Iteration:83,  Accuracy: 73.7593984962406\n","Iteration:84,  Accuracy: 73.77089783281734\n","Iteration:85,  Accuracy: 73.76376988984089\n","Iteration:86,  Accuracy: 73.7568058076225\n","Iteration:87,  Accuracy: 73.78588516746412\n","Iteration:88,  Accuracy: 73.84387936132465\n","Iteration:89,  Accuracy: 73.7953216374269\n","Iteration:90,  Accuracy: 73.85193753614806\n","Iteration:91,  Accuracy: 73.89016018306636\n","Iteration:92,  Accuracy: 73.87662705149971\n","Iteration:93,  Accuracy: 73.86338185890257\n","Iteration:94,  Accuracy: 73.8781163434903\n","Iteration:95,  Accuracy: 73.86513157894737\n","Iteration:96,  Accuracy: 73.82528486163864\n","Iteration:97,  Accuracy: 73.8077336197637\n","Iteration:98,  Accuracy: 73.81180223285486\n","Iteration:99,  Accuracy: 73.86315789473684\n","Iteration:100,  Accuracy: 73.90307451797811\n","Iteration:101,  Accuracy: 73.90092879256966\n","Iteration:102,  Accuracy: 73.90904445579969\n","Iteration:103,  Accuracy: 73.88663967611336\n","Iteration:104,  Accuracy: 73.8796992481203\n","Iteration:105,  Accuracy: 73.87785501489573\n","Iteration:106,  Accuracy: 73.87604525332021\n","Iteration:107,  Accuracy: 73.87914230019493\n","Iteration:108,  Accuracy: 73.90632544664413\n","Iteration:109,  Accuracy: 73.92344497607655\n","Iteration:110,  Accuracy: 73.95922238027501\n","Iteration:111,  Accuracy: 73.96146616541354\n","Iteration:112,  Accuracy: 73.9496972519795\n","Iteration:113,  Accuracy: 73.95198522622346\n","Iteration:114,  Accuracy: 73.93592677345538\n","Iteration:115,  Accuracy: 73.95644283121597\n","Iteration:116,  Accuracy: 73.96761133603239\n","Iteration:117,  Accuracy: 73.95628902765388\n","Iteration:118,  Accuracy: 73.94957983193277\n","Iteration:119,  Accuracy: 73.91666666666667\n","Iteration:120,  Accuracy: 73.89299695519792\n","Iteration:121,  Accuracy: 73.89991371872304\n","Iteration:122,  Accuracy: 73.8938810440736\n","Iteration:123,  Accuracy: 73.87096774193549\n","Iteration:124,  Accuracy: 73.85263157894737\n","Iteration:125,  Accuracy: 73.89306599832915\n","Iteration:126,  Accuracy: 73.87484459179444\n","Iteration:127,  Accuracy: 73.91036184210526\n","Iteration:128,  Accuracy: 73.95756833945329\n","Iteration:129,  Accuracy: 73.97975708502024\n","Iteration:130,  Accuracy: 73.98151868220168\n","Iteration:131,  Accuracy: 73.97129186602871\n","Iteration:132,  Accuracy: 73.99287692916502\n","Iteration:133,  Accuracy: 73.96700706991359\n","Iteration:134,  Accuracy: 73.94931773879142\n","Iteration:135,  Accuracy: 73.9783281733746\n","Iteration:136,  Accuracy: 74.00691509796388\n","Iteration:137,  Accuracy: 74.01983218916857\n","Iteration:138,  Accuracy: 73.97576675501703\n","Iteration:139,  Accuracy: 73.93609022556392\n","Iteration:140,  Accuracy: 73.94550205300486\n","Iteration:141,  Accuracy: 73.95478131949592\n","Iteration:142,  Accuracy: 73.93816709606183\n","Iteration:143,  Accuracy: 73.94371345029239\n","Iteration:144,  Accuracy: 73.94192377495463\n","Iteration:145,  Accuracy: 73.9293439077145\n","Iteration:146,  Accuracy: 73.94557823129252\n","Iteration:147,  Accuracy: 73.9651493598862\n","Iteration:148,  Accuracy: 73.97739314729778\n","Iteration:149,  Accuracy: 73.97543859649123\n","Iteration:150,  Accuracy: 73.98396653886371\n","Iteration:151,  Accuracy: 74.00969529085873\n","Iteration:152,  Accuracy: 74.01444788441692\n","Iteration:153,  Accuracy: 74.03622693096378\n","Iteration:154,  Accuracy: 74.03056027164686\n","Iteration:155,  Accuracy: 74.02159244264507\n","Iteration:156,  Accuracy: 74.0194435132417\n","Iteration:157,  Accuracy: 73.99733510992671\n","Iteration:158,  Accuracy: 74.00529625951671\n","Iteration:159,  Accuracy: 73.97697368421052\n","Iteration:160,  Accuracy: 73.96861719516181\n","Iteration:161,  Accuracy: 73.9766081871345\n","Iteration:162,  Accuracy: 73.99741685502099\n","Iteration:163,  Accuracy: 73.97304236200257\n","Iteration:164,  Accuracy: 73.96810207336523\n","Iteration:165,  Accuracy: 73.97590361445783\n","Iteration:166,  Accuracy: 74.0119760479042\n","Iteration:167,  Accuracy: 74.01002506265664\n","Iteration:168,  Accuracy: 74.0080971659919\n","Iteration:169,  Accuracy: 73.9858059317569\n","Test Accuracy of the model: 73.9858059317569 %\n","              precision    recall  f1-score   support\n","\n","     nominal       0.95      0.84      0.89     16935\n","        R0.3       0.88      0.69      0.77      2527\n","        L0.9       0.21      0.51      0.30      1431\n","        L0.8       0.45      0.64      0.53      1458\n","        L0.7       0.53      0.68      0.60      1481\n","        L0.6       0.73      0.51      0.60      1505\n","        L0.5       0.67      0.53      0.59      1549\n","        L0.4       0.44      0.31      0.37      1620\n","        L0.3       0.72      0.85      0.78      3761\n","\n","    accuracy                           0.74     32267\n","   macro avg       0.62      0.62      0.60     32267\n","weighted avg       0.79      0.74      0.76     32267\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WtqV-HH6awoK"},"source":["# GCS data 으로 성능 평가\n","## 1. Binary Classification"]},{"cell_type":"code","metadata":{"id":"sQztyhZ7I7Jj","executionInfo":{"status":"ok","timestamp":1624280127481,"user_tz":-540,"elapsed":7376,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}}},"source":["X_train_bin = np.load(\"X_train_bin.npy\")\n","y_train_bin = np.load(\"y_train_bin.npy\")\n","y_train_bin = y_train_bin.astype(np.int64)\n","X_test_bin = np.load(\"X_test_bin.npy\")\n","y_test_bin = np.load(\"y_test_bin.npy\")\n","y_test_bin = y_test_bin.astype(np.int64)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"icYwyQ4Ub_Pa","executionInfo":{"elapsed":927,"status":"ok","timestamp":1623793753556,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"},"user_tz":-540},"outputId":"8a82dfd1-bd9b-4474-95b4-d700b4c2aa0f"},"source":["print(y_train_bin.shape)\n","print(y_test_bin.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(171384,)\n","(42846,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"-Swn11_jbPfd","outputId":"f5c066fe-28eb-4996-b1ca-21441755c3bc"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 2\n","batch_size = 1000\n","num_epochs = len(y_train_bin) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train_bin, y_train_bin)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=1000, shuffle=True)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/171], Step [100/172], Loss: 0.31622278690338135\n","Epoch [2/171], Step [100/172], Loss: 0.3009490370750427\n","Epoch [3/171], Step [100/172], Loss: 0.28477317094802856\n","Epoch [4/171], Step [100/172], Loss: 0.2578229606151581\n","Epoch [5/171], Step [100/172], Loss: 0.21452702581882477\n","Epoch [6/171], Step [100/172], Loss: 0.1627504974603653\n","Epoch [7/171], Step [100/172], Loss: 0.127942755818367\n","Epoch [8/171], Step [100/172], Loss: 0.12482455372810364\n","Epoch [9/171], Step [100/172], Loss: 0.13061602413654327\n","Epoch [10/171], Step [100/172], Loss: 0.10844194144010544\n","Epoch [11/171], Step [100/172], Loss: 0.08855195343494415\n","Epoch [12/171], Step [100/172], Loss: 0.09990593791007996\n","Epoch [13/171], Step [100/172], Loss: 0.08401764184236526\n","Epoch [14/171], Step [100/172], Loss: 0.09484050422906876\n","Epoch [15/171], Step [100/172], Loss: 0.05528608709573746\n","Epoch [16/171], Step [100/172], Loss: 0.06260576099157333\n","Epoch [17/171], Step [100/172], Loss: 0.06557295471429825\n","Epoch [18/171], Step [100/172], Loss: 0.06275389343500137\n","Epoch [19/171], Step [100/172], Loss: 0.05137713998556137\n","Epoch [20/171], Step [100/172], Loss: 0.04513583704829216\n","Epoch [21/171], Step [100/172], Loss: 0.027470560744404793\n","Epoch [22/171], Step [100/172], Loss: 0.04992758855223656\n","Epoch [23/171], Step [100/172], Loss: 0.03664061054587364\n","Epoch [24/171], Step [100/172], Loss: 0.04327080771327019\n","Epoch [25/171], Step [100/172], Loss: 0.041331857442855835\n","Epoch [26/171], Step [100/172], Loss: 0.038720570504665375\n","Epoch [27/171], Step [100/172], Loss: 0.05189574137330055\n","Epoch [28/171], Step [100/172], Loss: 0.036921851336956024\n","Epoch [29/171], Step [100/172], Loss: 0.05481583625078201\n","Epoch [30/171], Step [100/172], Loss: 0.028727475553750992\n","Epoch [31/171], Step [100/172], Loss: 0.031928665935993195\n","Epoch [32/171], Step [100/172], Loss: 0.031126603484153748\n","Epoch [33/171], Step [100/172], Loss: 0.03455306217074394\n","Epoch [34/171], Step [100/172], Loss: 0.03285682201385498\n","Epoch [35/171], Step [100/172], Loss: 0.039709657430648804\n","Epoch [36/171], Step [100/172], Loss: 0.0484008826315403\n","Epoch [37/171], Step [100/172], Loss: 0.03175998106598854\n","Epoch [38/171], Step [100/172], Loss: 0.023218674585223198\n","Epoch [39/171], Step [100/172], Loss: 0.03171789273619652\n","Epoch [40/171], Step [100/172], Loss: 0.02772313356399536\n","Epoch [41/171], Step [100/172], Loss: 0.04041387513279915\n","Epoch [42/171], Step [100/172], Loss: 0.04725213721394539\n","Epoch [43/171], Step [100/172], Loss: 0.03545792028307915\n","Epoch [44/171], Step [100/172], Loss: 0.038984883576631546\n","Epoch [45/171], Step [100/172], Loss: 0.04510829597711563\n","Epoch [46/171], Step [100/172], Loss: 0.02458413876593113\n","Epoch [47/171], Step [100/172], Loss: 0.031274277716875076\n","Epoch [48/171], Step [100/172], Loss: 0.018171224743127823\n","Epoch [49/171], Step [100/172], Loss: 0.02828969620168209\n","Epoch [50/171], Step [100/172], Loss: 0.017072182148694992\n","Epoch [51/171], Step [100/172], Loss: 0.038794729858636856\n","Epoch [52/171], Step [100/172], Loss: 0.03319481387734413\n","Epoch [53/171], Step [100/172], Loss: 0.020067214965820312\n","Epoch [54/171], Step [100/172], Loss: 0.06226303428411484\n","Epoch [55/171], Step [100/172], Loss: 0.031060129404067993\n","Epoch [56/171], Step [100/172], Loss: 0.019374419003725052\n","Epoch [57/171], Step [100/172], Loss: 0.03678224980831146\n","Epoch [58/171], Step [100/172], Loss: 0.03260330855846405\n","Epoch [59/171], Step [100/172], Loss: 0.01556727197021246\n","Epoch [60/171], Step [100/172], Loss: 0.041658807545900345\n","Epoch [61/171], Step [100/172], Loss: 0.019163010641932487\n","Epoch [62/171], Step [100/172], Loss: 0.04639118164777756\n","Epoch [63/171], Step [100/172], Loss: 0.03291068226099014\n","Epoch [64/171], Step [100/172], Loss: 0.03611840307712555\n","Epoch [65/171], Step [100/172], Loss: 0.04463328421115875\n","Epoch [66/171], Step [100/172], Loss: 0.03605382889509201\n","Epoch [67/171], Step [100/172], Loss: 0.04472139850258827\n","Epoch [68/171], Step [100/172], Loss: 0.02994210459291935\n","Epoch [69/171], Step [100/172], Loss: 0.03194180503487587\n","Epoch [70/171], Step [100/172], Loss: 0.026599828153848648\n","Epoch [71/171], Step [100/172], Loss: 0.024015437811613083\n","Epoch [72/171], Step [100/172], Loss: 0.020265474915504456\n","Epoch [73/171], Step [100/172], Loss: 0.03774148225784302\n","Epoch [74/171], Step [100/172], Loss: 0.04374224320054054\n","Epoch [75/171], Step [100/172], Loss: 0.02227737195789814\n","Epoch [76/171], Step [100/172], Loss: 0.037354741245508194\n","Epoch [77/171], Step [100/172], Loss: 0.03524889796972275\n","Epoch [78/171], Step [100/172], Loss: 0.030556822195649147\n","Epoch [79/171], Step [100/172], Loss: 0.024952460080385208\n","Epoch [80/171], Step [100/172], Loss: 0.031344879418611526\n","Epoch [81/171], Step [100/172], Loss: 0.040840696543455124\n","Epoch [82/171], Step [100/172], Loss: 0.025808723643422127\n","Epoch [83/171], Step [100/172], Loss: 0.027657609432935715\n","Epoch [84/171], Step [100/172], Loss: 0.06933511048555374\n","Epoch [85/171], Step [100/172], Loss: 0.04044413939118385\n","Epoch [86/171], Step [100/172], Loss: 0.029737094417214394\n","Epoch [87/171], Step [100/172], Loss: 0.03228874132037163\n","Epoch [88/171], Step [100/172], Loss: 0.024271877482533455\n","Epoch [89/171], Step [100/172], Loss: 0.015008877962827682\n","Epoch [90/171], Step [100/172], Loss: 0.01538724172860384\n","Epoch [91/171], Step [100/172], Loss: 0.021890854462981224\n","Epoch [92/171], Step [100/172], Loss: 0.035105954855680466\n","Epoch [93/171], Step [100/172], Loss: 0.06259386241436005\n","Epoch [94/171], Step [100/172], Loss: 0.02127198874950409\n","Epoch [95/171], Step [100/172], Loss: 0.030737275257706642\n","Epoch [96/171], Step [100/172], Loss: 0.023864571005105972\n","Epoch [97/171], Step [100/172], Loss: 0.03680308535695076\n","Epoch [98/171], Step [100/172], Loss: 0.02965574525296688\n","Epoch [99/171], Step [100/172], Loss: 0.05789618566632271\n","Epoch [100/171], Step [100/172], Loss: 0.020963089540600777\n","Epoch [101/171], Step [100/172], Loss: 0.025174440816044807\n","Epoch [102/171], Step [100/172], Loss: 0.027089962735772133\n","Epoch [103/171], Step [100/172], Loss: 0.04504392296075821\n","Epoch [104/171], Step [100/172], Loss: 0.024844717234373093\n","Epoch [105/171], Step [100/172], Loss: 0.01964092068374157\n","Epoch [106/171], Step [100/172], Loss: 0.022007061168551445\n","Epoch [107/171], Step [100/172], Loss: 0.015764296054840088\n","Epoch [108/171], Step [100/172], Loss: 0.013688495382666588\n","Epoch [109/171], Step [100/172], Loss: 0.02302270196378231\n","Epoch [110/171], Step [100/172], Loss: 0.03232523798942566\n","Epoch [111/171], Step [100/172], Loss: 0.02920813485980034\n","Epoch [112/171], Step [100/172], Loss: 0.04308518022298813\n","Epoch [113/171], Step [100/172], Loss: 0.023469598963856697\n","Epoch [114/171], Step [100/172], Loss: 0.042602356523275375\n","Epoch [115/171], Step [100/172], Loss: 0.11269037425518036\n","Epoch [116/171], Step [100/172], Loss: 0.06489359587430954\n","Epoch [117/171], Step [100/172], Loss: 0.05962328612804413\n","Epoch [118/171], Step [100/172], Loss: 0.05748951807618141\n","Epoch [119/171], Step [100/172], Loss: 0.046110279858112335\n","Epoch [120/171], Step [100/172], Loss: 0.05384938418865204\n","Epoch [121/171], Step [100/172], Loss: 0.05054057016968727\n","Epoch [122/171], Step [100/172], Loss: 0.04785889759659767\n","Epoch [123/171], Step [100/172], Loss: 0.02486116625368595\n","Epoch [124/171], Step [100/172], Loss: 0.024849828332662582\n","Epoch [125/171], Step [100/172], Loss: 0.06373561918735504\n","Epoch [126/171], Step [100/172], Loss: 0.07880166172981262\n","Epoch [127/171], Step [100/172], Loss: 0.042493198066949844\n","Epoch [128/171], Step [100/172], Loss: 0.044789355248212814\n","Epoch [129/171], Step [100/172], Loss: 0.08329536020755768\n","Epoch [130/171], Step [100/172], Loss: 0.06761570274829865\n","Epoch [131/171], Step [100/172], Loss: 0.08242761343717575\n","Epoch [132/171], Step [100/172], Loss: 0.08177398145198822\n","Epoch [133/171], Step [100/172], Loss: 0.042899925261735916\n","Epoch [134/171], Step [100/172], Loss: 0.06194941699504852\n","Epoch [135/171], Step [100/172], Loss: 0.07968388497829437\n","Epoch [136/171], Step [100/172], Loss: 0.04453307390213013\n","Epoch [137/171], Step [100/172], Loss: 0.052798036485910416\n","Epoch [138/171], Step [100/172], Loss: 0.05618743598461151\n","Epoch [139/171], Step [100/172], Loss: 0.06832346320152283\n","Epoch [140/171], Step [100/172], Loss: 0.057450491935014725\n","Epoch [141/171], Step [100/172], Loss: 0.05384627357125282\n","Epoch [142/171], Step [100/172], Loss: 0.0770488977432251\n","Epoch [143/171], Step [100/172], Loss: 0.0566357746720314\n","Epoch [144/171], Step [100/172], Loss: 0.05235211178660393\n","Epoch [145/171], Step [100/172], Loss: 0.06179872527718544\n","Epoch [146/171], Step [100/172], Loss: 0.037856101989746094\n","Epoch [147/171], Step [100/172], Loss: 0.029707757756114006\n","Epoch [148/171], Step [100/172], Loss: 0.04046394303441048\n","Epoch [149/171], Step [100/172], Loss: 0.037116698920726776\n","Epoch [150/171], Step [100/172], Loss: 0.060564130544662476\n","Epoch [151/171], Step [100/172], Loss: 0.04569217935204506\n","Epoch [152/171], Step [100/172], Loss: 0.035632360726594925\n","Epoch [153/171], Step [100/172], Loss: 0.045675888657569885\n","Epoch [154/171], Step [100/172], Loss: 0.09870485216379166\n","Epoch [155/171], Step [100/172], Loss: 0.1695508062839508\n","Epoch [156/171], Step [100/172], Loss: 0.13204370439052582\n","Epoch [157/171], Step [100/172], Loss: 0.10263282805681229\n","Epoch [158/171], Step [100/172], Loss: 0.16663908958435059\n","Epoch [159/171], Step [100/172], Loss: 0.17204415798187256\n","Epoch [160/171], Step [100/172], Loss: 0.22044867277145386\n","Epoch [161/171], Step [100/172], Loss: 0.15446016192436218\n","Epoch [162/171], Step [100/172], Loss: 0.23157857358455658\n","Epoch [163/171], Step [100/172], Loss: 0.253401517868042\n","Epoch [164/171], Step [100/172], Loss: 0.229172483086586\n","Epoch [165/171], Step [100/172], Loss: 0.28687599301338196\n","Epoch [166/171], Step [100/172], Loss: 0.2544409930706024\n","Epoch [167/171], Step [100/172], Loss: 0.23340389132499695\n","Epoch [168/171], Step [100/172], Loss: 0.24979710578918457\n","Epoch [169/171], Step [100/172], Loss: 0.2540881931781769\n","Epoch [170/171], Step [100/172], Loss: 0.27968472242355347\n","Epoch [171/171], Step [100/172], Loss: 0.22735947370529175\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kr58hWDmb746","executionInfo":{"status":"ok","timestamp":1623802369757,"user_tz":-540,"elapsed":5163,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"821d9e11-60be-47ed-85cf-bf4c5dcc843b"},"source":["# Test the model\n","test_dataset = CustomDataset(X_test_bin, y_test_bin)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=400, shuffle=False)\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (images, labels) in enumerate(test_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 90.5\n","Iteration:1,  Accuracy: 90.375\n","Iteration:2,  Accuracy: 89.91666666666667\n","Iteration:3,  Accuracy: 90.0625\n","Iteration:4,  Accuracy: 90.25\n","Iteration:5,  Accuracy: 90.29166666666667\n","Iteration:6,  Accuracy: 90.17857142857143\n","Iteration:7,  Accuracy: 90.0\n","Iteration:8,  Accuracy: 90.16666666666667\n","Iteration:9,  Accuracy: 90.1\n","Iteration:10,  Accuracy: 90.11363636363636\n","Iteration:11,  Accuracy: 90.125\n","Iteration:12,  Accuracy: 90.0\n","Iteration:13,  Accuracy: 90.14285714285714\n","Iteration:14,  Accuracy: 90.18333333333334\n","Iteration:15,  Accuracy: 90.1875\n","Iteration:16,  Accuracy: 90.30882352941177\n","Iteration:17,  Accuracy: 90.25\n","Iteration:18,  Accuracy: 90.15789473684211\n","Iteration:19,  Accuracy: 90.175\n","Iteration:20,  Accuracy: 90.0\n","Iteration:21,  Accuracy: 89.89772727272727\n","Iteration:22,  Accuracy: 89.97826086956522\n","Iteration:23,  Accuracy: 90.0\n","Iteration:24,  Accuracy: 89.99\n","Iteration:25,  Accuracy: 90.0576923076923\n","Iteration:26,  Accuracy: 90.01851851851852\n","Iteration:27,  Accuracy: 89.94642857142857\n","Iteration:28,  Accuracy: 89.90517241379311\n","Iteration:29,  Accuracy: 89.85833333333333\n","Iteration:30,  Accuracy: 89.80645161290323\n","Iteration:31,  Accuracy: 89.8125\n","Iteration:32,  Accuracy: 89.81060606060606\n","Iteration:33,  Accuracy: 89.83823529411765\n","Iteration:34,  Accuracy: 89.85714285714286\n","Iteration:35,  Accuracy: 89.75\n","Iteration:36,  Accuracy: 89.72297297297297\n","Iteration:37,  Accuracy: 89.74342105263158\n","Iteration:38,  Accuracy: 89.73076923076923\n","Iteration:39,  Accuracy: 89.7125\n","Iteration:40,  Accuracy: 89.73780487804878\n","Iteration:41,  Accuracy: 89.69642857142857\n","Iteration:42,  Accuracy: 89.70348837209302\n","Iteration:43,  Accuracy: 89.69318181818181\n","Iteration:44,  Accuracy: 89.72222222222223\n","Iteration:45,  Accuracy: 89.68478260869566\n","Iteration:46,  Accuracy: 89.7127659574468\n","Iteration:47,  Accuracy: 89.71354166666667\n","Iteration:48,  Accuracy: 89.71938775510205\n","Iteration:49,  Accuracy: 89.725\n","Iteration:50,  Accuracy: 89.69117647058823\n","Iteration:51,  Accuracy: 89.67307692307692\n","Iteration:52,  Accuracy: 89.68867924528301\n","Iteration:53,  Accuracy: 89.70833333333333\n","Iteration:54,  Accuracy: 89.73636363636363\n","Iteration:55,  Accuracy: 89.74553571428571\n","Iteration:56,  Accuracy: 89.80263157894737\n","Iteration:57,  Accuracy: 89.80172413793103\n","Iteration:58,  Accuracy: 89.8177966101695\n","Iteration:59,  Accuracy: 89.90416666666667\n","Iteration:60,  Accuracy: 89.90983606557377\n","Iteration:61,  Accuracy: 89.88306451612904\n","Iteration:62,  Accuracy: 89.86111111111111\n","Iteration:63,  Accuracy: 89.87109375\n","Iteration:64,  Accuracy: 89.86538461538461\n","Iteration:65,  Accuracy: 89.81439393939394\n","Iteration:66,  Accuracy: 89.81716417910448\n","Iteration:67,  Accuracy: 89.81985294117646\n","Iteration:68,  Accuracy: 89.83333333333333\n","Iteration:69,  Accuracy: 89.85357142857143\n","Iteration:70,  Accuracy: 89.87676056338029\n","Iteration:71,  Accuracy: 89.92013888888889\n","Iteration:72,  Accuracy: 89.93835616438356\n","Iteration:73,  Accuracy: 89.91891891891892\n","Iteration:74,  Accuracy: 89.94\n","Iteration:75,  Accuracy: 89.94736842105263\n","Iteration:76,  Accuracy: 89.93181818181819\n","Iteration:77,  Accuracy: 89.9198717948718\n","Iteration:78,  Accuracy: 89.87341772151899\n","Iteration:79,  Accuracy: 89.89375\n","Iteration:80,  Accuracy: 89.88888888888889\n","Iteration:81,  Accuracy: 89.89634146341463\n","Iteration:82,  Accuracy: 89.88855421686748\n","Iteration:83,  Accuracy: 89.88392857142857\n","Iteration:84,  Accuracy: 89.88823529411765\n","Iteration:85,  Accuracy: 89.87209302325581\n","Iteration:86,  Accuracy: 89.85344827586206\n","Iteration:87,  Accuracy: 89.85795454545455\n","Iteration:88,  Accuracy: 89.86516853932584\n","Iteration:89,  Accuracy: 89.88055555555556\n","Iteration:90,  Accuracy: 89.88186813186813\n","Iteration:91,  Accuracy: 89.89945652173913\n","Iteration:92,  Accuracy: 89.89247311827957\n","Iteration:93,  Accuracy: 89.87765957446808\n","Iteration:94,  Accuracy: 89.88684210526316\n","Iteration:95,  Accuracy: 89.875\n","Iteration:96,  Accuracy: 89.84536082474227\n","Iteration:97,  Accuracy: 89.87755102040816\n","Iteration:98,  Accuracy: 89.88383838383838\n","Iteration:99,  Accuracy: 89.87\n","Iteration:100,  Accuracy: 89.89603960396039\n","Iteration:101,  Accuracy: 89.90196078431373\n","Iteration:102,  Accuracy: 89.87621359223301\n","Iteration:103,  Accuracy: 89.86538461538461\n","Iteration:104,  Accuracy: 89.87857142857143\n","Iteration:105,  Accuracy: 89.89622641509433\n","Iteration:106,  Accuracy: 89.89953271028037\n","Iteration:107,  Accuracy: 89.90337487746814\n","Test Accuracy of the model: 89.90337487746814 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fMSIk_EJcQei"},"source":["## 2. 3-class multi Classification"]},{"cell_type":"code","metadata":{"id":"0REbV9pKcTB4"},"source":["X_train_three = np.load(\"X_train_three.npy\")\n","y_train_three = np.load(\"y_train_three.npy\")\n","y_train_three = y_train_three.astype(np.int64)\n","X_test_three = np.load(\"X_test_three.npy\")\n","y_test_three = np.load(\"y_test_three.npy\")\n","y_test_three = y_test_three.astype(np.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BS846JMkcddD","executionInfo":{"status":"ok","timestamp":1623802430564,"user_tz":-540,"elapsed":11,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"04ddac33-574d-4464-dd32-ceb0850f5134"},"source":["print(y_train_three.shape)\n","print(y_test_three.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(50192,)\n","(12548,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ozxfJrxyck4a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623805146875,"user_tz":-540,"elapsed":359515,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"7affa088-e1b1-40e0-b1b5-29c722854d87"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 3\n","batch_size = 270\n","num_epochs = len(y_train_three) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train_three, y_train_three)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=270, shuffle=True)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/185], Step [100/186], Loss: 0.3426899313926697\n","Epoch [2/185], Step [100/186], Loss: 0.3482637107372284\n","Epoch [3/185], Step [100/186], Loss: 0.3827095627784729\n","Epoch [4/185], Step [100/186], Loss: 0.22269335389137268\n","Epoch [5/185], Step [100/186], Loss: 0.21475282311439514\n","Epoch [6/185], Step [100/186], Loss: 0.21794582903385162\n","Epoch [7/185], Step [100/186], Loss: 0.17095205187797546\n","Epoch [8/185], Step [100/186], Loss: 0.13830289244651794\n","Epoch [9/185], Step [100/186], Loss: 0.14321444928646088\n","Epoch [10/185], Step [100/186], Loss: 0.16504447162151337\n","Epoch [11/185], Step [100/186], Loss: 0.09946553409099579\n","Epoch [12/185], Step [100/186], Loss: 0.09435024112462997\n","Epoch [13/185], Step [100/186], Loss: 0.11817925423383713\n","Epoch [14/185], Step [100/186], Loss: 0.07228156179189682\n","Epoch [15/185], Step [100/186], Loss: 0.06672212481498718\n","Epoch [16/185], Step [100/186], Loss: 0.0883413627743721\n","Epoch [17/185], Step [100/186], Loss: 0.08756371587514877\n","Epoch [18/185], Step [100/186], Loss: 0.07904423028230667\n","Epoch [19/185], Step [100/186], Loss: 0.0641636848449707\n","Epoch [20/185], Step [100/186], Loss: 0.02074471488595009\n","Epoch [21/185], Step [100/186], Loss: 0.10306309163570404\n","Epoch [22/185], Step [100/186], Loss: 0.03990974649786949\n","Epoch [23/185], Step [100/186], Loss: 0.07696071267127991\n","Epoch [24/185], Step [100/186], Loss: 0.06355360895395279\n","Epoch [25/185], Step [100/186], Loss: 0.05414152145385742\n","Epoch [26/185], Step [100/186], Loss: 0.026754043996334076\n","Epoch [27/185], Step [100/186], Loss: 0.036367956548929214\n","Epoch [28/185], Step [100/186], Loss: 0.041850581765174866\n","Epoch [29/185], Step [100/186], Loss: 0.04626551643013954\n","Epoch [30/185], Step [100/186], Loss: 0.053844235837459564\n","Epoch [31/185], Step [100/186], Loss: 0.017108511179685593\n","Epoch [32/185], Step [100/186], Loss: 0.052194446325302124\n","Epoch [33/185], Step [100/186], Loss: 0.05418756231665611\n","Epoch [34/185], Step [100/186], Loss: 0.04430912062525749\n","Epoch [35/185], Step [100/186], Loss: 0.04598291218280792\n","Epoch [36/185], Step [100/186], Loss: 0.051820263266563416\n","Epoch [37/185], Step [100/186], Loss: 0.03482648357748985\n","Epoch [38/185], Step [100/186], Loss: 0.051986176520586014\n","Epoch [39/185], Step [100/186], Loss: 0.03463565930724144\n","Epoch [40/185], Step [100/186], Loss: 0.03893864154815674\n","Epoch [41/185], Step [100/186], Loss: 0.04606693238019943\n","Epoch [42/185], Step [100/186], Loss: 0.04897386580705643\n","Epoch [43/185], Step [100/186], Loss: 0.054322950541973114\n","Epoch [44/185], Step [100/186], Loss: 0.02833227999508381\n","Epoch [45/185], Step [100/186], Loss: 0.045390672981739044\n","Epoch [46/185], Step [100/186], Loss: 0.03630514815449715\n","Epoch [47/185], Step [100/186], Loss: 0.030814923346042633\n","Epoch [48/185], Step [100/186], Loss: 0.025920238345861435\n","Epoch [49/185], Step [100/186], Loss: 0.052219171077013016\n","Epoch [50/185], Step [100/186], Loss: 0.06427907943725586\n","Epoch [51/185], Step [100/186], Loss: 0.020192017778754234\n","Epoch [52/185], Step [100/186], Loss: 0.0350610576570034\n","Epoch [53/185], Step [100/186], Loss: 0.037470825016498566\n","Epoch [54/185], Step [100/186], Loss: 0.023006649687886238\n","Epoch [55/185], Step [100/186], Loss: 0.022790422663092613\n","Epoch [56/185], Step [100/186], Loss: 0.03323661908507347\n","Epoch [57/185], Step [100/186], Loss: 0.07772402465343475\n","Epoch [58/185], Step [100/186], Loss: 0.03963194414973259\n","Epoch [59/185], Step [100/186], Loss: 0.015690147876739502\n","Epoch [60/185], Step [100/186], Loss: 0.01180607546120882\n","Epoch [61/185], Step [100/186], Loss: 0.08630844950675964\n","Epoch [62/185], Step [100/186], Loss: 0.03343659266829491\n","Epoch [63/185], Step [100/186], Loss: 0.040800541639328\n","Epoch [64/185], Step [100/186], Loss: 0.08617100119590759\n","Epoch [65/185], Step [100/186], Loss: 0.021718895062804222\n","Epoch [66/185], Step [100/186], Loss: 0.014679016545414925\n","Epoch [67/185], Step [100/186], Loss: 0.045202236622571945\n","Epoch [68/185], Step [100/186], Loss: 0.00771314837038517\n","Epoch [69/185], Step [100/186], Loss: 0.07838337123394012\n","Epoch [70/185], Step [100/186], Loss: 0.02606731653213501\n","Epoch [71/185], Step [100/186], Loss: 0.02228553406894207\n","Epoch [72/185], Step [100/186], Loss: 0.03392860293388367\n","Epoch [73/185], Step [100/186], Loss: 0.011034521274268627\n","Epoch [74/185], Step [100/186], Loss: 0.01456811185926199\n","Epoch [75/185], Step [100/186], Loss: 0.03053978830575943\n","Epoch [76/185], Step [100/186], Loss: 0.0595785416662693\n","Epoch [77/185], Step [100/186], Loss: 0.029030952602624893\n","Epoch [78/185], Step [100/186], Loss: 0.08783143013715744\n","Epoch [79/185], Step [100/186], Loss: 0.025059593841433525\n","Epoch [80/185], Step [100/186], Loss: 0.05086313560605049\n","Epoch [81/185], Step [100/186], Loss: 0.038658931851387024\n","Epoch [82/185], Step [100/186], Loss: 0.012297693639993668\n","Epoch [83/185], Step [100/186], Loss: 0.039144549518823624\n","Epoch [84/185], Step [100/186], Loss: 0.02468496561050415\n","Epoch [85/185], Step [100/186], Loss: 0.06262286752462387\n","Epoch [86/185], Step [100/186], Loss: 0.10218998789787292\n","Epoch [87/185], Step [100/186], Loss: 0.058837685734033585\n","Epoch [88/185], Step [100/186], Loss: 0.04629901051521301\n","Epoch [89/185], Step [100/186], Loss: 0.030576281249523163\n","Epoch [90/185], Step [100/186], Loss: 0.02710893377661705\n","Epoch [91/185], Step [100/186], Loss: 0.04803183674812317\n","Epoch [92/185], Step [100/186], Loss: 0.021352652460336685\n","Epoch [93/185], Step [100/186], Loss: 0.03724215552210808\n","Epoch [94/185], Step [100/186], Loss: 0.051125068217515945\n","Epoch [95/185], Step [100/186], Loss: 0.02886105701327324\n","Epoch [96/185], Step [100/186], Loss: 0.027248650789260864\n","Epoch [97/185], Step [100/186], Loss: 0.009994485415518284\n","Epoch [98/185], Step [100/186], Loss: 0.04211046174168587\n","Epoch [99/185], Step [100/186], Loss: 0.051245491951704025\n","Epoch [100/185], Step [100/186], Loss: 0.014427910558879375\n","Epoch [101/185], Step [100/186], Loss: 0.009112178348004818\n","Epoch [102/185], Step [100/186], Loss: 0.01905548758804798\n","Epoch [103/185], Step [100/186], Loss: 0.11332309246063232\n","Epoch [104/185], Step [100/186], Loss: 0.018010126426815987\n","Epoch [105/185], Step [100/186], Loss: 0.03219792991876602\n","Epoch [106/185], Step [100/186], Loss: 0.04258647561073303\n","Epoch [107/185], Step [100/186], Loss: 0.03909007087349892\n","Epoch [108/185], Step [100/186], Loss: 0.04113774001598358\n","Epoch [109/185], Step [100/186], Loss: 0.03327683359384537\n","Epoch [110/185], Step [100/186], Loss: 0.030425449833273888\n","Epoch [111/185], Step [100/186], Loss: 0.01794227585196495\n","Epoch [112/185], Step [100/186], Loss: 0.016111081466078758\n","Epoch [113/185], Step [100/186], Loss: 0.07592505216598511\n","Epoch [114/185], Step [100/186], Loss: 0.01824701391160488\n","Epoch [115/185], Step [100/186], Loss: 0.017968660220503807\n","Epoch [116/185], Step [100/186], Loss: 0.049512725323438644\n","Epoch [117/185], Step [100/186], Loss: 0.011888919398188591\n","Epoch [118/185], Step [100/186], Loss: 0.013035598210990429\n","Epoch [119/185], Step [100/186], Loss: 0.08480928093194962\n","Epoch [120/185], Step [100/186], Loss: 0.06906349211931229\n","Epoch [121/185], Step [100/186], Loss: 0.03416039049625397\n","Epoch [122/185], Step [100/186], Loss: 0.05927058309316635\n","Epoch [123/185], Step [100/186], Loss: 0.02912290208041668\n","Epoch [124/185], Step [100/186], Loss: 0.056854069232940674\n","Epoch [125/185], Step [100/186], Loss: 0.02152368426322937\n","Epoch [126/185], Step [100/186], Loss: 0.02674955129623413\n","Epoch [127/185], Step [100/186], Loss: 0.0039055622182786465\n","Epoch [128/185], Step [100/186], Loss: 0.0070137083530426025\n","Epoch [129/185], Step [100/186], Loss: 0.0494346059858799\n","Epoch [130/185], Step [100/186], Loss: 0.015271446667611599\n","Epoch [131/185], Step [100/186], Loss: 0.035798460245132446\n","Epoch [132/185], Step [100/186], Loss: 0.06544791162014008\n","Epoch [133/185], Step [100/186], Loss: 0.062251895666122437\n","Epoch [134/185], Step [100/186], Loss: 0.046016912907361984\n","Epoch [135/185], Step [100/186], Loss: 0.025064757093787193\n","Epoch [136/185], Step [100/186], Loss: 0.02420913241803646\n","Epoch [137/185], Step [100/186], Loss: 0.02361038699746132\n","Epoch [138/185], Step [100/186], Loss: 0.029550446197390556\n","Epoch [139/185], Step [100/186], Loss: 0.04991399496793747\n","Epoch [140/185], Step [100/186], Loss: 0.09000084549188614\n","Epoch [141/185], Step [100/186], Loss: 0.0359695665538311\n","Epoch [142/185], Step [100/186], Loss: 0.02567448653280735\n","Epoch [143/185], Step [100/186], Loss: 0.011447879485785961\n","Epoch [144/185], Step [100/186], Loss: 0.04592340439558029\n","Epoch [145/185], Step [100/186], Loss: 0.06176578253507614\n","Epoch [146/185], Step [100/186], Loss: 0.09852222353219986\n","Epoch [147/185], Step [100/186], Loss: 0.08196429908275604\n","Epoch [148/185], Step [100/186], Loss: 0.056052666157484055\n","Epoch [149/185], Step [100/186], Loss: 0.12591266632080078\n","Epoch [150/185], Step [100/186], Loss: 0.09911362826824188\n","Epoch [151/185], Step [100/186], Loss: 0.06719183921813965\n","Epoch [152/185], Step [100/186], Loss: 0.04317042604088783\n","Epoch [153/185], Step [100/186], Loss: 0.02864396572113037\n","Epoch [154/185], Step [100/186], Loss: 0.056492894887924194\n","Epoch [155/185], Step [100/186], Loss: 0.19195407629013062\n","Epoch [156/185], Step [100/186], Loss: 0.10800357908010483\n","Epoch [157/185], Step [100/186], Loss: 0.19405384361743927\n","Epoch [158/185], Step [100/186], Loss: 0.2531881630420685\n","Epoch [159/185], Step [100/186], Loss: 0.16571304202079773\n","Epoch [160/185], Step [100/186], Loss: 0.15633712708950043\n","Epoch [161/185], Step [100/186], Loss: 0.12898267805576324\n","Epoch [162/185], Step [100/186], Loss: 0.2818039059638977\n","Epoch [163/185], Step [100/186], Loss: 0.23437689244747162\n","Epoch [164/185], Step [100/186], Loss: 0.25383007526397705\n","Epoch [165/185], Step [100/186], Loss: 0.35633349418640137\n","Epoch [166/185], Step [100/186], Loss: 0.19666078686714172\n","Epoch [167/185], Step [100/186], Loss: 0.2718014419078827\n","Epoch [168/185], Step [100/186], Loss: 0.29735690355300903\n","Epoch [169/185], Step [100/186], Loss: 0.2559717297554016\n","Epoch [170/185], Step [100/186], Loss: 0.32686862349510193\n","Epoch [171/185], Step [100/186], Loss: 0.2716379165649414\n","Epoch [172/185], Step [100/186], Loss: 0.31649550795555115\n","Epoch [173/185], Step [100/186], Loss: 0.2366136759519577\n","Epoch [174/185], Step [100/186], Loss: 0.29887980222702026\n","Epoch [175/185], Step [100/186], Loss: 0.274606317281723\n","Epoch [176/185], Step [100/186], Loss: 0.23973719775676727\n","Epoch [177/185], Step [100/186], Loss: 0.26357007026672363\n","Epoch [178/185], Step [100/186], Loss: 0.3129594027996063\n","Epoch [179/185], Step [100/186], Loss: 0.2733798325061798\n","Epoch [180/185], Step [100/186], Loss: 0.361772745847702\n","Epoch [181/185], Step [100/186], Loss: 0.26263073086738586\n","Epoch [182/185], Step [100/186], Loss: 0.24347443878650665\n","Epoch [183/185], Step [100/186], Loss: 0.2949203550815582\n","Epoch [184/185], Step [100/186], Loss: 0.27206119894981384\n","Epoch [185/185], Step [100/186], Loss: 0.25447800755500793\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r9i_zCwSd4VG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623805148143,"user_tz":-540,"elapsed":1277,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"31fcbabc-8a84-4b04-d630-8d742e7239d0"},"source":["# Test the model\n","test_dataset = CustomDataset(X_test_three, y_test_three)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (images, labels) in enumerate(test_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 88.0\n","Iteration:1,  Accuracy: 87.0\n","Iteration:2,  Accuracy: 89.33333333333333\n","Iteration:3,  Accuracy: 90.0\n","Iteration:4,  Accuracy: 90.4\n","Iteration:5,  Accuracy: 89.5\n","Iteration:6,  Accuracy: 88.71428571428571\n","Iteration:7,  Accuracy: 88.5\n","Iteration:8,  Accuracy: 88.77777777777777\n","Iteration:9,  Accuracy: 88.4\n","Iteration:10,  Accuracy: 88.81818181818181\n","Iteration:11,  Accuracy: 89.33333333333333\n","Iteration:12,  Accuracy: 89.23076923076923\n","Iteration:13,  Accuracy: 89.21428571428571\n","Iteration:14,  Accuracy: 89.46666666666667\n","Iteration:15,  Accuracy: 89.5\n","Iteration:16,  Accuracy: 89.76470588235294\n","Iteration:17,  Accuracy: 89.55555555555556\n","Iteration:18,  Accuracy: 89.47368421052632\n","Iteration:19,  Accuracy: 89.4\n","Iteration:20,  Accuracy: 89.38095238095238\n","Iteration:21,  Accuracy: 89.54545454545455\n","Iteration:22,  Accuracy: 89.30434782608695\n","Iteration:23,  Accuracy: 89.45833333333333\n","Iteration:24,  Accuracy: 89.6\n","Iteration:25,  Accuracy: 89.61538461538461\n","Iteration:26,  Accuracy: 89.4074074074074\n","Iteration:27,  Accuracy: 89.53571428571429\n","Iteration:28,  Accuracy: 89.51724137931035\n","Iteration:29,  Accuracy: 89.46666666666667\n","Iteration:30,  Accuracy: 89.48387096774194\n","Iteration:31,  Accuracy: 89.5625\n","Iteration:32,  Accuracy: 89.60606060606061\n","Iteration:33,  Accuracy: 89.61764705882354\n","Iteration:34,  Accuracy: 89.45714285714286\n","Iteration:35,  Accuracy: 89.58333333333333\n","Iteration:36,  Accuracy: 89.54054054054055\n","Iteration:37,  Accuracy: 89.44736842105263\n","Iteration:38,  Accuracy: 89.51282051282051\n","Iteration:39,  Accuracy: 89.5\n","Iteration:40,  Accuracy: 89.63414634146342\n","Iteration:41,  Accuracy: 89.66666666666667\n","Iteration:42,  Accuracy: 89.6046511627907\n","Iteration:43,  Accuracy: 89.72727272727273\n","Iteration:44,  Accuracy: 89.71111111111111\n","Iteration:45,  Accuracy: 89.76086956521739\n","Iteration:46,  Accuracy: 89.80851063829788\n","Iteration:47,  Accuracy: 89.83333333333333\n","Iteration:48,  Accuracy: 89.91836734693878\n","Iteration:49,  Accuracy: 89.88\n","Iteration:50,  Accuracy: 89.88235294117646\n","Iteration:51,  Accuracy: 89.90384615384616\n","Iteration:52,  Accuracy: 89.86792452830188\n","Iteration:53,  Accuracy: 89.75925925925925\n","Iteration:54,  Accuracy: 89.65454545454546\n","Iteration:55,  Accuracy: 89.58928571428571\n","Iteration:56,  Accuracy: 89.66666666666667\n","Iteration:57,  Accuracy: 89.6896551724138\n","Iteration:58,  Accuracy: 89.66101694915254\n","Iteration:59,  Accuracy: 89.65\n","Iteration:60,  Accuracy: 89.67213114754098\n","Iteration:61,  Accuracy: 89.7258064516129\n","Iteration:62,  Accuracy: 89.85714285714286\n","Iteration:63,  Accuracy: 89.796875\n","Iteration:64,  Accuracy: 89.72307692307692\n","Iteration:65,  Accuracy: 89.75757575757575\n","Iteration:66,  Accuracy: 89.76119402985074\n","Iteration:67,  Accuracy: 89.79411764705883\n","Iteration:68,  Accuracy: 89.81159420289855\n","Iteration:69,  Accuracy: 89.9\n","Iteration:70,  Accuracy: 89.92957746478874\n","Iteration:71,  Accuracy: 89.94444444444444\n","Iteration:72,  Accuracy: 89.94520547945206\n","Iteration:73,  Accuracy: 89.94594594594595\n","Iteration:74,  Accuracy: 89.93333333333334\n","Iteration:75,  Accuracy: 89.90789473684211\n","Iteration:76,  Accuracy: 89.87012987012987\n","Iteration:77,  Accuracy: 89.82051282051282\n","Iteration:78,  Accuracy: 89.81012658227849\n","Iteration:79,  Accuracy: 89.8875\n","Iteration:80,  Accuracy: 89.85185185185185\n","Iteration:81,  Accuracy: 89.85365853658537\n","Iteration:82,  Accuracy: 89.855421686747\n","Iteration:83,  Accuracy: 89.80952380952381\n","Iteration:84,  Accuracy: 89.7764705882353\n","Iteration:85,  Accuracy: 89.76744186046511\n","Iteration:86,  Accuracy: 89.74712643678161\n","Iteration:87,  Accuracy: 89.77272727272727\n","Iteration:88,  Accuracy: 89.8314606741573\n","Iteration:89,  Accuracy: 89.85555555555555\n","Iteration:90,  Accuracy: 89.86813186813187\n","Iteration:91,  Accuracy: 89.80434782608695\n","Iteration:92,  Accuracy: 89.84946236559139\n","Iteration:93,  Accuracy: 89.90425531914893\n","Iteration:94,  Accuracy: 89.92631578947369\n","Iteration:95,  Accuracy: 89.98958333333333\n","Iteration:96,  Accuracy: 90.03092783505154\n","Iteration:97,  Accuracy: 90.04081632653062\n","Iteration:98,  Accuracy: 89.97979797979798\n","Iteration:99,  Accuracy: 89.95\n","Iteration:100,  Accuracy: 89.93069306930693\n","Iteration:101,  Accuracy: 89.98039215686275\n","Iteration:102,  Accuracy: 89.98058252427184\n","Iteration:103,  Accuracy: 89.91346153846153\n","Iteration:104,  Accuracy: 89.92380952380952\n","Iteration:105,  Accuracy: 89.93396226415095\n","Iteration:106,  Accuracy: 89.97196261682242\n","Iteration:107,  Accuracy: 89.94444444444444\n","Iteration:108,  Accuracy: 89.96330275229357\n","Iteration:109,  Accuracy: 89.92727272727272\n","Iteration:110,  Accuracy: 89.90990990990991\n","Iteration:111,  Accuracy: 89.92857142857143\n","Iteration:112,  Accuracy: 89.93805309734513\n","Iteration:113,  Accuracy: 89.96491228070175\n","Iteration:114,  Accuracy: 89.95652173913044\n","Iteration:115,  Accuracy: 89.95689655172414\n","Iteration:116,  Accuracy: 89.93162393162393\n","Iteration:117,  Accuracy: 89.92372881355932\n","Iteration:118,  Accuracy: 89.94957983193277\n","Iteration:119,  Accuracy: 89.95833333333333\n","Iteration:120,  Accuracy: 89.99173553719008\n","Iteration:121,  Accuracy: 89.98360655737704\n","Iteration:122,  Accuracy: 90.01626016260163\n","Iteration:123,  Accuracy: 90.06451612903226\n","Iteration:124,  Accuracy: 90.072\n","Iteration:125,  Accuracy: 90.10200828817341\n","Test Accuracy of the model: 90.10200828817341 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4b19ClA7d4tF"},"source":["## 3. 9-class multi Classification"]},{"cell_type":"code","metadata":{"id":"YzzoW_YHd8Ep"},"source":["X_train_nine = np.load(\"X_train_nine.npy\")\n","y_train_nine = np.load(\"y_train_nine.npy\")\n","y_train_nine = y_train_nine.astype(np.int64)\n","X_test_nine = np.load(\"X_test_nine.npy\")\n","y_test_nine = np.load(\"y_test_nine.npy\")\n","y_test_nine = y_test_nine.astype(np.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A8jnnRXWd9JS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623805195847,"user_tz":-540,"elapsed":613,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"7e03f993-1c19-4ff9-cb78-8ecdc628482e"},"source":["print(y_train_nine.shape)\n","print(y_test_nine.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(23696,)\n","(5924,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9LeCh7p3d9k7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623806253824,"user_tz":-540,"elapsed":937321,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"d893a9d7-445f-4cde-80d0-bea36341f8d9"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# Hyper-parameters\n","sequence_length = 20\n","input_size = 8\n","hidden_size = 64\n","num_layers = 2\n","num_classes = 9\n","batch_size = 160\n","num_epochs = len(y_train_nine) // batch_size # 2\n","learning_rate = 0.01\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, X_data, Y_data):\n","    self.x_data = X_data\n","    self.y_data = Y_data\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self):\n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = self.y_data[idx]  # y_data가 list 안에 4900 여개의 숫자가 들어있는 식의 형태...? 이다보니 구조 변경이 필요함. 이렇게 하면 그냥 숫자 하나 받아지는 것임\n","    # 내지는\n","    return x, y\n","\n","\n","# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_dataset = CustomDataset(X_train_nine, y_train_nine)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=160, shuffle=True)\n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch [1/148], Step [100/149], Loss: 1.1275221109390259\n","Epoch [2/148], Step [100/149], Loss: 0.9590297937393188\n","Epoch [3/148], Step [100/149], Loss: 0.9057259559631348\n","Epoch [4/148], Step [100/149], Loss: 0.8030657768249512\n","Epoch [5/148], Step [100/149], Loss: 0.8367018699645996\n","Epoch [6/148], Step [100/149], Loss: 0.6803274154663086\n","Epoch [7/148], Step [100/149], Loss: 0.6746034622192383\n","Epoch [8/148], Step [100/149], Loss: 0.43692389130592346\n","Epoch [9/148], Step [100/149], Loss: 0.6171703934669495\n","Epoch [10/148], Step [100/149], Loss: 0.3945068120956421\n","Epoch [11/148], Step [100/149], Loss: 0.2965227961540222\n","Epoch [12/148], Step [100/149], Loss: 0.22448770701885223\n","Epoch [13/148], Step [100/149], Loss: 0.3887699544429779\n","Epoch [14/148], Step [100/149], Loss: 0.21288970112800598\n","Epoch [15/148], Step [100/149], Loss: 0.1538824737071991\n","Epoch [16/148], Step [100/149], Loss: 0.3264177441596985\n","Epoch [17/148], Step [100/149], Loss: 0.18274766206741333\n","Epoch [18/148], Step [100/149], Loss: 0.17323164641857147\n","Epoch [19/148], Step [100/149], Loss: 0.3228970170021057\n","Epoch [20/148], Step [100/149], Loss: 0.10856954753398895\n","Epoch [21/148], Step [100/149], Loss: 0.2082197666168213\n","Epoch [22/148], Step [100/149], Loss: 0.13404510915279388\n","Epoch [23/148], Step [100/149], Loss: 0.1631505787372589\n","Epoch [24/148], Step [100/149], Loss: 0.15923473238945007\n","Epoch [25/148], Step [100/149], Loss: 0.12062517553567886\n","Epoch [26/148], Step [100/149], Loss: 0.16405579447746277\n","Epoch [27/148], Step [100/149], Loss: 0.14636972546577454\n","Epoch [28/148], Step [100/149], Loss: 0.07584710419178009\n","Epoch [29/148], Step [100/149], Loss: 0.101991668343544\n","Epoch [30/148], Step [100/149], Loss: 0.1572090983390808\n","Epoch [31/148], Step [100/149], Loss: 0.07870753109455109\n","Epoch [32/148], Step [100/149], Loss: 0.16444334387779236\n","Epoch [33/148], Step [100/149], Loss: 0.061063140630722046\n","Epoch [34/148], Step [100/149], Loss: 0.12454422563314438\n","Epoch [35/148], Step [100/149], Loss: 0.06761539727449417\n","Epoch [36/148], Step [100/149], Loss: 0.17600136995315552\n","Epoch [37/148], Step [100/149], Loss: 0.10585600137710571\n","Epoch [38/148], Step [100/149], Loss: 0.09431202709674835\n","Epoch [39/148], Step [100/149], Loss: 0.11677136272192001\n","Epoch [40/148], Step [100/149], Loss: 0.11922384798526764\n","Epoch [41/148], Step [100/149], Loss: 0.16077016294002533\n","Epoch [42/148], Step [100/149], Loss: 0.10656615346670151\n","Epoch [43/148], Step [100/149], Loss: 0.18634667992591858\n","Epoch [44/148], Step [100/149], Loss: 0.16229547560214996\n","Epoch [45/148], Step [100/149], Loss: 0.18795160949230194\n","Epoch [46/148], Step [100/149], Loss: 0.1415347009897232\n","Epoch [47/148], Step [100/149], Loss: 0.03675119951367378\n","Epoch [48/148], Step [100/149], Loss: 0.03385329991579056\n","Epoch [49/148], Step [100/149], Loss: 0.12067284435033798\n","Epoch [50/148], Step [100/149], Loss: 0.047582149505615234\n","Epoch [51/148], Step [100/149], Loss: 0.08071160316467285\n","Epoch [52/148], Step [100/149], Loss: 0.09540481865406036\n","Epoch [53/148], Step [100/149], Loss: 0.19251000881195068\n","Epoch [54/148], Step [100/149], Loss: 0.07497285306453705\n","Epoch [55/148], Step [100/149], Loss: 0.12486518919467926\n","Epoch [56/148], Step [100/149], Loss: 0.06672115623950958\n","Epoch [57/148], Step [100/149], Loss: 0.05889003351330757\n","Epoch [58/148], Step [100/149], Loss: 0.16854362189769745\n","Epoch [59/148], Step [100/149], Loss: 0.06957831978797913\n","Epoch [60/148], Step [100/149], Loss: 0.29356810450553894\n","Epoch [61/148], Step [100/149], Loss: 0.18852806091308594\n","Epoch [62/148], Step [100/149], Loss: 0.04489501565694809\n","Epoch [63/148], Step [100/149], Loss: 0.19557520747184753\n","Epoch [64/148], Step [100/149], Loss: 0.11769689619541168\n","Epoch [65/148], Step [100/149], Loss: 0.0951453447341919\n","Epoch [66/148], Step [100/149], Loss: 0.1191285252571106\n","Epoch [67/148], Step [100/149], Loss: 0.11943505704402924\n","Epoch [68/148], Step [100/149], Loss: 0.10105409473180771\n","Epoch [69/148], Step [100/149], Loss: 0.15837359428405762\n","Epoch [70/148], Step [100/149], Loss: 0.10982774198055267\n","Epoch [71/148], Step [100/149], Loss: 0.13026928901672363\n","Epoch [72/148], Step [100/149], Loss: 0.09873305261135101\n","Epoch [73/148], Step [100/149], Loss: 0.0761803388595581\n","Epoch [74/148], Step [100/149], Loss: 0.2456357777118683\n","Epoch [75/148], Step [100/149], Loss: 0.09080082923173904\n","Epoch [76/148], Step [100/149], Loss: 0.10405023396015167\n","Epoch [77/148], Step [100/149], Loss: 0.12898893654346466\n","Epoch [78/148], Step [100/149], Loss: 0.16765756905078888\n","Epoch [79/148], Step [100/149], Loss: 0.10386054217815399\n","Epoch [80/148], Step [100/149], Loss: 0.2212587296962738\n","Epoch [81/148], Step [100/149], Loss: 0.19083271920681\n","Epoch [82/148], Step [100/149], Loss: 0.22445163130760193\n","Epoch [83/148], Step [100/149], Loss: 0.12156156450510025\n","Epoch [84/148], Step [100/149], Loss: 0.16822148859500885\n","Epoch [85/148], Step [100/149], Loss: 0.0406586229801178\n","Epoch [86/148], Step [100/149], Loss: 0.1012931689620018\n","Epoch [87/148], Step [100/149], Loss: 0.029152611270546913\n","Epoch [88/148], Step [100/149], Loss: 0.0663623958826065\n","Epoch [89/148], Step [100/149], Loss: 0.06594380736351013\n","Epoch [90/148], Step [100/149], Loss: 0.0898885652422905\n","Epoch [91/148], Step [100/149], Loss: 0.08505251258611679\n","Epoch [92/148], Step [100/149], Loss: 0.10506673902273178\n","Epoch [93/148], Step [100/149], Loss: 0.08722762763500214\n","Epoch [94/148], Step [100/149], Loss: 0.18861283361911774\n","Epoch [95/148], Step [100/149], Loss: 0.0802769809961319\n","Epoch [96/148], Step [100/149], Loss: 0.08562231808900833\n","Epoch [97/148], Step [100/149], Loss: 0.11154042184352875\n","Epoch [98/148], Step [100/149], Loss: 0.09570229053497314\n","Epoch [99/148], Step [100/149], Loss: 0.09684540331363678\n","Epoch [100/148], Step [100/149], Loss: 0.5335004925727844\n","Epoch [101/148], Step [100/149], Loss: 0.31822797656059265\n","Epoch [102/148], Step [100/149], Loss: 0.1562959998846054\n","Epoch [103/148], Step [100/149], Loss: 0.13178704679012299\n","Epoch [104/148], Step [100/149], Loss: 0.16472981870174408\n","Epoch [105/148], Step [100/149], Loss: 0.180135577917099\n","Epoch [106/148], Step [100/149], Loss: 0.07871442288160324\n","Epoch [107/148], Step [100/149], Loss: 0.0808296948671341\n","Epoch [108/148], Step [100/149], Loss: 0.046122629195451736\n","Epoch [109/148], Step [100/149], Loss: 0.06473423540592194\n","Epoch [110/148], Step [100/149], Loss: 0.11557646840810776\n","Epoch [111/148], Step [100/149], Loss: 0.12915757298469543\n","Epoch [112/148], Step [100/149], Loss: 0.18569830060005188\n","Epoch [113/148], Step [100/149], Loss: 0.051454879343509674\n","Epoch [114/148], Step [100/149], Loss: 0.09370888024568558\n","Epoch [115/148], Step [100/149], Loss: 0.3121739625930786\n","Epoch [116/148], Step [100/149], Loss: 0.1620657742023468\n","Epoch [117/148], Step [100/149], Loss: 0.08227277547121048\n","Epoch [118/148], Step [100/149], Loss: 0.13417421281337738\n","Epoch [119/148], Step [100/149], Loss: 0.03919856995344162\n","Epoch [120/148], Step [100/149], Loss: 0.17977988719940186\n","Epoch [121/148], Step [100/149], Loss: 0.08506475389003754\n","Epoch [122/148], Step [100/149], Loss: 0.09286167472600937\n","Epoch [123/148], Step [100/149], Loss: 0.10284841060638428\n","Epoch [124/148], Step [100/149], Loss: 0.19786562025547028\n","Epoch [125/148], Step [100/149], Loss: 0.18809351325035095\n","Epoch [126/148], Step [100/149], Loss: 0.059427954256534576\n","Epoch [127/148], Step [100/149], Loss: 0.1733689159154892\n","Epoch [128/148], Step [100/149], Loss: 0.13025322556495667\n","Epoch [129/148], Step [100/149], Loss: 0.06150665879249573\n","Epoch [130/148], Step [100/149], Loss: 0.190835103392601\n","Epoch [131/148], Step [100/149], Loss: 0.20582041144371033\n","Epoch [132/148], Step [100/149], Loss: 0.14640888571739197\n","Epoch [133/148], Step [100/149], Loss: 0.15276190638542175\n","Epoch [134/148], Step [100/149], Loss: 0.1231069341301918\n","Epoch [135/148], Step [100/149], Loss: 0.13041386008262634\n","Epoch [136/148], Step [100/149], Loss: 0.081297367811203\n","Epoch [137/148], Step [100/149], Loss: 0.47910231351852417\n","Epoch [138/148], Step [100/149], Loss: 0.31765592098236084\n","Epoch [139/148], Step [100/149], Loss: 0.2662074863910675\n","Epoch [140/148], Step [100/149], Loss: 0.1995585709810257\n","Epoch [141/148], Step [100/149], Loss: 0.07233770936727524\n","Epoch [142/148], Step [100/149], Loss: 0.10966215282678604\n","Epoch [143/148], Step [100/149], Loss: 0.16597986221313477\n","Epoch [144/148], Step [100/149], Loss: 0.23932740092277527\n","Epoch [145/148], Step [100/149], Loss: 0.06949003040790558\n","Epoch [146/148], Step [100/149], Loss: 0.14063270390033722\n","Epoch [147/148], Step [100/149], Loss: 0.1323401927947998\n","Epoch [148/148], Step [100/149], Loss: 0.06372158229351044\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bGx5-cmneXwb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623806256062,"user_tz":-540,"elapsed":976,"user":{"displayName":"‍김원경[학생](소프트웨어융합대학 소프트웨어융합학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr9Sn9H6px3B5w47bQEfd1zN8MSMNMf-N78DGr=s64","userId":"18409268269417739585"}},"outputId":"29dc6044-e55a-45d1-8924-2a0e1a4a899a"},"source":["# Test the model\n","test_dataset = CustomDataset(X_test_nine, y_test_nine)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=60, shuffle=False)\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for i, (images, labels) in enumerate(test_loader):\n","        images = images.reshape(-1, sequence_length, input_size).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        print('Iteration:{},  Accuracy: {}'.format(i, 100 * correct / total))\n","\n","    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration:0,  Accuracy: 91.66666666666667\n","Iteration:1,  Accuracy: 91.66666666666667\n","Iteration:2,  Accuracy: 92.77777777777777\n","Iteration:3,  Accuracy: 92.5\n","Iteration:4,  Accuracy: 92.0\n","Iteration:5,  Accuracy: 92.5\n","Iteration:6,  Accuracy: 93.57142857142857\n","Iteration:7,  Accuracy: 93.75\n","Iteration:8,  Accuracy: 93.88888888888889\n","Iteration:9,  Accuracy: 94.33333333333333\n","Iteration:10,  Accuracy: 94.39393939393939\n","Iteration:11,  Accuracy: 93.88888888888889\n","Iteration:12,  Accuracy: 93.71794871794872\n","Iteration:13,  Accuracy: 93.80952380952381\n","Iteration:14,  Accuracy: 93.66666666666667\n","Iteration:15,  Accuracy: 93.64583333333333\n","Iteration:16,  Accuracy: 93.62745098039215\n","Iteration:17,  Accuracy: 93.51851851851852\n","Iteration:18,  Accuracy: 93.42105263157895\n","Iteration:19,  Accuracy: 93.0\n","Iteration:20,  Accuracy: 93.01587301587301\n","Iteration:21,  Accuracy: 92.72727272727273\n","Iteration:22,  Accuracy: 92.68115942028986\n","Iteration:23,  Accuracy: 92.98611111111111\n","Iteration:24,  Accuracy: 93.0\n","Iteration:25,  Accuracy: 92.88461538461539\n","Iteration:26,  Accuracy: 92.90123456790124\n","Iteration:27,  Accuracy: 92.73809523809524\n","Iteration:28,  Accuracy: 92.8735632183908\n","Iteration:29,  Accuracy: 92.77777777777777\n","Iteration:30,  Accuracy: 92.68817204301075\n","Iteration:31,  Accuracy: 92.70833333333333\n","Iteration:32,  Accuracy: 92.67676767676768\n","Iteration:33,  Accuracy: 92.69607843137256\n","Iteration:34,  Accuracy: 92.80952380952381\n","Iteration:35,  Accuracy: 92.77777777777777\n","Iteration:36,  Accuracy: 92.70270270270271\n","Iteration:37,  Accuracy: 92.63157894736842\n","Iteration:38,  Accuracy: 92.73504273504274\n","Iteration:39,  Accuracy: 92.75\n","Iteration:40,  Accuracy: 92.8048780487805\n","Iteration:41,  Accuracy: 92.77777777777777\n","Iteration:42,  Accuracy: 92.71317829457364\n","Iteration:43,  Accuracy: 92.8409090909091\n","Iteration:44,  Accuracy: 92.92592592592592\n","Iteration:45,  Accuracy: 92.93478260869566\n","Iteration:46,  Accuracy: 92.87234042553192\n","Iteration:47,  Accuracy: 92.88194444444444\n","Iteration:48,  Accuracy: 92.92517006802721\n","Iteration:49,  Accuracy: 92.9\n","Iteration:50,  Accuracy: 92.90849673202614\n","Iteration:51,  Accuracy: 92.75641025641026\n","Iteration:52,  Accuracy: 92.73584905660377\n","Iteration:53,  Accuracy: 92.74691358024691\n","Iteration:54,  Accuracy: 92.81818181818181\n","Iteration:55,  Accuracy: 92.88690476190476\n","Iteration:56,  Accuracy: 92.92397660818713\n","Iteration:57,  Accuracy: 92.98850574712644\n","Iteration:58,  Accuracy: 92.79661016949153\n","Iteration:59,  Accuracy: 92.83333333333333\n","Iteration:60,  Accuracy: 92.81420765027322\n","Iteration:61,  Accuracy: 92.8763440860215\n","Iteration:62,  Accuracy: 92.83068783068784\n","Iteration:63,  Accuracy: 92.83854166666667\n","Iteration:64,  Accuracy: 92.87179487179488\n","Iteration:65,  Accuracy: 92.92929292929293\n","Iteration:66,  Accuracy: 92.91044776119404\n","Iteration:67,  Accuracy: 92.91666666666667\n","Iteration:68,  Accuracy: 92.8743961352657\n","Iteration:69,  Accuracy: 92.85714285714286\n","Iteration:70,  Accuracy: 92.8169014084507\n","Iteration:71,  Accuracy: 92.73148148148148\n","Iteration:72,  Accuracy: 92.78538812785388\n","Iteration:73,  Accuracy: 92.77027027027027\n","Iteration:74,  Accuracy: 92.8\n","Iteration:75,  Accuracy: 92.82894736842105\n","Iteration:76,  Accuracy: 92.81385281385282\n","Iteration:77,  Accuracy: 92.77777777777777\n","Iteration:78,  Accuracy: 92.84810126582279\n","Iteration:79,  Accuracy: 92.8125\n","Iteration:80,  Accuracy: 92.86008230452676\n","Iteration:81,  Accuracy: 92.84552845528455\n","Iteration:82,  Accuracy: 92.83132530120481\n","Iteration:83,  Accuracy: 92.85714285714286\n","Iteration:84,  Accuracy: 92.92156862745098\n","Iteration:85,  Accuracy: 92.92635658914729\n","Iteration:86,  Accuracy: 92.89272030651341\n","Iteration:87,  Accuracy: 92.8409090909091\n","Iteration:88,  Accuracy: 92.84644194756554\n","Iteration:89,  Accuracy: 92.81481481481481\n","Iteration:90,  Accuracy: 92.87545787545787\n","Iteration:91,  Accuracy: 92.89855072463769\n","Iteration:92,  Accuracy: 92.8673835125448\n","Iteration:93,  Accuracy: 92.85460992907801\n","Iteration:94,  Accuracy: 92.91228070175438\n","Iteration:95,  Accuracy: 92.95138888888889\n","Iteration:96,  Accuracy: 92.86941580756013\n","Iteration:97,  Accuracy: 92.87414965986395\n","Iteration:98,  Accuracy: 92.85955435516543\n","Test Accuracy of the model: 92.85955435516543 %\n"],"name":"stdout"}]}]}